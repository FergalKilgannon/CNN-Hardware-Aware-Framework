{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST FORWARD PASS\n",
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from naive_mnist import NaiveModel\n",
    "from nni.algorithms.compression.pytorch.quantization import QAT_Quantizer\n",
    "from nni.compression.pytorch.quantization.settings import set_quant_scheme_dtype\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "mismatch = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Pass Class\n",
    "\n",
    "Note the function *forward_pass()*. This contains our CNN design, and is generalized numerically. Later in the notebook, it is possible to customize some aspects of this CNN (number of kernels used, size of the kernels, fully connected node counts, etc.), but the overall structure will need to be recoded. For instance, if you wanted a third convolutional layer, you will add that function wherever in the order you require it, but also will need to take note of the numerical values being fed into and out of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class forward_custom:\n",
    "  def __init__(self, model, data, batch, num_bits, ifmap,\n",
    "               ofmap, idim, knl, stride, conv2kernels, maxpool_dim, \n",
    "               fc1_nodes, fc2_nodes, non_ideal_file, ncol, nrow, device):\n",
    "    self.model = model\n",
    "    self.data = data\n",
    "    self.batch = batch\n",
    "    self.num_bits = num_bits\n",
    "    self.ifmap = ifmap                # Image (grayscale MNIST)\n",
    "    self.ofmap = ofmap                # Output maps (from applying ofmap knl x knl weight kernels)\n",
    "    self.idim = idim                  # Input image dimensions\n",
    "    self.knl = knl                    # Kernel dimensions\n",
    "    self.stride = stride              # Convolutional stride\n",
    "    self.conv2kernels = conv2kernels  # Output channels to the second conv layer\n",
    "    self.maxpool_dim = maxpool_dim    # Maxpool area\n",
    "    self.fc1_nodes = fc1_nodes        # Fully connected layer 2 output nodes\n",
    "    self.fc2_nodes = fc2_nodes        # Fully connected layer 2 output nodes\n",
    "    self.ncol = ncol                  # Hardware matrix-vector-multiplication (MVM) columns (corresponds to number of output ADCs)\n",
    "    self.nrow = nrow                  # Hardware MVM rows (corresponds to number of input DACs)\n",
    "                                      # In-memory computing hardware therefore has ncol*nrow weights.\n",
    "\n",
    "    self.nl_mult = self.read_non_idealities(non_ideal_file)\n",
    "    self.device = device\n",
    "    self.gain_mismatch = self.gen_gain_mismatch()\n",
    "    self.offset_mismatch = self.gen_offset_mismatch()\n",
    "\n",
    "  def gen_gain_mismatch(self):\n",
    "     return torch.transpose(torch.from_numpy(np.zeros((self.nrow, 1)) + (np.random.normal(0, mismatch/100, (self.ncol)) + 1)), 0, 1)\n",
    "     \n",
    "  def gen_offset_mismatch(self):\n",
    "     return torch.transpose(torch.from_numpy(np.zeros((self.nrow, 1)) + np.random.normal(0, mismatch/100, (self.ncol))), 0, 1)\n",
    "\n",
    "\n",
    "  def read_non_idealities(self, file_name):\n",
    "    return pd.read_excel(file_name, index_col=0).values\n",
    "\n",
    "  def forward_pass(self):\n",
    "    # Scale input\n",
    "    s_in, x = self.scale_quant(self.data, self.num_bits)\n",
    "    \n",
    "    # Convolutional layer 1\n",
    "    x = self.conv2d(x, self.model.conv1.module, s_in, self.idim, self.ifmap, self.ofmap)\n",
    "\n",
    "    # ReLU 1\n",
    "    x = self.relu6(x, 0, 6, self.model.relu1.module)\n",
    "\n",
    "    # Maxpool layer 1\n",
    "    x = self.maxpool2d(x, self.idim - self.knl + 1, self.ofmap, self.maxpool_dim)\n",
    "    \n",
    "    # Scale for convolutional layer 2\n",
    "    sin_conv2 = self.model.conv2.module.input_scale\n",
    "    x = self.noscale_quant(x, sin_conv2.cpu(), 0, self.num_bits)\n",
    "    x = self.conv2d(x, self.model.conv2.module, sin_conv2, (self.idim-self.knl+1)/2, self.ofmap, self.conv2kernels)\n",
    "\n",
    "    # ReLU 2\n",
    "    x = self.relu6(x, 0, 6, self.model.relu2.module)\n",
    "\n",
    "    # Maxpool layer 2\n",
    "    x = self.maxpool2d(x, ((self.idim - self.knl+1)/2) - self.knl + 1, self.conv2kernels, self.maxpool_dim)\n",
    "\n",
    "    # Fully connected layer 1\n",
    "    x = x.view(-1, x.size()[1:].numel())  # Flatten outputs of out_maxpool2 layer to feed to FC layers.\n",
    "    x = self.fc(x, self.model.fc1.module, self.fc1_nodes)\n",
    "\n",
    "    # ReLU 3\n",
    "    x = self.relu6(x, 0, 6, self.model.relu3.module)\n",
    "\n",
    "    # Fully connected layer 2\n",
    "    x = self.fc(x.cpu(), self.model.fc2.module, self.fc2_nodes)\n",
    "\n",
    "    # Softmax layer\n",
    "    return F.log_softmax(x, dim=1)\n",
    "\n",
    "  def conv2d(self, x, conv, s_in, idim, ifmap, ofmap):\n",
    "    # Scale input image, conv weight and bias to bits used\n",
    "    sw_conv, filters_conv = self.scale_quant(conv.weight.cpu(), self.num_bits)\n",
    "    bias_conv = conv.bias / (s_in * sw_conv)\n",
    "\n",
    "    # CONV layer (WSAB dataflow - see convolve2D_wsab function)\n",
    "    out_conv = self.convolve2D_wsab(x, filters_conv, bias_conv, 0, self.stride, self.batch, ifmap, ofmap, idim, self.knl, self.ncol, self.nrow)\n",
    "    out_conv = torch.from_numpy(out_conv)\n",
    "    out_conv = out_conv.to(self.device)\n",
    "\n",
    "    # Applying scaling normalization s_in*sw_conv (convert back to float)\n",
    "    return out_conv * s_in * sw_conv\n",
    "\n",
    "\n",
    "  def convolve2D_wsab(self, image, kernel, bias, padding, strides, batch, ifmap, ofmap, idim, knl, ncol, nrow):\n",
    "    xKernShape = knl\n",
    "    yKernShape = knl\n",
    "    xImgShape = idim\n",
    "    yImgShape = idim\n",
    "\n",
    "    # Shape of Output Convolution\n",
    "    xOutput = int(((xImgShape - xKernShape + 2 * padding) / strides) + 1)\n",
    "    yOutput = int(((yImgShape - yKernShape + 2 * padding) / strides) + 1)\n",
    "    output = np.zeros((batch, ofmap, xOutput, yOutput))\n",
    "\n",
    "    # WEIGHT STATIONARY\n",
    "\n",
    "    # Number of weight block partitions to fit into hardware\n",
    "    block_col = int(np.ceil(ofmap/ncol))\n",
    "    block_row = int(np.ceil(knl*knl*ifmap/nrow))\n",
    "    kernel_flat = np.zeros((block_col*ncol, block_row*nrow))\n",
    "\n",
    "    kernel_flat[0:ofmap,0:ifmap*knl*knl] = torch.reshape(kernel, [ofmap, ifmap*knl*knl]).cpu().numpy()\n",
    "    # data_moved_count = 0\n",
    "    # mac_completed_count = 0\n",
    "    # batch = 1\n",
    "    # Process image by image\n",
    "    for b in range(batch):\n",
    "      # Apply Equal Padding to All Sides\n",
    "      if padding != 0:\n",
    "          imagePadded = np.zeros((idim + padding * 2, idim + padding * 2))\n",
    "          imagePadded[int(padding):int(-1 * padding), int(padding):int(-1 * padding)] = image[b]\n",
    "      else:\n",
    "          imagePadded = image[b]\n",
    "\n",
    "      # Iterate through image\n",
    "      image_block = np.zeros((block_row * nrow, xOutput, yOutput))\n",
    "      otemp = torch.zeros(block_row, block_col*ncol, xOutput, yOutput)\n",
    "      for bc in range(block_col):\n",
    "          for br in range(block_row):\n",
    "              # data_moved_count += nrow*ncol\n",
    "              ktemp = torch.zeros(ncol,nrow)\n",
    "              ktemp = kernel_flat[ncol*bc:(bc+1)*ncol, nrow*br:(br+1)*nrow]\n",
    "              ktemp = torch.from_numpy(ktemp)\n",
    "\n",
    "              for y in range(yOutput):\n",
    "                  for x in range(xOutput):\n",
    "                      # Fetch image section x,y, bc,br\n",
    "                      image_block[0:knl * knl * ifmap, x, y] = imagePadded[0:ifmap,\n",
    "                                                                strides * x: strides * x + xKernShape,\n",
    "                                                                strides * y: strides * y + yKernShape].reshape(\n",
    "                          knl * knl * ifmap).cpu().numpy()\n",
    "                      # data_moved_count += xKernShape*yKernShape\n",
    "                      # mac_completed_count += ncol\n",
    "                      # data_moved_count += nrow + 1\n",
    "\n",
    "                      itemp = image_block[br*nrow:(br+1)*nrow, x, y].reshape(nrow)\n",
    "                      itemp = torch.from_numpy(itemp)\n",
    "                      # Replace this line with Arduino SPI function call\n",
    "                      otemp[br, bc*ncol:(bc+1)*ncol, x, y] = self.non_linear_test(ktemp, itemp, dim=1)\n",
    "                      #otemp[br, bc*ncol:(bc+1)*ncol, x, y] = self.mismatch_grid(ktemp, itemp, dim=1)\n",
    "                      #otemp[br, bc*ncol:(bc+1)*ncol, x, y] = self.scaled_MAC(ktemp, itemp, dim=1)\n",
    "                      #otemp[br, bc*ncol:(bc+1)*ncol, x, y] = torch.sum(ktemp*itemp, dim=1)\n",
    "                      #otemp[br, bc*ncol:(bc+1)*ncol, x, y] = self.non_linear_mult(ktemp, itemp)\n",
    "                      # print(\"Batch:%d,BROW:%d,BCOL:%d,X:%d,Y:%d\" % (b,br,bc,x,y))\n",
    "\n",
    "      output[b] = torch.sum(otemp[:, 0:ofmap, :, :], dim=0) + (bias.reshape(ofmap, 1, 1).cpu()*torch.ones(xOutput, yOutput)).detach().numpy()\n",
    "\n",
    "    # # print(f\"Moved {data_moved_count} pieces of data, {mac_completed_count} MAC operations completed\")\n",
    "    # # print(ktemp[1000])  # Workaround breakpoint\n",
    "\n",
    "    # # OUTPUT STATIONARY\n",
    "    # # Number of output block partitions to fit into hardware\n",
    "    # block_col = int(np.ceil(xOutput/ncol))\n",
    "    # block_row = int(np.ceil(knl*knl*ifmap/nrow))\n",
    "\n",
    "    # kernel_flat = np.zeros((ofmap, block_row*nrow))\n",
    "    # kernel_flat[0:ofmap, 0:ifmap*knl*knl] = torch.reshape(kernel, [ofmap, ifmap*knl*knl]).cpu().numpy()\n",
    "\n",
    "    # data_moved_count = 0\n",
    "    # mac_completed_count = 0\n",
    "\n",
    "    # # Process image by image\n",
    "    # for b in range(batch):\n",
    "    #   # Apply Equal Padding to All Sides\n",
    "    #   if padding != 0:\n",
    "    #       imagePadded = np.zeros((idim + padding * 2, idim + padding * 2))\n",
    "    #       imagePadded[int(padding):int(-1 * padding), int(padding):int(-1 * padding)] = image[b]\n",
    "    #   else:\n",
    "    #       imagePadded = image[b]\n",
    "\n",
    "    #   image_block = np.zeros((nrow*block_row, ncol*block_col, yOutput))\n",
    "    #   otemp = torch.zeros((nrow*block_row, ofmap, ncol*block_col, yOutput))\n",
    "\n",
    "    #   for y in range(yOutput):\n",
    "    #      for x in range(xOutput):\n",
    "    #         image_block[0:knl * knl * ifmap, x, y] = imagePadded[0:ifmap,\n",
    "    #                                                       strides * x: strides * x + xKernShape,\n",
    "    #                                                       strides * y: strides * y + yKernShape].reshape(\n",
    "    #                 knl * knl * ifmap).cpu().numpy()\n",
    "\n",
    "    #   # Iterate through image\n",
    "    #   for y in range(yOutput):\n",
    "    #       for bc in range(block_col):\n",
    "    #         for br in range(block_row):\n",
    "    #           data_moved_count += nrow*ncol\n",
    "    #           for kerCol in range(ofmap):\n",
    "    #             data_moved_count += nrow\n",
    "    #             ktemp = kernel_flat[kerCol, br*nrow:(br+1)*nrow]\n",
    "    #             ktemp = torch.from_numpy(ktemp)\n",
    "    #             for x in range(ncol):\n",
    "    #                 mac_completed_count += 1\n",
    "    #                 data_moved_count += 1\n",
    "    #                 # Fetch image section x,y, bc,br\n",
    "    #                 itemp = image_block[br*nrow:(br+1)*nrow, (bc*ncol)+x, y].reshape(1, nrow)\n",
    "    #                 itemp = torch.from_numpy(itemp)\n",
    "    #                 # Replace this line with Arduino SPI function call\n",
    "    #                 otemp[br, kerCol, (bc*ncol)+x, y] = torch.sum(ktemp*itemp, dim=1)\n",
    "    #                 # otemp[kerCol, x, y] = self.non_linear_mult(ktemp, itemp[0])\n",
    "    #                 # print(\"Batch:%d,BROW:%d,BCOL:%d,X:%d,Y:%d\" % (b,br,bc,x,y))\n",
    "    #   output[b] = torch.sum(otemp[0:block_row, 0:ofmap, 0:xOutput, 0:yOutput], dim=0) + (bias.reshape(ofmap, 1, 1).cpu()*torch.ones(xOutput, yOutput)).detach().numpy()\n",
    "    # # #   print(f\"Moved {data_moved_count} pieces of data, {mac_completed_count} MAC operations completed\")\n",
    "    # # #   print(ktemp[1000]) # Workaround breakpoint\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "  def relu6(self, x, min_val, max_val, relu_module):\n",
    "    i = (x >= min_val) * x\n",
    "    out_relu = (i <= max_val) * (i - max_val) + max_val\n",
    "    so_relu = relu_module.output_scale\n",
    "\n",
    "    # Apply fake quantization to relu1 output.\n",
    "    out_relu = self.noscale_quant(out_relu, so_relu, 0, self.num_bits)\n",
    "    return self.dequantize(out_relu, so_relu, 0)\n",
    "\n",
    "\n",
    "  def maxpool2d(self, x, idim, ofmap, knl):\n",
    "    # Maxpool layer - downsample by knl x knl with maxpool (no quantization required for max function)\n",
    "    return torch.from_numpy(self.maxpool2D_wsa(self.batch, x, idim, ofmap, knl))\n",
    "\n",
    "\n",
    "  # Maxpool layer (apply to all ofmaps simultaneously - faster!)\n",
    "  def maxpool2D_wsa(self, batch, image, idim, ofmap, knl):\n",
    "    xKernShape = knl\n",
    "    yKernShape = knl\n",
    "    xImgShape = idim\n",
    "    yImgShape = idim\n",
    "    strides = knl\n",
    "    padding = 0\n",
    "    # Shape of Output Convolution\n",
    "    xOutput = int(((xImgShape - xKernShape + 2 * padding) / strides) + 1)\n",
    "    yOutput = int(((yImgShape - yKernShape + 2 * padding) / strides) + 1)\n",
    "    output = np.zeros((batch, ofmap, xOutput, yOutput))\n",
    "\n",
    "    for b in range(batch):\n",
    "        # Apply Equal Padding to All Sides\n",
    "        if padding != 0:\n",
    "            imagePadded = np.zeros((idim + padding * 2, idim + padding * 2))\n",
    "            imagePadded[int(padding):int(-1 * padding), int(padding):int(-1 * padding)] = image[b]\n",
    "        else:\n",
    "            imagePadded = image[b]\n",
    "\n",
    "        # Iterate through image\n",
    "        for y in range(yOutput):\n",
    "            for x in range(xOutput):\n",
    "                output[b, :, x, y] = torch.amax(\n",
    "                        imagePadded[:, strides * x: strides * x + xKernShape, strides * y: strides * y + yKernShape], dim=(1,2)).detach().cpu().numpy()\n",
    "    return output\n",
    "\n",
    "\n",
    "  def avgpool2d(self, x, idim, ofmap, knl):\n",
    "    # Maxpool layer - downsample by knl x knl with maxpool (no quantization required for max function)\n",
    "    return torch.from_numpy(self.avgpool2D_wsa(self.batch, x, idim, ofmap, knl))\n",
    "  \n",
    "\n",
    "  # Maxpool layer (apply to all ofmaps simultaneously - faster!)\n",
    "  def avgpool2D_wsa(self, batch, image, idim, ofmap, knl):\n",
    "    xKernShape = knl\n",
    "    yKernShape = knl\n",
    "    xImgShape = idim\n",
    "    yImgShape = idim\n",
    "    strides = knl\n",
    "    padding = 0\n",
    "    # Shape of Output Convolution\n",
    "    xOutput = int(((xImgShape - xKernShape + 2 * padding) / strides) + 1)\n",
    "    yOutput = int(((yImgShape - yKernShape + 2 * padding) / strides) + 1)\n",
    "    output = np.zeros((batch, ofmap, xOutput, yOutput))\n",
    "\n",
    "    for b in range(batch):\n",
    "        # Apply Equal Padding to All Sides\n",
    "        if padding != 0:\n",
    "            imagePadded = np.zeros((idim + padding * 2, idim + padding * 2))\n",
    "            imagePadded[int(padding):int(-1 * padding), int(padding):int(-1 * padding)] = image[b]\n",
    "        else:\n",
    "            imagePadded = image[b]\n",
    "\n",
    "        # Iterate through image\n",
    "        for y in range(yOutput):\n",
    "            for x in range(xOutput):\n",
    "                for channel in range(ofmap):\n",
    "                  # output[b, channel, x, y] = torch.sum(\n",
    "                  #    imagePadded[channel, strides * x: strides * x + xKernShape, strides * y: strides * y + yKernShape].reshape(knl*knl) \n",
    "                  #    * torch.ones(xKernShape*yKernShape), dim=0)/(knl*knl)\n",
    "                  output[b, channel, x, y] = self.non_linear_test(imagePadded[channel, strides * x: strides * x + xKernShape, strides * y: strides * y + yKernShape].reshape(knl*knl),\n",
    "                                                                  torch.ones(xKernShape*yKernShape))/(knl*knl)\n",
    "                  # output[b, channel, x, y] = self.non_linear_mult(\n",
    "                  #       imagePadded[channel, strides * x: strides * x + xKernShape, strides * y: strides * y + yKernShape].reshape(knl*knl),\n",
    "                  #       torch.ones(xKernShape*yKernShape))/(knl*knl)\n",
    "    return output\n",
    "\n",
    "\n",
    "  def fc(self, x, module, odim):\n",
    "    sin_fc = module.input_scale.cpu()\n",
    "    in_fcs = self.noscale_quant(x, sin_fc, 0, self.num_bits)\n",
    "    sw_fc, filters_fc = self.scale_quant(module.weight.cpu(), self.num_bits)\n",
    "    bias_fc = module.bias.cpu() / (sw_fc * sin_fc)\n",
    "\n",
    "\n",
    "    # FC layer using (WSA dataflow)\n",
    "    # N.B. Need to implement WSAB function to implement 'hardware acceleration' of FC layer.\n",
    "    #out_fc = torch.from_numpy(self.fc_custom_wsa(in_fcs, filters_fc, bias_fc, self.batch, odim))\n",
    "    out_fc = torch.from_numpy(self.fc_custom_wsab(in_fcs, filters_fc, bias_fc, self.batch, odim, self.ncol, self.nrow))\n",
    "\n",
    "    # FC output scaling\n",
    "    out_fcs = out_fc * sin_fc * sw_fc\n",
    "\n",
    "    so_fc = module.output_scale.cpu()\n",
    "    # FC output fake quantization\n",
    "    return self.dequantize(self.noscale_quant(out_fcs, so_fc, 0, self.num_bits), so_fc, 0).to(self.device)\n",
    "\n",
    "\n",
    "  # 'Fake' Quantization function [Jacob et. al]\n",
    "  def quantize(self, real_value, scale, zero_point, qmin, qmax):\n",
    "    transformed_val = zero_point + real_value / scale\n",
    "    clamped_val = torch.clamp(transformed_val, qmin, qmax)\n",
    "    quantized_val = torch.round(clamped_val)\n",
    "    return quantized_val\n",
    "\n",
    "\n",
    "  # 'Fake' Dequantization function [Jacob et. al]\n",
    "  def dequantize(self, quantized_val, scale, zero_point):\n",
    "    real_val = scale * (quantized_val - zero_point)\n",
    "    return real_val\n",
    "\n",
    "\n",
    "  # Scaling function (Jacob et. al)\n",
    "  def scale_quant(self, real_value, num_bits):\n",
    "    qmin = -(2 ** (num_bits - 1) - 1)\n",
    "    qmax = 2 ** (num_bits - 1) - 1\n",
    "    abs_max = torch.abs(real_value).max()\n",
    "    scale = abs_max / (float(qmax - qmin) / 2)\n",
    "    zero_point = 0\n",
    "    quant = self.quantize(real_value, scale, zero_point, qmin, qmax)\n",
    "    return scale, quant\n",
    "\n",
    "\n",
    "  # Scaling function (Jacob et. al)\n",
    "  def noscale_quant(self, real_value, scale, zero_point, num_bits):\n",
    "    qmin = -(2 ** (num_bits - 1) - 1)\n",
    "    qmax = 2 ** (num_bits - 1) - 1\n",
    "    quant = self.quantize(real_value, scale, zero_point, qmin, qmax)\n",
    "    return quant\n",
    "\n",
    "\n",
    "  # Custom FC layer (vector multiplication style - much faster than for loop implementation)\n",
    "  def fc_custom_wsa(self, fc_input, filters, bias, batch, ofmap):\n",
    "    output = np.zeros((batch, ofmap))\n",
    "    for b in range(batch):\n",
    "        outputi = np.zeros(ofmap)\n",
    "        input_batch = fc_input[b]\n",
    "        outputi = torch.sum((input_batch * filters),dim=1) + bias\n",
    "        output[b, :] = outputi.detach().numpy()\n",
    "    return output\n",
    "  \n",
    "  def fc_custom_wsab(self, fc_input, filters, bias, batch, ofmap, ncol, nrow):\n",
    "    output = np.zeros((batch, ofmap))\n",
    "    for b in range(batch):\n",
    "        # WEIGHT STATIONARY\n",
    "\n",
    "        # Iterate through image\n",
    "        block_col = int(np.ceil(ofmap/ncol))\n",
    "        block_row = int(np.ceil(fc_input.size()[1]/nrow))\n",
    "\n",
    "        filters_flat = np.zeros((block_col*ncol, block_row*nrow), dtype='int64')\n",
    "        image_col = np.zeros((block_row*nrow), dtype='int64')\n",
    "\n",
    "        filters_flat[0:ofmap,0:fc_input.size()[1]] = torch.reshape(filters, [ofmap, fc_input.size()[1]]).cpu().numpy()\n",
    "        image_col[0:fc_input.size()[1]] = fc_input[b, :]\n",
    "\n",
    "        otemp = torch.zeros(block_row, block_col*ncol)\n",
    "\n",
    "        for bc in range(block_col):\n",
    "            for br in range(block_row):\n",
    "                ftemp = torch.zeros(ncol,nrow)\n",
    "                ftemp = filters_flat[ncol*bc:(bc+1)*ncol, nrow*br:(br+1)*nrow]\n",
    "                ftemp = torch.from_numpy(ftemp)\n",
    "\n",
    "                itemp = image_col[br*nrow:(br+1)*nrow].reshape(nrow)\n",
    "                itemp = torch.from_numpy(itemp)\n",
    "\n",
    "                for col in range(ncol):\n",
    "                  # Replace this line with Arduino SPI function call\n",
    "                  otemp[br, (bc*ncol)+col] = self.non_linear_test(ftemp[col, :], itemp)\n",
    "                  #otemp[br, (bc*ncol)+col] = self.mismatch_grid(ftemp[col, :], itemp, col)\n",
    "                  #otemp[br, (bc*ncol)+col] = self.scaled_MAC(ftemp[col, :], itemp)\n",
    "                  #otemp[br, (bc*ncol)+col] = self.non_linear_mult(ftemp[col, :], itemp)\n",
    "                  # print(\"Batch:%d,BROW:%d,BCOL:%d,X:%d,Y:%d\" % (b,br,bc,x,y))\n",
    "        output[b] = torch.sum(otemp[:, 0:ofmap], dim=0) + (bias.reshape(ofmap).cpu()*torch.ones(ofmap)).detach().numpy()\n",
    "    return output\n",
    "\n",
    "\n",
    "  # Non-linear look-up\n",
    "  def non_linear_conv(self, kernel, image):\n",
    "    sum = 0\n",
    "    if self.num_bits == 9 or self.num_bits == 8:\n",
    "       bit = 9\n",
    "    else:\n",
    "       bit = 5\n",
    "    for i in range(len(kernel)):\n",
    "      sum = sum + self.nl_mult[int(kernel[i]+(2**(bit-1))-1)][int(image[i]+(2**(bit-1))-1)]\n",
    "    return sum\n",
    "\n",
    "  def non_linear_mult(self, kernel, image):\n",
    "    output = np.empty(len(kernel))\n",
    "    if len(kernel.shape) == 2:\n",
    "      for i in range(len(kernel)):\n",
    "        output[i] = self.non_linear_conv(kernel[i, :].tolist(), image.tolist())\n",
    "      return torch.from_numpy(output)\n",
    "    else:\n",
    "      output = self.non_linear_conv(kernel.tolist(), image.tolist())\n",
    "      return output\n",
    "    \n",
    "\n",
    "  def non_linear_test(self, kernel, image, dim=0):\n",
    "    # sigma = 3\n",
    "    # noise = torch.from_numpy(np.random.normal(0, 1, kernel.shape))*torch.sqrt(torch.abs(kernel))*sigma\n",
    "    # return torch.sum(image * (kernel + noise), dim=dim, dtype=torch.int64)\n",
    "    a1 = 1.3\n",
    "    a3 = 10*10**(-5)\n",
    "    return torch.sum(image*((a1*kernel) + a3*(kernel**3)), dim=dim, dtype=torch.int64)\n",
    "    #return torch.sum(kernel*image, dim=dim, dtype=torch.int64)\n",
    "      \n",
    "  def slice_input(self, array):\n",
    "    array = array.int()\n",
    "    up_array = np.sign(array)*(abs(array)&np.int16(240))>>4\n",
    "    low_array = np.sign(array)*(abs(array)&np.int16(15))\n",
    "    return up_array, low_array\n",
    "      \n",
    "  \n",
    "  def scaled_MAC(self, kernel, image, dim=0):\n",
    "\n",
    "    if len(kernel.shape) == 2:\n",
    "        kernel_l = torch.empty((kernel.shape[0], kernel.shape[1]))\n",
    "        kernel_u = torch.empty((kernel.shape[0], kernel.shape[1]))\n",
    "        i = 0\n",
    "        for channel in kernel:\n",
    "          kernel_u[i], kernel_l[i] = self.slice_input(channel)\n",
    "          i += 1\n",
    "    else:\n",
    "      kernel_u, kernel_l = self.slice_input(kernel)\n",
    "\n",
    "    image_u, image_l = self.slice_input(image)\n",
    "    \n",
    "    result = torch.add(torch.add(self.non_linear_test(kernel_l, image_l, dim=dim), (self.non_linear_test(kernel_u, image_l, dim=dim)\n",
    "                                                      + self.non_linear_test(kernel_l, image_u, dim=dim))<<4), (self.non_linear_test(kernel_u, image_u, dim=dim))<<8)\n",
    "    return result\n",
    "    \n",
    "  def mismatch_grid(self, kernel, image, col=0, dim=0):\n",
    "   if len(kernel.shape) == 2:\n",
    "    return torch.sum(image * (self.gain_mismatch*kernel + self.offset_mismatch*kernel), dim=dim, dtype=torch.int64)\n",
    "   else:\n",
    "    return torch.sum(image * (self.gain_mismatch[col]*kernel + self.offset_mismatch[col]*kernel), dim=dim, dtype=torch.int64)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel dimensions (used in conv layers)\n",
    "knl = 5\n",
    "\n",
    "# Kernel stride\n",
    "stride = 1\n",
    "\n",
    "# Output Feature Maps (same as kernels in conv layer 1)\n",
    "ofmap = 16\n",
    "\n",
    "# Kernels in conv layer 2\n",
    "conv2kernels = 32\n",
    "\n",
    "# Maxpool dimensions (assuming square area)\n",
    "maxpool_dim = 2\n",
    "\n",
    "# Fully Connected 1 output nodes\n",
    "fc1_nodes = 50\n",
    "\n",
    "# Hardware MVM columns\n",
    "ncol = 16\n",
    "\n",
    "# Hardware MVM rows\n",
    "nrow = 32\n",
    "\n",
    "# Non-Idealities File\n",
    "non_ideal_file = '../HardwareSpec/4x4_Mac_result_final.xlsx'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom test model function\n",
    "# Only implements forward pass of neural network - no training done.\n",
    "def test_custom(model, device, test_loader, num_bits, idim, ifmap, ofmap, knl, stride,\n",
    "                conv2kernels, maxpool_dim, fc1_nodes, fc2_nodes, non_ideal_file, ncol, nrow):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    i = 1\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            batch = data.size()[0]\n",
    "\n",
    "            # Custom forward pass function\n",
    "            non_linear_model = forward_custom(model, data, batch, num_bits, ifmap, ofmap, idim, knl, stride, conv2kernels,\n",
    "                                              maxpool_dim, fc1_nodes, fc2_nodes, non_ideal_file, ncol, nrow, device)\n",
    "            output = non_linear_model.forward_pass()\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            print(f\"ITERATION {i}: Accuracy = {correct/i:.2f}% (cumulative = {correct})\")\n",
    "            i = i+1\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print(f\"Loss: {test_loss}   Accuracy: {100 * correct / len(test_loader.dataset)}%\\n\")\n",
    "\n",
    "\n",
    "# Test original/default model forward pass.\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print(f\"Loss: {test_loss}   Accuracy: {100 * correct / len(test_loader.dataset)}%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization and Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float32)\n",
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# INT9 weight, INT9 activations\n",
    "num_bits = 8\n",
    "# Make sure this matches quantization config from MNIST_CNN_Training\n",
    "configure_list = [{\n",
    "    'quant_types': ['weight', 'input'],\n",
    "    'quant_bits': {'weight': num_bits, 'input': num_bits},\n",
    "    'quant_start_step': 2,\n",
    "    'op_names': ['conv1', 'conv2']\n",
    "}, {\n",
    "    'quant_types': ['output'],\n",
    "    'quant_bits': {'output': num_bits},\n",
    "    'quant_start_step': 2,\n",
    "    'op_names': ['relu1', 'relu2', 'relu3']\n",
    "}, {\n",
    "    'quant_types': ['output', 'weight', 'input'],\n",
    "    'quant_bits': {'output': num_bits, 'weight': num_bits, 'input': num_bits},\n",
    "    'quant_start_step': 2,\n",
    "    'op_names': ['fc1', 'fc2'],\n",
    "}]\n",
    "\n",
    "set_quant_scheme_dtype('weight', 'per_tensor_symmetric', 'int')\n",
    "set_quant_scheme_dtype('output', 'per_tensor_symmetric', 'int')\n",
    "set_quant_scheme_dtype('input', 'per_tensor_symmetric', 'int')\n",
    "\n",
    "# Load MNIST dataset with train/test split sets.\n",
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=True, download=True, transform=trans),\n",
    "    batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=False, transform=trans),\n",
    "    batch_size=100, shuffle=True)\n",
    "\n",
    "idim = next(iter(train_loader))[0][0].size()[1]\n",
    "ifmap = next(iter(train_loader))[0][0].size()[0]\n",
    "fc2_nodes = len(torch.unique(train_loader.dataset.targets))\n",
    "\n",
    "# Create a NaiveModel object and apply QAT_Quantizer setup\n",
    "model_path = \"models/mnist_model_8bit.pth\"\n",
    "qmodel = NaiveModel().to(device)\n",
    "dummy_input = torch.randn(1, ifmap, idim, idim).to(device)\n",
    "optimizer = torch.optim.SGD(qmodel.parameters(), lr=0.01, momentum=0.5)\n",
    "# To enable batch normalization folding in the training process, you should\n",
    "# pass dummy_input to the QAT_Quantizer.\n",
    "quantizer = QAT_Quantizer(qmodel, configure_list, optimizer, dummy_input=dummy_input)\n",
    "quantizer.compress()\n",
    "\n",
    "# Load trained model (from MNIST_CNN_Training step).\n",
    "state = torch.load(model_path, map_location=device)\n",
    "qmodel.load_state_dict(state, strict=True)\n",
    "qmodel.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.039150652197003366   Accuracy: 98.57%\n",
      "\n",
      "ITERATION 1: Accuracy = 85.00% (cumulative = 85)\n",
      "ITERATION 2: Accuracy = 86.50% (cumulative = 173)\n",
      "ITERATION 3: Accuracy = 87.00% (cumulative = 261)\n",
      "ITERATION 4: Accuracy = 85.00% (cumulative = 340)\n",
      "ITERATION 5: Accuracy = 84.60% (cumulative = 423)\n",
      "ITERATION 6: Accuracy = 84.00% (cumulative = 504)\n",
      "ITERATION 7: Accuracy = 83.71% (cumulative = 586)\n",
      "ITERATION 8: Accuracy = 83.25% (cumulative = 666)\n",
      "ITERATION 9: Accuracy = 83.33% (cumulative = 750)\n",
      "ITERATION 10: Accuracy = 83.80% (cumulative = 838)\n",
      "ITERATION 11: Accuracy = 83.82% (cumulative = 922)\n",
      "ITERATION 12: Accuracy = 84.08% (cumulative = 1009)\n",
      "ITERATION 13: Accuracy = 84.00% (cumulative = 1092)\n",
      "ITERATION 14: Accuracy = 84.07% (cumulative = 1177)\n",
      "ITERATION 15: Accuracy = 84.27% (cumulative = 1264)\n",
      "ITERATION 16: Accuracy = 84.12% (cumulative = 1346)\n",
      "ITERATION 17: Accuracy = 84.29% (cumulative = 1433)\n",
      "ITERATION 18: Accuracy = 84.11% (cumulative = 1514)\n",
      "ITERATION 19: Accuracy = 84.26% (cumulative = 1601)\n",
      "ITERATION 20: Accuracy = 84.15% (cumulative = 1683)\n",
      "ITERATION 21: Accuracy = 84.05% (cumulative = 1765)\n",
      "ITERATION 22: Accuracy = 84.27% (cumulative = 1854)\n",
      "ITERATION 23: Accuracy = 84.35% (cumulative = 1940)\n",
      "ITERATION 24: Accuracy = 84.33% (cumulative = 2024)\n",
      "ITERATION 25: Accuracy = 84.44% (cumulative = 2111)\n",
      "ITERATION 26: Accuracy = 84.58% (cumulative = 2199)\n",
      "ITERATION 27: Accuracy = 84.48% (cumulative = 2281)\n",
      "ITERATION 28: Accuracy = 84.46% (cumulative = 2365)\n",
      "ITERATION 29: Accuracy = 84.59% (cumulative = 2453)\n",
      "ITERATION 30: Accuracy = 84.53% (cumulative = 2536)\n",
      "ITERATION 31: Accuracy = 84.35% (cumulative = 2615)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m test(qmodel, device, test_loader)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Evaluate test accuracy of qmodel, with custom forward pass.\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[43mtest_custom\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mifmap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mofmap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mknl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconv2kernels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxpool_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfc1_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfc2_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_ideal_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mncol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnrow\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[34], line 17\u001b[0m, in \u001b[0;36mtest_custom\u001b[1;34m(model, device, test_loader, num_bits, idim, ifmap, ofmap, knl, stride, conv2kernels, maxpool_dim, fc1_nodes, fc2_nodes, non_ideal_file, ncol, nrow)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Custom forward pass function\u001b[39;00m\n\u001b[0;32m     15\u001b[0m non_linear_model \u001b[38;5;241m=\u001b[39m forward_custom(model, data, batch, num_bits, ifmap, ofmap, idim, knl, stride, conv2kernels,\n\u001b[0;32m     16\u001b[0m                                   maxpool_dim, fc1_nodes, fc2_nodes, non_ideal_file, ncol, nrow, device)\n\u001b[1;32m---> 17\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mnon_linear_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m test_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnll_loss(output, target, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     19\u001b[0m pred \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[32], line 48\u001b[0m, in \u001b[0;36mforward_custom.forward_pass\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     45\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu6(x, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mrelu1\u001b[38;5;241m.\u001b[39mmodule)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Maxpool layer 1\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mavgpool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43midim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mknl\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mofmap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaxpool_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Scale for convolutional layer 2\u001b[39;00m\n\u001b[0;32m     51\u001b[0m sin_conv2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconv2\u001b[38;5;241m.\u001b[39mmodule\u001b[38;5;241m.\u001b[39minput_scale\n",
      "Cell \u001b[1;32mIn[32], line 257\u001b[0m, in \u001b[0;36mforward_custom.avgpool2d\u001b[1;34m(self, x, idim, ofmap, knl)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mavgpool2d\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, idim, ofmap, knl):\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;66;03m# Maxpool layer - downsample by knl x knl with maxpool (no quantization required for max function)\u001b[39;00m\n\u001b[1;32m--> 257\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mavgpool2D_wsa\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mofmap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mknl\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[32], line 288\u001b[0m, in \u001b[0;36mforward_custom.avgpool2D_wsa\u001b[1;34m(self, batch, image, idim, ofmap, knl)\u001b[0m\n\u001b[0;32m    283\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(xOutput):\n\u001b[0;32m    284\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m channel \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(ofmap):\n\u001b[0;32m    285\u001b[0m               \u001b[38;5;66;03m# output[b, channel, x, y] = torch.sum(\u001b[39;00m\n\u001b[0;32m    286\u001b[0m               \u001b[38;5;66;03m#    imagePadded[channel, strides * x: strides * x + xKernShape, strides * y: strides * y + yKernShape].reshape(knl*knl) \u001b[39;00m\n\u001b[0;32m    287\u001b[0m               \u001b[38;5;66;03m#    * torch.ones(xKernShape*yKernShape), dim=0)/(knl*knl)\u001b[39;00m\n\u001b[1;32m--> 288\u001b[0m               output[b, channel, x, y] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnon_linear_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimagePadded\u001b[49m\u001b[43m[\u001b[49m\u001b[43mchannel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrides\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrides\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mxKernShape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrides\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrides\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43myKernShape\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mknl\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mknl\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[43m                                                              \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxKernShape\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43myKernShape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m/\u001b[39m(knl\u001b[38;5;241m*\u001b[39mknl)\n\u001b[0;32m    290\u001b[0m               \u001b[38;5;66;03m# output[b, channel, x, y] = self.non_linear_mult(\u001b[39;00m\n\u001b[0;32m    291\u001b[0m               \u001b[38;5;66;03m#       imagePadded[channel, strides * x: strides * x + xKernShape, strides * y: strides * y + yKernShape].reshape(knl*knl),\u001b[39;00m\n\u001b[0;32m    292\u001b[0m               \u001b[38;5;66;03m#       torch.ones(xKernShape*yKernShape))/(knl*knl)\u001b[39;00m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "Cell \u001b[1;32mIn[32], line 424\u001b[0m, in \u001b[0;36mforward_custom.non_linear_test\u001b[1;34m(self, kernel, image, dim)\u001b[0m\n\u001b[0;32m    422\u001b[0m a1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.3\u001b[39m\n\u001b[0;32m    423\u001b[0m a3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m10\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m--> 424\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma1\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ma3\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkernel\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint64\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Evaluate test accuracy with imported quantized model (qmodel) from mnist_model.pth.\n",
    "test(qmodel, device, test_loader)\n",
    "\n",
    "# Evaluate test accuracy of qmodel, with custom forward pass.\n",
    "test_custom(qmodel, device, test_loader, num_bits, idim, ifmap, ofmap, knl, stride,\n",
    "            conv2kernels, maxpool_dim, fc1_nodes, fc2_nodes, non_ideal_file, ncol, nrow)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
