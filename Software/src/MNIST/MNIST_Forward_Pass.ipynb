{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST FORWARD PASS\n",
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from naive_mnistAVGPOOL import NaiveModel\n",
    "from nni.algorithms.compression.pytorch.quantization import QAT_Quantizer\n",
    "from nni.compression.pytorch.quantization.settings import set_quant_scheme_dtype\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Pass Class\n",
    "\n",
    "Note the function *forward_pass()*. This contains our CNN design, and is generalized numerically. Later in the notebook, it is possible to customize some aspects of this CNN (number of kernels used, size of the kernels, fully connected node counts, etc.), but the overall structure will need to be recoded. For instance, if you wanted a third convolutional layer, you will add that function wherever in the order you require it, but also will need to take note of the numerical values being fed into and out of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class forward_custom:\n",
    "  def __init__(self, model, data, batch, num_bits, ifmap,\n",
    "               ofmap, idim, knl, stride, conv2kernels, maxpool_dim, \n",
    "               fc1_nodes, fc2_nodes, non_ideal_file, ncol, nrow, device):\n",
    "    self.model = model\n",
    "    self.data = data\n",
    "    self.batch = batch\n",
    "    self.num_bits = num_bits\n",
    "    self.ifmap = ifmap                # Image (grayscale MNIST)\n",
    "    self.ofmap = ofmap                # Output maps (from applying ofmap knl x knl weight kernels)\n",
    "    self.idim = idim                  # Input image dimensions\n",
    "    self.knl = knl                    # Kernel dimensions\n",
    "    self.stride = stride              # Convolutional stride\n",
    "    self.conv2kernels = conv2kernels  # Output channels to the second conv layer\n",
    "    self.maxpool_dim = maxpool_dim    # Maxpool area\n",
    "    self.fc1_nodes = fc1_nodes        # Fully connected layer 2 output nodes\n",
    "    self.fc2_nodes = fc2_nodes        # Fully connected layer 2 output nodes\n",
    "    self.ncol = ncol                  # Hardware matrix-vector-multiplication (MVM) columns (corresponds to number of output ADCs)\n",
    "    self.nrow = nrow                  # Hardware MVM rows (corresponds to number of input DACs)\n",
    "                                      # In-memory computing hardware therefore has ncol*nrow weights.\n",
    "\n",
    "    self.nl_mult = self.read_non_idealities(non_ideal_file)\n",
    "    self.device = device\n",
    "\n",
    "\n",
    "  def read_non_idealities(self, file_name):\n",
    "    return pd.read_excel(file_name, index_col=0).values\n",
    "\n",
    "  def forward_pass(self):\n",
    "    # Scale input\n",
    "    s_in, x = self.scale_quant(self.data, self.num_bits)\n",
    "    \n",
    "    # Convolutional layer 1\n",
    "    x = self.conv2d(x, self.model.conv1.module, s_in, self.idim, self.ifmap, self.ofmap)\n",
    "\n",
    "    # ReLU 1\n",
    "    x = self.relu6(x, 0, 6, self.model.relu1.module)\n",
    "\n",
    "    # Maxpool layer 1\n",
    "    x = self.avgpool2d(x, self.idim - self.knl + 1, self.ofmap, self.maxpool_dim)\n",
    "    \n",
    "    # Scale for convolutional layer 2\n",
    "    sin_conv2 = self.model.conv2.module.input_scale\n",
    "    x = self.noscale_quant(x, sin_conv2.cpu(), 0, self.num_bits)\n",
    "    x = self.conv2d(x, self.model.conv2.module, sin_conv2, (self.idim-self.knl+1)/2, self.ofmap, self.conv2kernels)\n",
    "\n",
    "    # ReLU 2\n",
    "    x = self.relu6(x, 0, 6, self.model.relu2.module)\n",
    "\n",
    "    # Maxpool layer 2\n",
    "    x = self.avgpool2d(x, ((self.idim - self.knl+1)/2) - self.knl + 1, self.conv2kernels, self.maxpool_dim)\n",
    "\n",
    "    # Fully connected layer 1\n",
    "    x = x.view(-1, x.size()[1:].numel())  # Flatten outputs of out_maxpool2 layer to feed to FC layers.\n",
    "    x = self.fc(x, self.model.fc1.module, self.fc1_nodes)\n",
    "\n",
    "    # ReLU 3\n",
    "    x = self.relu6(x, 0, 6, self.model.relu3.module)\n",
    "\n",
    "    # Fully connected layer 2\n",
    "    x = self.fc(x.cpu(), self.model.fc2.module, self.fc2_nodes)\n",
    "\n",
    "    # Softmax layer\n",
    "    return F.log_softmax(x, dim=1)\n",
    "\n",
    "  def conv2d(self, x, conv, s_in, idim, ifmap, ofmap):\n",
    "    # Scale input image, conv weight and bias to bits used\n",
    "    sw_conv, filters_conv = self.scale_quant(conv.weight.cpu(), self.num_bits)\n",
    "    bias_conv = conv.bias / (s_in * sw_conv)\n",
    "\n",
    "    # CONV layer (WSAB dataflow - see convolve2D_wsab function)\n",
    "    out_conv = self.convolve2D_wsab(x, filters_conv, bias_conv, 0, self.stride, self.batch, ifmap, ofmap, idim, self.knl, self.ncol, self.nrow)\n",
    "    out_conv = torch.from_numpy(out_conv)\n",
    "    out_conv = out_conv.to(self.device)\n",
    "\n",
    "    # Applying scaling normalization s_in*sw_conv (convert back to float)\n",
    "    return out_conv * s_in * sw_conv\n",
    "\n",
    "\n",
    "  def convolve2D_wsab(self, image, kernel, bias, padding, strides, batch, ifmap, ofmap, idim, knl, ncol, nrow):\n",
    "    xKernShape = knl\n",
    "    yKernShape = knl\n",
    "    xImgShape = idim\n",
    "    yImgShape = idim\n",
    "\n",
    "    # Shape of Output Convolution\n",
    "    xOutput = int(((xImgShape - xKernShape + 2 * padding) / strides) + 1)\n",
    "    yOutput = int(((yImgShape - yKernShape + 2 * padding) / strides) + 1)\n",
    "    output = np.zeros((batch, ofmap, xOutput, yOutput))\n",
    "\n",
    "    # WEIGHT STATIONARY\n",
    "\n",
    "    # Number of weight block partitions to fit into hardware\n",
    "    block_col = int(np.ceil(ofmap/ncol))\n",
    "    block_row = int(np.ceil(knl*knl*ifmap/nrow))\n",
    "    kernel_flat = np.zeros((block_col*ncol, block_row*nrow))\n",
    "\n",
    "    kernel_flat[0:ofmap,0:ifmap*knl*knl] = torch.reshape(kernel, [ofmap, ifmap*knl*knl]).cpu().numpy()\n",
    "    # data_moved_count = 0\n",
    "    # mac_completed_count = 0\n",
    "    # batch = 1\n",
    "    # Process image by image\n",
    "    for b in range(batch):\n",
    "      # Apply Equal Padding to All Sides\n",
    "      if padding != 0:\n",
    "          imagePadded = np.zeros((idim + padding * 2, idim + padding * 2))\n",
    "          imagePadded[int(padding):int(-1 * padding), int(padding):int(-1 * padding)] = image[b]\n",
    "      else:\n",
    "          imagePadded = image[b]\n",
    "\n",
    "      # Iterate through image\n",
    "\n",
    "      image_block = np.zeros((block_row * nrow, xOutput, yOutput))\n",
    "      otemp = torch.zeros(block_row, block_col*ncol, xOutput, yOutput)\n",
    "      for bc in range(block_col):\n",
    "          for br in range(block_row):\n",
    "              # data_moved_count += nrow*ncol\n",
    "              ktemp = torch.zeros(ncol,nrow)\n",
    "              ktemp = kernel_flat[ncol*bc:(bc+1)*ncol, nrow*br:(br+1)*nrow]\n",
    "              ktemp = torch.from_numpy(ktemp)\n",
    "\n",
    "              for y in range(yOutput):\n",
    "                  for x in range(xOutput):\n",
    "                      # Fetch image section x,y, bc,br\n",
    "                      image_block[0:knl * knl * ifmap, x, y] = imagePadded[0:ifmap,\n",
    "                                                                strides * x: strides * x + xKernShape,\n",
    "                                                                strides * y: strides * y + yKernShape].reshape(\n",
    "                          knl * knl * ifmap).cpu().numpy()\n",
    "                      # data_moved_count += xKernShape*yKernShape\n",
    "                      # mac_completed_count += ncol\n",
    "                      # data_moved_count += nrow + 1\n",
    "\n",
    "                      itemp = image_block[br*nrow:(br+1)*nrow, x, y].reshape(1, nrow)\n",
    "                      itemp = torch.from_numpy(itemp)\n",
    "                      # Replace this line with Arduino SPI function call\n",
    "                      otemp[br, bc*ncol:(bc+1)*ncol, x, y] = torch.sum(ktemp*itemp, dim=1)\n",
    "                      #otemp[br, bc*ncol:(bc+1)*ncol, x, y] = self.non_linear_mult(ktemp, itemp[0])\n",
    "                      # print(\"Batch:%d,BROW:%d,BCOL:%d,X:%d,Y:%d\" % (b,br,bc,x,y))\n",
    "\n",
    "      output[b] = torch.sum(otemp[:, 0:ofmap, :, :], dim=0) + (bias.reshape(ofmap, 1, 1).cpu()*torch.ones(xOutput, yOutput)).detach().numpy()\n",
    "\n",
    "    # print(f\"Moved {data_moved_count} pieces of data, {mac_completed_count} MAC operations completed\")\n",
    "    # print(ktemp[1000])  # Workaround breakpoint\n",
    "\n",
    "    # # OUTPUT STATIONARY\n",
    "    # # Number of output block partitions to fit into hardware\n",
    "    # block_col = int(np.ceil(xOutput/ncol))\n",
    "    # block_row = int(np.ceil(knl*knl*ifmap/nrow))\n",
    "\n",
    "    # kernel_flat = np.zeros((block_col*ncol, block_row*nrow))\n",
    "    # kernel_flat[0:ofmap,0:ifmap*knl*knl] = torch.reshape(kernel, [ofmap, ifmap*knl*knl]).cpu().numpy()\n",
    "\n",
    "    # data_moved_count = 0\n",
    "    # mac_completed_count = 0\n",
    "    # batch = 1\n",
    "\n",
    "    # # Process image by image\n",
    "    # for b in range(batch):\n",
    "    #   # Apply Equal Padding to All Sides\n",
    "    #   if padding != 0:\n",
    "    #       imagePadded = np.zeros((idim + padding * 2, idim + padding * 2))\n",
    "    #       imagePadded[int(padding):int(-1 * padding), int(padding):int(-1 * padding)] = image[b]\n",
    "    #   else:\n",
    "    #       imagePadded = image[b]\n",
    "\n",
    "    #   image_block = np.zeros((nrow * block_row, ncol*block_col, yOutput))\n",
    "    #   otemp = torch.zeros((nrow * block_row, ofmap, ncol*block_col, yOutput))\n",
    "\n",
    "    #   for y in range(yOutput):\n",
    "    #      for x in range(xOutput):\n",
    "    #         image_block[0:knl * knl * ifmap, x, y] = imagePadded[0:ifmap,\n",
    "    #                                                       strides * x: strides * x + xKernShape,\n",
    "    #                                                       strides * y: strides * y + yKernShape].reshape(\n",
    "    #                 knl * knl * ifmap).cpu().numpy()\n",
    "\n",
    "    #   # Iterate through image\n",
    "    #   for y in range(yOutput):\n",
    "    #       for bc in range(block_col):\n",
    "    #         for br in range(block_row):\n",
    "    #           data_moved_count += nrow*ncol\n",
    "    #           for kerCol in range(ofmap):\n",
    "    #             data_moved_count += nrow\n",
    "    #             ktemp = kernel_flat[kerCol, br*nrow:(br+1)*nrow]\n",
    "    #             ktemp = torch.from_numpy(ktemp)\n",
    "    #             for x in range(ncol):\n",
    "    #                 mac_completed_count += 1\n",
    "    #                 data_moved_count += 1\n",
    "    #                 # Fetch image section x,y, bc,br\n",
    "    #                 itemp = image_block[br*nrow:(br+1)*nrow, (bc*ncol)+x, y].reshape(1, nrow)\n",
    "    #                 itemp = torch.from_numpy(itemp)\n",
    "    #                 # Replace this line with Arduino SPI function call\n",
    "    #                 otemp[br, kerCol, (bc*ncol)+x, y] = torch.sum(ktemp*itemp, dim=1)\n",
    "    #                 # otemp[kerCol, x, y] = self.non_linear_mult(ktemp, itemp[0])\n",
    "    #                 # print(\"Batch:%d,BROW:%d,BCOL:%d,X:%d,Y:%d\" % (b,br,bc,x,y))\n",
    "    #   output[b] = torch.sum(otemp[0:block_row, 0:ofmap, 0:xOutput, 0:yOutput], dim=0) + (bias.reshape(ofmap, 1, 1).cpu()*torch.ones(xOutput, yOutput)).detach().numpy()\n",
    "    #   print(f\"Moved {data_moved_count} pieces of data, {mac_completed_count} MAC operations completed\")\n",
    "    #   print(ktemp[1000]) # Workaround breakpoint\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "  def relu6(self, x, min_val, max_val, relu_module):\n",
    "    i = (x >= min_val) * x\n",
    "    out_relu = (i <= max_val) * (i - max_val) + max_val\n",
    "    so_relu = relu_module.output_scale\n",
    "\n",
    "    # Apply fake quantization to relu1 output.\n",
    "    out_relu = self.noscale_quant(out_relu, so_relu, 0, self.num_bits)\n",
    "    return self.dequantize(out_relu, so_relu, 0)\n",
    "\n",
    "\n",
    "  def maxpool2d(self, x, idim, ofmap, knl):\n",
    "    # Maxpool layer - downsample by knl x knl with maxpool (no quantization required for max function)\n",
    "    return torch.from_numpy(self.maxpool2D_wsa(self.batch, x, idim, ofmap, knl))\n",
    "\n",
    "\n",
    "  # Maxpool layer (apply to all ofmaps simultaneously - faster!)\n",
    "  def maxpool2D_wsa(self, batch, image, idim, ofmap, knl):\n",
    "    xKernShape = knl\n",
    "    yKernShape = knl\n",
    "    xImgShape = idim\n",
    "    yImgShape = idim\n",
    "    strides = knl\n",
    "    padding = 0\n",
    "    # Shape of Output Convolution\n",
    "    xOutput = int(((xImgShape - xKernShape + 2 * padding) / strides) + 1)\n",
    "    yOutput = int(((yImgShape - yKernShape + 2 * padding) / strides) + 1)\n",
    "    output = np.zeros((batch, ofmap, xOutput, yOutput))\n",
    "\n",
    "    for b in range(batch):\n",
    "        # Apply Equal Padding to All Sides\n",
    "        if padding != 0:\n",
    "            imagePadded = np.zeros((idim + padding * 2, idim + padding * 2))\n",
    "            imagePadded[int(padding):int(-1 * padding), int(padding):int(-1 * padding)] = image[b]\n",
    "        else:\n",
    "            imagePadded = image[b]\n",
    "\n",
    "        # Iterate through image\n",
    "        for y in range(yOutput):\n",
    "            for x in range(xOutput):\n",
    "                output[b, :, x, y] = torch.amax(\n",
    "                        imagePadded[:, strides * x: strides * x + xKernShape, strides * y: strides * y + yKernShape], dim=(1,2)).detach().cpu().numpy()\n",
    "    return output\n",
    "\n",
    "\n",
    "  def avgpool2d(self, x, idim, ofmap, knl):\n",
    "    # Maxpool layer - downsample by knl x knl with maxpool (no quantization required for max function)\n",
    "    return torch.from_numpy(self.avgpool2D_wsa(self.batch, x, idim, ofmap, knl))\n",
    "  \n",
    "\n",
    "  # Maxpool layer (apply to all ofmaps simultaneously - faster!)\n",
    "  def avgpool2D_wsa(self, batch, image, idim, ofmap, knl):\n",
    "    xKernShape = knl\n",
    "    yKernShape = knl\n",
    "    xImgShape = idim\n",
    "    yImgShape = idim\n",
    "    strides = knl\n",
    "    padding = 0\n",
    "    # Shape of Output Convolution\n",
    "    xOutput = int(((xImgShape - xKernShape + 2 * padding) / strides) + 1)\n",
    "    yOutput = int(((yImgShape - yKernShape + 2 * padding) / strides) + 1)\n",
    "    output = np.zeros((batch, ofmap, xOutput, yOutput))\n",
    "\n",
    "    for b in range(batch):\n",
    "        # Apply Equal Padding to All Sides\n",
    "        if padding != 0:\n",
    "            imagePadded = np.zeros((idim + padding * 2, idim + padding * 2))\n",
    "            imagePadded[int(padding):int(-1 * padding), int(padding):int(-1 * padding)] = image[b]\n",
    "        else:\n",
    "            imagePadded = image[b]\n",
    "\n",
    "        # Iterate through image\n",
    "        for y in range(yOutput):\n",
    "            for x in range(xOutput):\n",
    "                for channel in range(ofmap):\n",
    "                  # output[b, channel, x, y] = torch.sum(\n",
    "                  #    imagePadded[channel, strides * x: strides * x + xKernShape, strides * y: strides * y + yKernShape].reshape(knl*knl) \n",
    "                  #    * torch.ones(xKernShape*yKernShape), dim=0)/(knl*knl)\n",
    "                  output[b, channel, x, y] = self.non_linear_mult(\n",
    "                        imagePadded[channel, strides * x: strides * x + xKernShape, strides * y: strides * y + yKernShape].reshape(knl*knl),\n",
    "                        torch.ones(xKernShape*yKernShape))/(knl*knl)\n",
    "    return output\n",
    "\n",
    "\n",
    "  def fc(self, x, module, odim):\n",
    "    sin_fc = module.input_scale.cpu()\n",
    "    in_fcs = self.noscale_quant(x, sin_fc, 0, self.num_bits)\n",
    "    sw_fc, filters_fc = self.scale_quant(module.weight.cpu(), self.num_bits)\n",
    "    bias_fc = module.bias.cpu() / (sw_fc * sin_fc)\n",
    "\n",
    "\n",
    "    # FC layer using (WSA dataflow)\n",
    "    # N.B. Need to implement WSAB function to implement 'hardware acceleration' of FC layer.\n",
    "    #out_fc = torch.from_numpy(self.fc_custom_wsa(in_fcs, filters_fc, bias_fc, self.batch, odim))\n",
    "    out_fc = torch.from_numpy(self.fc_custom_wsab(in_fcs, filters_fc, bias_fc, self.batch, odim, self.ncol, self.nrow))\n",
    "\n",
    "    # FC output scaling\n",
    "    out_fcs = out_fc * sin_fc * sw_fc\n",
    "\n",
    "    so_fc = module.output_scale.cpu()\n",
    "    # FC output fake quantization\n",
    "    return self.dequantize(self.noscale_quant(out_fcs, so_fc, 0, self.num_bits), so_fc, 0).to(self.device)\n",
    "\n",
    "\n",
    "  # 'Fake' Quantization function [Jacob et. al]\n",
    "  def quantize(self, real_value, scale, zero_point, qmin, qmax):\n",
    "    transformed_val = zero_point + real_value / scale\n",
    "    clamped_val = torch.clamp(transformed_val, qmin, qmax)\n",
    "    quantized_val = torch.round(clamped_val)\n",
    "    return quantized_val\n",
    "\n",
    "\n",
    "  # 'Fake' Dequantization function [Jacob et. al]\n",
    "  def dequantize(self, quantized_val, scale, zero_point):\n",
    "    real_val = scale * (quantized_val - zero_point)\n",
    "    return real_val\n",
    "\n",
    "\n",
    "  # Scaling function (Jacob et. al)\n",
    "  def scale_quant(self, real_value, num_bits):\n",
    "    qmin = -(2 ** (num_bits - 1) - 1)\n",
    "    qmax = 2 ** (num_bits - 1) - 1\n",
    "    abs_max = torch.abs(real_value).max()\n",
    "    scale = abs_max / (float(qmax - qmin) / 2)\n",
    "    zero_point = 0\n",
    "    quant = self.quantize(real_value, scale, zero_point, qmin, qmax)\n",
    "    return scale, quant\n",
    "\n",
    "\n",
    "  # Scaling function (Jacob et. al)\n",
    "  def noscale_quant(self, real_value, scale, zero_point, num_bits):\n",
    "    qmin = -(2 ** (num_bits - 1) - 1)\n",
    "    qmax = 2 ** (num_bits - 1) - 1\n",
    "    quant = self.quantize(real_value, scale, zero_point, qmin, qmax)\n",
    "    return quant\n",
    "\n",
    "\n",
    "  # Custom FC layer (vector multiplication style - much faster than for loop implementation)\n",
    "  def fc_custom_wsa(self, fc_input, filters, bias, batch, ofmap):\n",
    "    output = np.zeros((batch, ofmap))\n",
    "    for b in range(batch):\n",
    "        outputi = np.zeros(ofmap)\n",
    "        input_batch = fc_input[b]\n",
    "        outputi = torch.sum((input_batch * filters),dim=1) + bias\n",
    "        output[b, :] = outputi.detach().numpy()\n",
    "    return output\n",
    "  \n",
    "  def fc_custom_wsab(self, fc_input, filters, bias, batch, ofmap, ncol, nrow):\n",
    "    output = np.zeros((batch, ofmap))\n",
    "    for b in range(batch):\n",
    "        # WEIGHT STATIONARY\n",
    "\n",
    "        # Iterate through image\n",
    "        block_col = int(np.ceil(ofmap/ncol))\n",
    "        block_row = int(np.ceil(fc_input.size()[1]/nrow))\n",
    "\n",
    "        filters_flat = np.zeros((block_col*ncol, block_row*nrow), dtype='int64')\n",
    "        image_col = np.zeros((block_row*nrow), dtype='int64')\n",
    "\n",
    "        filters_flat[0:ofmap,0:fc_input.size()[1]] = torch.reshape(filters, [ofmap, fc_input.size()[1]]).cpu().numpy()\n",
    "        image_col[0:fc_input.size()[1]] = fc_input[b, :]\n",
    "\n",
    "        otemp = torch.zeros(block_row, block_col*ncol)\n",
    "\n",
    "        for bc in range(block_col):\n",
    "            for br in range(block_row):\n",
    "                ftemp = torch.zeros(ncol,nrow)\n",
    "                ftemp = filters_flat[ncol*bc:(bc+1)*ncol, nrow*br:(br+1)*nrow]\n",
    "                ftemp = torch.from_numpy(ftemp)\n",
    "\n",
    "                itemp = image_col[br*nrow:(br+1)*nrow].reshape(nrow)\n",
    "                itemp = torch.from_numpy(itemp)\n",
    "\n",
    "                for col in range(ncol):\n",
    "                  # Replace this line with Arduino SPI function call\n",
    "                  otemp[br, (bc*ncol)+col] = torch.sum(ftemp[col, :]*itemp, dim=0)\n",
    "                  #otemp[br, (bc*ncol)+col] = self.non_linear_mult(ftemp[col, :], itemp[0])\n",
    "                  # print(\"Batch:%d,BROW:%d,BCOL:%d,X:%d,Y:%d\" % (b,br,bc,x,y))\n",
    "        output[b] = torch.sum(otemp[:, 0:ofmap], dim=0) + (bias.reshape(ofmap).cpu()*torch.ones(ofmap)).detach().numpy()\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "  # Non-linear look-up\n",
    "  def non_linear_conv(self, kernel, image):\n",
    "    sum = 0\n",
    "    if self.num_bits == 9 or self.num_bits == 8:\n",
    "       bit = 9\n",
    "    else:\n",
    "       bit = 5\n",
    "    for i in range(len(kernel)):\n",
    "      sum = sum + self.nl_mult[int(kernel[i]+(2**(bit-1))-1)][int(image[i]+(2**(bit-1))-1)]\n",
    "    return sum\n",
    "\n",
    "  def non_linear_mult(self, kernel, image):\n",
    "    output = np.empty(len(kernel))\n",
    "    if len(kernel.shape) == 2:\n",
    "      for i in range(len(kernel)):\n",
    "        output[i] = self.non_linear_conv(kernel[i, :].tolist(), image.tolist())\n",
    "      return torch.from_numpy(output)\n",
    "    else:\n",
    "      output = self.non_linear_conv(kernel.tolist(), image.tolist())\n",
    "      return output\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel dimensions (used in conv layers)\n",
    "knl = 5\n",
    "\n",
    "# Kernel stride\n",
    "stride = 1\n",
    "\n",
    "# Output Feature Maps (same as kernels in conv layer 1)\n",
    "ofmap = 16\n",
    "\n",
    "# Kernels in conv layer 2\n",
    "conv2kernels = 32\n",
    "\n",
    "# Maxpool dimensions (assuming square area)\n",
    "maxpool_dim = 2\n",
    "\n",
    "# Fully Connected 1 output nodes\n",
    "fc1_nodes = 50\n",
    "\n",
    "# Hardware MVM columns\n",
    "ncol = 16\n",
    "\n",
    "# Hardware MVM rows\n",
    "nrow = 32\n",
    "\n",
    "# Non-Idealities File\n",
    "non_ideal_file = '../HardwareSpec/4x4_Mac_result_final.xlsx'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom test model function\n",
    "# Only implements forward pass of neural network - no training done.\n",
    "def test_custom(model, device, test_loader, num_bits, idim, ifmap, ofmap, knl, stride,\n",
    "                conv2kernels, maxpool_dim, fc1_nodes, fc2_nodes, non_ideal_file, ncol, nrow):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    i = 1\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            batch = data.size()[0]\n",
    "\n",
    "            # Custom forward pass function\n",
    "            non_linear_model = forward_custom(model, data, batch, num_bits, ifmap, ofmap, idim, knl, stride, conv2kernels,\n",
    "                                              maxpool_dim, fc1_nodes, fc2_nodes, non_ideal_file, ncol, nrow, device)\n",
    "            output = non_linear_model.forward_pass()\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            print(f\"ITERATION {i}: Accuracy = {correct/i:.2f}% (cumulative = {correct})\")\n",
    "            i = i+1\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print(f\"Loss: {test_loss}   Accuracy: {100 * correct / len(test_loader.dataset)}%\\n\")\n",
    "\n",
    "\n",
    "# Test original/default model forward pass.\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print(f\"Loss: {test_loss}   Accuracy: {100 * correct / len(test_loader.dataset)}%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization and Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float32)\n",
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# INT9 weight, INT9 activations\n",
    "num_bits = 5\n",
    "# Make sure this matches quantization config from MNIST_CNN_Training\n",
    "configure_list = [{\n",
    "    'quant_types': ['weight', 'input'],\n",
    "    'quant_bits': {'weight': num_bits, 'input': num_bits},\n",
    "    'quant_start_step': 2,\n",
    "    'op_names': ['conv1', 'conv2']\n",
    "}, {\n",
    "    'quant_types': ['output'],\n",
    "    'quant_bits': {'output': num_bits},\n",
    "    'quant_start_step': 2,\n",
    "    'op_names': ['relu1', 'relu2', 'relu3']\n",
    "}, {\n",
    "    'quant_types': ['output', 'weight', 'input'],\n",
    "    'quant_bits': {'output': num_bits, 'weight': num_bits, 'input': num_bits},\n",
    "    'quant_start_step': 2,\n",
    "    'op_names': ['fc1', 'fc2'],\n",
    "}]\n",
    "\n",
    "set_quant_scheme_dtype('weight', 'per_tensor_symmetric', 'int')\n",
    "set_quant_scheme_dtype('output', 'per_tensor_symmetric', 'int')\n",
    "set_quant_scheme_dtype('input', 'per_tensor_symmetric', 'int')\n",
    "\n",
    "# Load MNIST dataset with train/test split sets.\n",
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=True, download=True, transform=trans),\n",
    "    batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=False, transform=trans),\n",
    "    batch_size=100, shuffle=True)\n",
    "\n",
    "idim = next(iter(train_loader))[0][0].size()[1]\n",
    "ifmap = next(iter(train_loader))[0][0].size()[0]\n",
    "fc2_nodes = len(torch.unique(train_loader.dataset.targets))\n",
    "\n",
    "# Create a NaiveModel object and apply QAT_Quantizer setup\n",
    "model_path = \"models/mnist_model_5bit_AVGPOOL.pth\"\n",
    "qmodel = NaiveModel().to(device)\n",
    "dummy_input = torch.randn(1, ifmap, idim, idim).to(device)\n",
    "optimizer = torch.optim.SGD(qmodel.parameters(), lr=0.01, momentum=0.5)\n",
    "# To enable batch normalization folding in the training process, you should\n",
    "# pass dummy_input to the QAT_Quantizer.\n",
    "quantizer = QAT_Quantizer(qmodel, configure_list, optimizer, dummy_input=dummy_input)\n",
    "quantizer.compress()\n",
    "\n",
    "# Load trained model (from MNIST_CNN_Training step).\n",
    "state = torch.load(model_path, map_location=device)\n",
    "qmodel.load_state_dict(state, strict=True)\n",
    "qmodel.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.04286044540405273   Accuracy: 98.37%\n",
      "\n",
      "ITERATION 1: Accuracy = 100.00% (cumulative = 100)\n",
      "ITERATION 2: Accuracy = 99.00% (cumulative = 198)\n",
      "ITERATION 3: Accuracy = 99.00% (cumulative = 297)\n",
      "ITERATION 4: Accuracy = 98.75% (cumulative = 395)\n",
      "ITERATION 5: Accuracy = 98.60% (cumulative = 493)\n",
      "ITERATION 6: Accuracy = 98.33% (cumulative = 590)\n",
      "ITERATION 7: Accuracy = 97.86% (cumulative = 685)\n",
      "ITERATION 8: Accuracy = 97.50% (cumulative = 780)\n",
      "ITERATION 9: Accuracy = 97.78% (cumulative = 880)\n",
      "ITERATION 10: Accuracy = 97.70% (cumulative = 977)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m test(qmodel, device, test_loader)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Evaluate test accuracy of qmodel, with custom forward pass.\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[43mtest_custom\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mifmap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mofmap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mknl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconv2kernels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxpool_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfc1_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfc2_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_ideal_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mncol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnrow\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[63], line 17\u001b[0m, in \u001b[0;36mtest_custom\u001b[1;34m(model, device, test_loader, num_bits, idim, ifmap, ofmap, knl, stride, conv2kernels, maxpool_dim, fc1_nodes, fc2_nodes, non_ideal_file, ncol, nrow)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Custom forward pass function\u001b[39;00m\n\u001b[0;32m     15\u001b[0m non_linear_model \u001b[38;5;241m=\u001b[39m forward_custom(model, data, batch, num_bits, ifmap, ofmap, idim, knl, stride, conv2kernels,\n\u001b[0;32m     16\u001b[0m                                   maxpool_dim, fc1_nodes, fc2_nodes, non_ideal_file, ncol, nrow, device)\n\u001b[1;32m---> 17\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mnon_linear_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m test_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnll_loss(output, target, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     19\u001b[0m pred \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[61], line 40\u001b[0m, in \u001b[0;36mforward_custom.forward_pass\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     37\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu6(x, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mrelu1\u001b[38;5;241m.\u001b[39mmodule)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Maxpool layer 1\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mavgpool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43midim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mknl\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mofmap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaxpool_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Scale for convolutional layer 2\u001b[39;00m\n\u001b[0;32m     43\u001b[0m sin_conv2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconv2\u001b[38;5;241m.\u001b[39mmodule\u001b[38;5;241m.\u001b[39minput_scale\n",
      "Cell \u001b[1;32mIn[61], line 248\u001b[0m, in \u001b[0;36mforward_custom.avgpool2d\u001b[1;34m(self, x, idim, ofmap, knl)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mavgpool2d\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, idim, ofmap, knl):\n\u001b[0;32m    247\u001b[0m   \u001b[38;5;66;03m# Maxpool layer - downsample by knl x knl with maxpool (no quantization required for max function)\u001b[39;00m\n\u001b[1;32m--> 248\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mavgpool2D_wsa\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mofmap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mknl\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[61], line 281\u001b[0m, in \u001b[0;36mforward_custom.avgpool2D_wsa\u001b[1;34m(self, batch, image, idim, ofmap, knl)\u001b[0m\n\u001b[0;32m    274\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(xOutput):\n\u001b[0;32m    275\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m channel \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(ofmap):\n\u001b[0;32m    276\u001b[0m               \u001b[38;5;66;03m# output[b, channel, x, y] = torch.sum(\u001b[39;00m\n\u001b[0;32m    277\u001b[0m               \u001b[38;5;66;03m#    imagePadded[channel, strides * x: strides * x + xKernShape, strides * y: strides * y + yKernShape].reshape(knl*knl) \u001b[39;00m\n\u001b[0;32m    278\u001b[0m               \u001b[38;5;66;03m#    * torch.ones(xKernShape*yKernShape), dim=0)/(knl*knl)\u001b[39;00m\n\u001b[0;32m    279\u001b[0m               output[b, channel, x, y] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnon_linear_mult(\n\u001b[0;32m    280\u001b[0m                     imagePadded[channel, strides \u001b[38;5;241m*\u001b[39m x: strides \u001b[38;5;241m*\u001b[39m x \u001b[38;5;241m+\u001b[39m xKernShape, strides \u001b[38;5;241m*\u001b[39m y: strides \u001b[38;5;241m*\u001b[39m y \u001b[38;5;241m+\u001b[39m yKernShape]\u001b[38;5;241m.\u001b[39mreshape(knl\u001b[38;5;241m*\u001b[39mknl),\n\u001b[1;32m--> 281\u001b[0m                     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxKernShape\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43myKernShape\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m/\u001b[39m(knl\u001b[38;5;241m*\u001b[39mknl)\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Evaluate test accuracy with imported quantized model (qmodel) from mnist_model.pth.\n",
    "test(qmodel, device, test_loader)\n",
    "\n",
    "# Evaluate test accuracy of qmodel, with custom forward pass.\n",
    "test_custom(qmodel, device, test_loader, num_bits, idim, ifmap, ofmap, knl, stride,\n",
    "            conv2kernels, maxpool_dim, fc1_nodes, fc2_nodes, non_ideal_file, ncol, nrow)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
