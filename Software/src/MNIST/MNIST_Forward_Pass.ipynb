{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST FORWARD PASS\n",
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from forwardPass import forwardPass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom test model function\n",
    "# Only implements forward pass of neural network - no training done.\n",
    "def test_custom(model, device, test_loader):\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    i = 1\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            batch = data.size()[0]\n",
    "\n",
    "            # Custom forward pass function\n",
    "            output = model.forward_pass(data, batch)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            print(f\"ITERATION {i}: Accuracy = {correct/i:.2f}% (cumulative = {correct})\")\n",
    "            i = i+1\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print(f\"Loss: {test_loss}   Accuracy: {100 * correct / len(test_loader.dataset)}%\\n\")\n",
    "\n",
    "\n",
    "# Test original/default model forward pass.\n",
    "def test(model, device, test_loader):\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model.model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print(f\"Loss: {test_loss}   Accuracy: {100 * correct / len(test_loader.dataset)}%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float32)\n",
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load MNIST dataset with train/test split sets.\n",
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=True, download=True, transform=trans),\n",
    "    batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=False, transform=trans),\n",
    "    batch_size=100, shuffle=True)\n",
    "\n",
    "idim = next(iter(train_loader))[0][0].size()[1]\n",
    "ifmap = next(iter(train_loader))[0][0].size()[0]\n",
    "fc2_nodes = len(torch.unique(train_loader.dataset.targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardware Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for quantized and trained model\n",
    "model_path = \"models/mnist_model_8bit.pth\"\n",
    "\n",
    "# Choose from: 'array', 'ppq_array', 'mismatched_array'\n",
    "array_choice = 'mismatched_array'\n",
    "\n",
    "# Choose from: 'linear', 'from_table', 'gain_nl', 'noise' (Ignored if using 'mismatched array')\n",
    "mac_choice = 'gain_nl'\n",
    "\n",
    "# MAC_CONSTANTS - What to set it to depends on above choices:\n",
    "#\n",
    "# array_choice = 'mismatched_array' - Set to percentage mismatch (2)\n",
    "# \n",
    "# mac_choice = 'from_table'         - Set to .xlsx file path containing MAC results ('../HardwareSpec/8x8_Mac_result_final.xlsx')\n",
    "# mac_choice = 'gain_nl'            - Set to the gain/non-linearity constants in an array: [a1, a3] ([1.1, 1*10**-5])\n",
    "# mac_choice = 'noise'              - Set to noise sigma value (1)\n",
    "mac_constants = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize hardware model\n",
    "non_linear_model = forwardPass(device, idim, ifmap, array_choice, mac_choice, mac_constants, model_path)\n",
    "\n",
    "# Evaluate test accuracy with imported quantized model from mnist_model.pth.\n",
    "test(non_linear_model, device, test_loader)\n",
    "\n",
    "# Evaluate test accuracy of qmodel, with custom forward pass.\n",
    "test_custom(non_linear_model, device, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
