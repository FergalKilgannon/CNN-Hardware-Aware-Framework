{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10 TRAINING\n",
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# NNI package for model quantization aware training (QAT).\n",
    "from nni.algorithms.compression.pytorch.quantization import QAT_Quantizer\n",
    "from nni.compression.pytorch.quantization.settings import set_quant_scheme_dtype\n",
    "\n",
    "# Import NaiveModel from naive.py\n",
    "from naive_cifar import NaiveModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training function\n",
    "def train(model, device, train_loader, optimizer):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"{100 * batch_idx / len(train_loader):2.0f}%     Loss {loss.item()}\")\n",
    "\n",
    "\n",
    "# Model testing function\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print(f\"Loss: {test_loss}   Accuracy: {100 * correct / len(test_loader.dataset)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "# Epoch 0 #\n",
      " 0%     Loss 2.2997584342956543\n",
      "13%     Loss 2.3074707984924316\n",
      "26%     Loss 2.298013210296631\n",
      "38%     Loss 2.3073699474334717\n",
      "51%     Loss 2.30338978767395\n",
      "64%     Loss 2.3042500019073486\n",
      "77%     Loss 2.2996866703033447\n",
      "90%     Loss 2.3027172088623047\n",
      "Loss: 2.3024109375   Accuracy: 10.0%\n",
      "# Epoch 1 #\n",
      " 0%     Loss 2.301466703414917\n",
      "13%     Loss 2.3045079708099365\n",
      "26%     Loss 2.301804542541504\n",
      "38%     Loss 2.2996795177459717\n",
      "51%     Loss 2.302232503890991\n",
      "64%     Loss 2.30591082572937\n",
      "77%     Loss 2.3016583919525146\n",
      "90%     Loss 2.300405979156494\n",
      "Loss: 2.30155078125   Accuracy: 10.0%\n",
      "# Epoch 2 #\n",
      " 0%     Loss 2.3017849922180176\n",
      "13%     Loss 2.2975661754608154\n",
      "26%     Loss 2.299680233001709\n",
      "38%     Loss 2.3004846572875977\n",
      "51%     Loss 2.301600933074951\n",
      "64%     Loss 2.298483371734619\n",
      "77%     Loss 2.294667959213257\n",
      "90%     Loss 2.280942678451538\n",
      "Loss: 2.259526953125   Accuracy: 20.88%\n",
      "# Epoch 3 #\n",
      " 0%     Loss 2.278991222381592\n",
      "13%     Loss 2.245361089706421\n",
      "26%     Loss 2.1369357109069824\n",
      "38%     Loss 2.0923569202423096\n",
      "51%     Loss 2.0348238945007324\n",
      "64%     Loss 2.0633769035339355\n",
      "77%     Loss 1.962725043296814\n",
      "90%     Loss 2.086843490600586\n",
      "Loss: 1.8947178344726563   Accuracy: 27.87%\n",
      "# Epoch 4 #\n",
      " 0%     Loss 1.9662216901779175\n",
      "13%     Loss 1.807112455368042\n",
      "26%     Loss 1.785891056060791\n",
      "38%     Loss 1.7723281383514404\n",
      "51%     Loss 1.7043678760528564\n",
      "64%     Loss 1.8090275526046753\n",
      "77%     Loss 1.861524224281311\n",
      "90%     Loss 1.6742069721221924\n",
      "Loss: 1.700695068359375   Accuracy: 37.16%\n",
      "# Epoch 5 #\n",
      " 0%     Loss 1.7416439056396484\n",
      "13%     Loss 1.7976717948913574\n",
      "26%     Loss 1.6629422903060913\n",
      "38%     Loss 1.6832959651947021\n",
      "51%     Loss 1.7737092971801758\n",
      "64%     Loss 1.7049388885498047\n",
      "77%     Loss 1.584017276763916\n",
      "90%     Loss 1.876777172088623\n",
      "Loss: 1.6002196411132812   Accuracy: 39.98%\n",
      "# Epoch 6 #\n",
      " 0%     Loss 1.6408159732818604\n",
      "13%     Loss 1.771270751953125\n",
      "26%     Loss 1.609537124633789\n",
      "38%     Loss 1.5793888568878174\n",
      "51%     Loss 1.6773746013641357\n",
      "64%     Loss 1.7000732421875\n",
      "77%     Loss 1.8901019096374512\n",
      "90%     Loss 1.7811065912246704\n",
      "Loss: 1.523928857421875   Accuracy: 42.9%\n",
      "# Epoch 7 #\n",
      " 0%     Loss 1.534292221069336\n",
      "13%     Loss 1.7398724555969238\n",
      "26%     Loss 1.438220739364624\n",
      "38%     Loss 1.666309118270874\n",
      "51%     Loss 1.4303312301635742\n",
      "64%     Loss 1.5260963439941406\n",
      "77%     Loss 1.7821259498596191\n",
      "90%     Loss 1.5236459970474243\n",
      "Loss: 1.435507958984375   Accuracy: 45.93%\n",
      "# Epoch 8 #\n",
      " 0%     Loss 1.541426181793213\n",
      "13%     Loss 1.494970679283142\n",
      "26%     Loss 1.3104490041732788\n",
      "38%     Loss 1.41249418258667\n",
      "51%     Loss 1.2836518287658691\n",
      "64%     Loss 1.5368072986602783\n",
      "77%     Loss 1.425997257232666\n",
      "90%     Loss 1.471813678741455\n",
      "Loss: 1.4236902587890625   Accuracy: 47.65%\n",
      "# Epoch 9 #\n",
      " 0%     Loss 1.3902357816696167\n",
      "13%     Loss 1.4087210893630981\n",
      "26%     Loss 1.3647994995117188\n",
      "38%     Loss 1.630807876586914\n",
      "51%     Loss 1.3601964712142944\n",
      "64%     Loss 1.330309510231018\n",
      "77%     Loss 1.332184076309204\n",
      "90%     Loss 1.3024834394454956\n",
      "Loss: 1.3817637817382813   Accuracy: 49.36%\n",
      "# Epoch 10 #\n",
      " 0%     Loss 1.6159679889678955\n",
      "13%     Loss 1.351434588432312\n",
      "26%     Loss 1.5432789325714111\n",
      "38%     Loss 1.4240572452545166\n",
      "51%     Loss 1.3605715036392212\n",
      "64%     Loss 1.3857511281967163\n",
      "77%     Loss 1.5940138101577759\n",
      "90%     Loss 1.4764957427978516\n",
      "Loss: 1.3171111938476563   Accuracy: 50.8%\n",
      "# Epoch 11 #\n",
      " 0%     Loss 1.2767109870910645\n",
      "13%     Loss 1.3455489873886108\n",
      "26%     Loss 1.383223533630371\n",
      "38%     Loss 1.2217634916305542\n",
      "51%     Loss 1.439559817314148\n",
      "64%     Loss 1.3452397584915161\n",
      "77%     Loss 1.3914321660995483\n",
      "90%     Loss 1.4283888339996338\n",
      "Loss: 1.2443928100585937   Accuracy: 55.11%\n",
      "# Epoch 12 #\n",
      " 0%     Loss 1.4333081245422363\n",
      "13%     Loss 1.0796595811843872\n",
      "26%     Loss 1.1979782581329346\n",
      "38%     Loss 1.493675708770752\n",
      "51%     Loss 1.1877117156982422\n",
      "64%     Loss 1.177464485168457\n",
      "77%     Loss 1.5388025045394897\n",
      "90%     Loss 1.6841437816619873\n",
      "Loss: 1.4401324340820312   Accuracy: 48.43%\n",
      "# Epoch 13 #\n",
      " 0%     Loss 1.084565281867981\n",
      "13%     Loss 1.3573912382125854\n",
      "26%     Loss 1.540292739868164\n",
      "38%     Loss 1.223517656326294\n",
      "51%     Loss 1.3556668758392334\n",
      "64%     Loss 1.3938905000686646\n",
      "77%     Loss 1.269880771636963\n",
      "90%     Loss 1.0810494422912598\n",
      "Loss: 1.1906620971679687   Accuracy: 56.47%\n",
      "# Epoch 14 #\n",
      " 0%     Loss 1.1527068614959717\n",
      "13%     Loss 1.044828176498413\n",
      "26%     Loss 1.308211326599121\n",
      "38%     Loss 1.3604081869125366\n",
      "51%     Loss 1.2623051404953003\n",
      "64%     Loss 1.4132699966430664\n",
      "77%     Loss 1.0768405199050903\n",
      "90%     Loss 1.2737478017807007\n",
      "Loss: 1.1479124389648439   Accuracy: 58.26%\n",
      "# Epoch 15 #\n",
      " 0%     Loss 1.3599528074264526\n",
      "13%     Loss 1.120808482170105\n",
      "26%     Loss 1.170899510383606\n",
      "38%     Loss 1.2908891439437866\n",
      "51%     Loss 1.3622184991836548\n",
      "64%     Loss 1.304222583770752\n",
      "77%     Loss 1.457735538482666\n",
      "90%     Loss 1.1489163637161255\n",
      "Loss: 1.1119975708007812   Accuracy: 60.27%\n",
      "# Epoch 16 #\n",
      " 0%     Loss 1.1880351305007935\n",
      "13%     Loss 1.1167696714401245\n",
      "26%     Loss 1.3092308044433594\n",
      "38%     Loss 1.1328225135803223\n",
      "51%     Loss 1.2984168529510498\n",
      "64%     Loss 1.1719728708267212\n",
      "77%     Loss 1.1434788703918457\n",
      "90%     Loss 1.1558598279953003\n",
      "Loss: 1.0182656799316405   Accuracy: 64.12%\n",
      "# Epoch 17 #\n",
      " 0%     Loss 1.2895089387893677\n",
      "13%     Loss 0.9895328283309937\n",
      "26%     Loss 1.1551871299743652\n",
      "38%     Loss 1.1823652982711792\n",
      "51%     Loss 1.2659111022949219\n",
      "64%     Loss 1.3738361597061157\n",
      "77%     Loss 1.0882554054260254\n",
      "90%     Loss 1.0228475332260132\n",
      "Loss: 1.0030629516601564   Accuracy: 63.55%\n",
      "# Epoch 18 #\n",
      " 0%     Loss 1.1292463541030884\n",
      "13%     Loss 1.049067735671997\n",
      "26%     Loss 0.9763988852500916\n",
      "38%     Loss 1.3065290451049805\n",
      "51%     Loss 1.2008552551269531\n",
      "64%     Loss 1.148566484451294\n",
      "77%     Loss 0.9783414006233215\n",
      "90%     Loss 1.294123649597168\n",
      "Loss: 0.9517702880859376   Accuracy: 66.11%\n",
      "# Epoch 19 #\n",
      " 0%     Loss 1.1086325645446777\n",
      "13%     Loss 0.9873861074447632\n",
      "26%     Loss 1.2556999921798706\n",
      "38%     Loss 1.2311574220657349\n",
      "51%     Loss 1.0683516263961792\n",
      "64%     Loss 1.0682953596115112\n",
      "77%     Loss 0.965225875377655\n",
      "90%     Loss 1.1457756757736206\n",
      "Loss: 0.9398102111816407   Accuracy: 66.86%\n",
      "# Epoch 20 #\n",
      " 0%     Loss 1.16414213180542\n",
      "13%     Loss 1.2410510778427124\n",
      "26%     Loss 0.9160990118980408\n",
      "38%     Loss 1.0057696104049683\n",
      "51%     Loss 0.9941698908805847\n",
      "64%     Loss 1.0312553644180298\n",
      "77%     Loss 1.039916753768921\n",
      "90%     Loss 1.1490763425827026\n",
      "Loss: 0.9138737670898438   Accuracy: 67.95%\n",
      "# Epoch 21 #\n",
      " 0%     Loss 1.0956664085388184\n",
      "13%     Loss 0.9987689256668091\n",
      "26%     Loss 0.9468065500259399\n",
      "38%     Loss 1.0316805839538574\n",
      "51%     Loss 0.8901987671852112\n",
      "64%     Loss 0.9020900130271912\n",
      "77%     Loss 0.9618434906005859\n",
      "90%     Loss 0.83719801902771\n",
      "Loss: 0.8493930969238281   Accuracy: 69.95%\n",
      "# Epoch 22 #\n",
      " 0%     Loss 0.9614969491958618\n",
      "13%     Loss 1.1429647207260132\n",
      "26%     Loss 1.012734293937683\n",
      "38%     Loss 0.9802530407905579\n",
      "51%     Loss 0.6576969623565674\n",
      "64%     Loss 1.0705362558364868\n",
      "77%     Loss 1.0213122367858887\n",
      "90%     Loss 1.0498703718185425\n",
      "Loss: 0.886246923828125   Accuracy: 68.66%\n",
      "# Epoch 23 #\n",
      " 0%     Loss 1.1632750034332275\n",
      "13%     Loss 1.1358463764190674\n",
      "26%     Loss 1.2419673204421997\n",
      "38%     Loss 0.9075167179107666\n",
      "51%     Loss 1.175667643547058\n",
      "64%     Loss 0.8234450221061707\n",
      "77%     Loss 0.858607292175293\n",
      "90%     Loss 0.8819742202758789\n",
      "Loss: 0.8356074584960937   Accuracy: 69.78%\n",
      "# Epoch 24 #\n",
      " 0%     Loss 1.0604560375213623\n",
      "13%     Loss 1.0671906471252441\n",
      "26%     Loss 0.7580121755599976\n",
      "38%     Loss 1.3167036771774292\n",
      "51%     Loss 0.726828932762146\n",
      "64%     Loss 0.7895032167434692\n",
      "77%     Loss 0.8216652274131775\n",
      "90%     Loss 0.8257759809494019\n",
      "Loss: 0.83556552734375   Accuracy: 70.59%\n",
      "# Epoch 25 #\n",
      " 0%     Loss 1.1280401945114136\n",
      "13%     Loss 0.8987427949905396\n",
      "26%     Loss 0.739687979221344\n",
      "38%     Loss 1.0280067920684814\n",
      "51%     Loss 0.7094608545303345\n",
      "64%     Loss 0.8016082048416138\n",
      "77%     Loss 1.0525232553482056\n",
      "90%     Loss 0.9698533415794373\n",
      "Loss: 0.8090756408691406   Accuracy: 71.54%\n",
      "# Epoch 26 #\n",
      " 0%     Loss 1.0237269401550293\n",
      "13%     Loss 0.7084705829620361\n",
      "26%     Loss 0.8113858699798584\n",
      "38%     Loss 0.9394161105155945\n",
      "51%     Loss 0.6972898244857788\n",
      "64%     Loss 0.9952263832092285\n",
      "77%     Loss 1.0033260583877563\n",
      "90%     Loss 0.878381073474884\n",
      "Loss: 0.7989670959472657   Accuracy: 72.03%\n",
      "# Epoch 27 #\n",
      " 0%     Loss 1.235114336013794\n",
      "13%     Loss 0.7118155360221863\n",
      "26%     Loss 0.8755887746810913\n",
      "38%     Loss 1.148960828781128\n",
      "51%     Loss 1.0380192995071411\n",
      "64%     Loss 0.9342187643051147\n",
      "77%     Loss 0.8836008310317993\n",
      "90%     Loss 0.8387967944145203\n",
      "Loss: 0.8177224365234375   Accuracy: 71.22%\n",
      "# Epoch 28 #\n",
      " 0%     Loss 0.9341819882392883\n",
      "13%     Loss 0.8326628804206848\n",
      "26%     Loss 0.5500048995018005\n",
      "38%     Loss 0.7546646595001221\n",
      "51%     Loss 0.9311453700065613\n",
      "64%     Loss 0.6931527853012085\n",
      "77%     Loss 0.8393850922584534\n",
      "90%     Loss 0.8106677532196045\n",
      "Loss: 0.7472719360351563   Accuracy: 73.73%\n",
      "# Epoch 29 #\n",
      " 0%     Loss 0.8819782137870789\n",
      "13%     Loss 1.0018709897994995\n",
      "26%     Loss 0.7243936657905579\n",
      "38%     Loss 0.955030083656311\n",
      "51%     Loss 0.9226639270782471\n",
      "64%     Loss 0.7033944725990295\n",
      "77%     Loss 0.7528268694877625\n",
      "90%     Loss 1.06449556350708\n",
      "Loss: 0.7217816833496093   Accuracy: 75.07%\n",
      "# Epoch 30 #\n",
      " 0%     Loss 0.7503373026847839\n",
      "13%     Loss 0.9293643236160278\n",
      "26%     Loss 0.7337161302566528\n",
      "38%     Loss 0.7702601552009583\n",
      "51%     Loss 0.7978109121322632\n",
      "64%     Loss 0.8983439803123474\n",
      "77%     Loss 0.8655122518539429\n",
      "90%     Loss 0.7331295609474182\n",
      "Loss: 0.7000930480957032   Accuracy: 75.52%\n",
      "# Epoch 31 #\n",
      " 0%     Loss 0.8484392762184143\n",
      "13%     Loss 0.5915206670761108\n",
      "26%     Loss 0.6963942050933838\n",
      "38%     Loss 0.948006808757782\n",
      "51%     Loss 0.9409198760986328\n",
      "64%     Loss 0.7018975615501404\n",
      "77%     Loss 0.7059840559959412\n",
      "90%     Loss 0.8014655709266663\n",
      "Loss: 0.7073329345703125   Accuracy: 75.64%\n",
      "# Epoch 32 #\n",
      " 0%     Loss 1.0399872064590454\n",
      "13%     Loss 0.8857859373092651\n",
      "26%     Loss 0.5240369439125061\n",
      "38%     Loss 0.8204027414321899\n",
      "51%     Loss 0.6486903429031372\n",
      "64%     Loss 0.671979546546936\n",
      "77%     Loss 0.6322207450866699\n",
      "90%     Loss 0.5893351435661316\n",
      "Loss: 0.7041419738769531   Accuracy: 75.19%\n",
      "# Epoch 33 #\n",
      " 0%     Loss 0.6082005500793457\n",
      "13%     Loss 0.7395445704460144\n",
      "26%     Loss 0.7474877238273621\n",
      "38%     Loss 0.9579089879989624\n",
      "51%     Loss 0.7397544384002686\n",
      "64%     Loss 0.9611517786979675\n",
      "77%     Loss 0.9720414876937866\n",
      "90%     Loss 0.6668194532394409\n",
      "Loss: 0.6947312744140625   Accuracy: 75.7%\n",
      "# Epoch 34 #\n",
      " 0%     Loss 0.5856415629386902\n",
      "13%     Loss 0.6427843570709229\n",
      "26%     Loss 0.9024448990821838\n",
      "38%     Loss 0.6830345392227173\n",
      "51%     Loss 0.8657631278038025\n",
      "64%     Loss 0.6110091805458069\n",
      "77%     Loss 0.6242260932922363\n",
      "90%     Loss 0.8792635798454285\n",
      "Loss: 0.6666803100585937   Accuracy: 76.69%\n",
      "# Epoch 35 #\n",
      " 0%     Loss 0.8155965805053711\n",
      "13%     Loss 0.7524585127830505\n",
      "26%     Loss 0.7749619483947754\n",
      "38%     Loss 0.988923192024231\n",
      "51%     Loss 0.5871638655662537\n",
      "64%     Loss 0.7481750845909119\n",
      "77%     Loss 1.3187925815582275\n",
      "90%     Loss 0.6062650680541992\n",
      "Loss: 0.7199245422363282   Accuracy: 75.62%\n",
      "# Epoch 36 #\n",
      " 0%     Loss 0.7599605321884155\n",
      "13%     Loss 0.6432617902755737\n",
      "26%     Loss 0.710252046585083\n",
      "38%     Loss 0.8181452751159668\n",
      "51%     Loss 0.7480279803276062\n",
      "64%     Loss 0.8426312208175659\n",
      "77%     Loss 0.6099661588668823\n",
      "90%     Loss 0.9371205568313599\n",
      "Loss: 0.6766234497070313   Accuracy: 76.35%\n",
      "# Epoch 37 #\n",
      " 0%     Loss 0.6233447194099426\n",
      "13%     Loss 0.58356773853302\n",
      "26%     Loss 0.8620122075080872\n",
      "38%     Loss 0.8139632344245911\n",
      "51%     Loss 0.760296106338501\n",
      "64%     Loss 0.8274970650672913\n",
      "77%     Loss 0.6815803647041321\n",
      "90%     Loss 0.5907180309295654\n",
      "Loss: 0.6816962524414063   Accuracy: 76.44%\n",
      "# Epoch 38 #\n",
      " 0%     Loss 0.6060699224472046\n",
      "13%     Loss 0.49123477935791016\n",
      "26%     Loss 0.7745320796966553\n",
      "38%     Loss 0.43212348222732544\n",
      "51%     Loss 0.6221145391464233\n",
      "64%     Loss 0.5811949372291565\n",
      "77%     Loss 0.8368826508522034\n",
      "90%     Loss 0.7898685932159424\n",
      "Loss: 0.6696992492675782   Accuracy: 77.09%\n",
      "# Epoch 39 #\n",
      " 0%     Loss 0.6188036799430847\n",
      "13%     Loss 0.661458432674408\n",
      "26%     Loss 0.7635484933853149\n",
      "38%     Loss 0.5558070540428162\n",
      "51%     Loss 0.5744845271110535\n",
      "64%     Loss 0.593559205532074\n",
      "77%     Loss 0.6658560633659363\n",
      "90%     Loss 0.8836879730224609\n",
      "Loss: 0.732287548828125   Accuracy: 74.71%\n",
      "# Epoch 40 #\n",
      " 0%     Loss 0.640870213508606\n",
      "13%     Loss 0.8241790533065796\n",
      "26%     Loss 0.5534864664077759\n",
      "38%     Loss 0.5803808569908142\n",
      "51%     Loss 0.5129209160804749\n",
      "64%     Loss 0.8063440918922424\n",
      "77%     Loss 0.7017911076545715\n",
      "90%     Loss 0.9717395305633545\n",
      "Loss: 0.6432016967773437   Accuracy: 77.81%\n",
      "# Epoch 41 #\n",
      " 0%     Loss 0.5415233969688416\n",
      "13%     Loss 0.7709428668022156\n",
      "26%     Loss 0.6349648833274841\n",
      "38%     Loss 0.7005513906478882\n",
      "51%     Loss 0.730131208896637\n",
      "64%     Loss 0.9801945686340332\n",
      "77%     Loss 0.733111560344696\n",
      "90%     Loss 0.637141764163971\n",
      "Loss: 0.6519667114257812   Accuracy: 77.78%\n",
      "# Epoch 42 #\n",
      " 0%     Loss 0.5498766303062439\n",
      "13%     Loss 0.4916636347770691\n",
      "26%     Loss 0.44901612401008606\n",
      "38%     Loss 0.6892428398132324\n",
      "51%     Loss 0.9388614296913147\n",
      "64%     Loss 0.911432147026062\n",
      "77%     Loss 0.8097265362739563\n",
      "90%     Loss 0.7103301882743835\n",
      "Loss: 0.6106752746582031   Accuracy: 79.06%\n",
      "# Epoch 43 #\n",
      " 0%     Loss 0.6926547884941101\n",
      "13%     Loss 0.632036030292511\n",
      "26%     Loss 0.769247829914093\n",
      "38%     Loss 0.5865623354911804\n",
      "51%     Loss 0.7894075512886047\n",
      "64%     Loss 0.6986912488937378\n",
      "77%     Loss 0.7116549015045166\n",
      "90%     Loss 0.7949851155281067\n",
      "Loss: 0.5894846618652344   Accuracy: 79.55%\n",
      "# Epoch 44 #\n",
      " 0%     Loss 0.9698008298873901\n",
      "13%     Loss 0.6245595812797546\n",
      "26%     Loss 0.6930396556854248\n",
      "38%     Loss 0.9094337821006775\n",
      "51%     Loss 0.6023460030555725\n",
      "64%     Loss 0.5137200355529785\n",
      "77%     Loss 0.8223174810409546\n",
      "90%     Loss 0.6810590624809265\n",
      "Loss: 0.5883501708984376   Accuracy: 79.73%\n",
      "# Epoch 45 #\n",
      " 0%     Loss 0.5457139015197754\n",
      "13%     Loss 0.6554729342460632\n",
      "26%     Loss 0.5247061252593994\n",
      "38%     Loss 0.7221671938896179\n",
      "51%     Loss 0.6993359327316284\n",
      "64%     Loss 0.7892813086509705\n",
      "77%     Loss 0.6930553913116455\n",
      "90%     Loss 0.6165263056755066\n",
      "Loss: 0.6086792053222656   Accuracy: 78.67%\n",
      "# Epoch 46 #\n",
      " 0%     Loss 0.7248032093048096\n",
      "13%     Loss 0.49076151847839355\n",
      "26%     Loss 0.5655796527862549\n",
      "38%     Loss 0.8065011501312256\n",
      "51%     Loss 0.47279152274131775\n",
      "64%     Loss 0.6027851700782776\n",
      "77%     Loss 0.5853588581085205\n",
      "90%     Loss 0.5390951633453369\n",
      "Loss: 0.6067543701171875   Accuracy: 78.65%\n",
      "# Epoch 47 #\n",
      " 0%     Loss 0.5790417194366455\n",
      "13%     Loss 0.7010396718978882\n",
      "26%     Loss 0.7138833999633789\n",
      "38%     Loss 0.6068829894065857\n",
      "51%     Loss 0.6041443943977356\n",
      "64%     Loss 0.6656901240348816\n",
      "77%     Loss 0.6984483003616333\n",
      "90%     Loss 0.6213496327400208\n",
      "Loss: 0.571453173828125   Accuracy: 80.21%\n",
      "# Epoch 48 #\n",
      " 0%     Loss 0.6611132621765137\n",
      "13%     Loss 0.7373940944671631\n",
      "26%     Loss 0.572325587272644\n",
      "38%     Loss 0.5485939979553223\n",
      "51%     Loss 0.42674076557159424\n",
      "64%     Loss 0.6936177015304565\n",
      "77%     Loss 0.5841429829597473\n",
      "90%     Loss 0.6152475476264954\n",
      "Loss: 0.5888698181152344   Accuracy: 79.52%\n",
      "# Epoch 49 #\n",
      " 0%     Loss 0.7132042646408081\n",
      "13%     Loss 0.5650156736373901\n",
      "26%     Loss 0.578993558883667\n",
      "38%     Loss 0.5846653580665588\n",
      "51%     Loss 0.5458382964134216\n",
      "64%     Loss 0.7342584133148193\n",
      "77%     Loss 0.4152977764606476\n",
      "90%     Loss 0.639145016670227\n",
      "Loss: 0.5768981811523437   Accuracy: 79.74%\n",
      "Loss: 0.57689814453125   Accuracy: 79.74%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Split dataset into train/test sets.\n",
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('data', train=True, download=True, transform=trans),\n",
    "    batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('data', train=False, transform=trans),\n",
    "    batch_size=1000, shuffle=True)\n",
    "\n",
    "\n",
    "idim = next(iter(train_loader))[0][0].size()[1]\n",
    "ifmap = next(iter(train_loader))[0][0].size()[0]\n",
    "\n",
    "# Two things should be kept in mind when set this configure_list:\n",
    "# 1. When deploying model on backend, some layers will be fused into one layer. For example, the consecutive\n",
    "# conv + bn + relu layers will be fused into one big layer. If we want to execute the big layer in quantization\n",
    "# mode, we should tell the backend the quantization information of the input, output, and the weight tensor of\n",
    "# the big layer, which correspond to conv's input, conv's weight and relu's output.\n",
    "# 2. Same tensor should be quantized only once. For example, if a tensor is the output of layer A and the input\n",
    "# of the layer B, you should configure either {'quant_types': ['output'], 'op_names': ['a']} or\n",
    "# {'quant_types': ['input'], 'op_names': ['b']} in the configure_list.\n",
    "\n",
    "# Quantization configuration -\n",
    "# conv1, conv2 layers have INT8 weight, INT8 input activations.\n",
    "# relu layers have INT8 output activations\n",
    "# FC layers have INT8 weight, input, output activations\n",
    "\n",
    "# INT9 weight, INT9 activations\n",
    "num_bits = 16\n",
    "\n",
    "configure_list = [{\n",
    "    'quant_types': ['weight', 'input'],\n",
    "    'quant_bits': {'weight': num_bits, 'input': num_bits},\n",
    "    'quant_start_step': 2,\n",
    "    'op_names': ['conv1', 'conv2', 'conv3', 'conv4', 'conv5', 'conv6']\n",
    "}, {\n",
    "    'quant_types': ['output'],\n",
    "    'quant_bits': {'output': num_bits},\n",
    "    'quant_start_step': 2,\n",
    "    'op_names': ['relu1', 'relu2', 'relu3', 'relu4', 'relu5', 'relu6', 'relu7']\n",
    "}, {\n",
    "    'quant_types': ['output', 'weight', 'input'],\n",
    "    'quant_bits': {'output': num_bits, 'weight': num_bits, 'input': num_bits},\n",
    "    'quant_start_step': 2,\n",
    "    'op_names': ['fc1', 'fc2'],\n",
    "}]\n",
    "\n",
    "# you can also set the quantization dtype and scheme layer-wise through configure_list like:\n",
    "# configure_list = [{\n",
    "#         'quant_types': ['weight', 'input'],\n",
    "#         'quant_bits': {'weight': 8, 'input': 8},\n",
    "#         'op_names': ['conv1', 'conv2'],\n",
    "#         'quant_dtype': 'int',\n",
    "#         'quant_scheme': 'per_channel_symmetric'\n",
    "#       }]\n",
    "# For now quant_dtype's options are 'int' and 'uint. And quant_scheme's options are per_tensor_affine,\n",
    "# per_tensor_symmetric, per_channel_affine and per_channel_symmetric.\n",
    "\n",
    "# per_tensor_symmetric quantization scheme - see [Jacob et. al]\n",
    "set_quant_scheme_dtype('weight', 'per_tensor_symmetric', 'int')\n",
    "set_quant_scheme_dtype('output', 'per_tensor_symmetric', 'int')\n",
    "set_quant_scheme_dtype('input', 'per_tensor_symmetric', 'int')\n",
    "\n",
    "model = NaiveModel().to(device)\n",
    "dummy_input = torch.randn(1, ifmap, idim, idim).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "# To enable batch normalization folding in the training process, you should\n",
    "# pass dummy_input to the QAT_Quantizer.\n",
    "\n",
    "quantizer = QAT_Quantizer(model, configure_list, optimizer, dummy_input=dummy_input)\n",
    "quantizer.compress()\n",
    "\n",
    "# Train and test/evaluate for 10 epoch\n",
    "model.to(device)\n",
    "for epoch in range(50):\n",
    "    print('# Epoch {} #'.format(epoch))\n",
    "    train(model, device, train_loader, optimizer)\n",
    "    test(model, device, test_loader)\n",
    "\n",
    "# Save quantized model parameters\n",
    "model_path = \"models/cifar_model_16bit.pth\"\n",
    "calibration_path = \"cifar_calibration.pth\"\n",
    "onnx_path = \"cifar_model.onnx\"\n",
    "input_shape = (1, ifmap, idim, idim)\n",
    "torch.save(model.state_dict(), model_path)\n",
    "\n",
    "# Quantize model with QAT_Quantizer (see NNI package)\n",
    "#https://nni.readthedocs.io/en/stable/compression/quantization.html\n",
    "# https://github.com/microsoft/nni\n",
    "\n",
    "qmodel = NaiveModel().to(device)\n",
    "dummy_input = torch.randn(1, ifmap, idim, idim).to(device)\n",
    "optimizer = torch.optim.SGD(qmodel.parameters(), lr=0.01, momentum=0.5)\n",
    "# To enable batch normalization folding in the training process, you should\n",
    "# pass dummy_input to the QAT_Quantizer.\n",
    "quantizer = QAT_Quantizer(qmodel, configure_list, optimizer, dummy_input=dummy_input)\n",
    "quantizer.compress()\n",
    "state = torch.load(model_path, map_location='cpu')\n",
    "qmodel.load_state_dict(state, strict=True)\n",
    "test(qmodel, device, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
