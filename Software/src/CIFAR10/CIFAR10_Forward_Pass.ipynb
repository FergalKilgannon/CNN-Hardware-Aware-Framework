{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ferga\\anaconda3\\Lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from naive_cifar import NaiveModel\n",
    "from nni.algorithms.compression.pytorch.quantization import QAT_Quantizer\n",
    "from nni.compression.pytorch.quantization.settings import set_quant_scheme_dtype\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Loss: 1.0538166458129883  Accuracy: 62.22%)\n",
      "\n",
      "ITERATION 1: Accuracy = 54.0 (cumulative = 54)\n",
      "ITERATION 2: Accuracy = 62.0 (cumulative = 124)\n",
      "ITERATION 3: Accuracy = 65.66666666666667 (cumulative = 197)\n",
      "ITERATION 4: Accuracy = 65.5 (cumulative = 262)\n",
      "ITERATION 5: Accuracy = 63.2 (cumulative = 316)\n",
      "ITERATION 6: Accuracy = 64.0 (cumulative = 384)\n",
      "ITERATION 7: Accuracy = 63.57142857142857 (cumulative = 445)\n",
      "ITERATION 8: Accuracy = 63.25 (cumulative = 506)\n",
      "ITERATION 9: Accuracy = 62.888888888888886 (cumulative = 566)\n",
      "ITERATION 10: Accuracy = 62.5 (cumulative = 625)\n",
      "ITERATION 11: Accuracy = 62.36363636363637 (cumulative = 686)\n"
     ]
    }
   ],
   "source": [
    "# Custom test model function\n",
    "# Only implements forward pass of neural network - no training done.\n",
    "def test_custom(model, device, test_loader, num_bits):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    i = 1\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            batch = data.size()[0]\n",
    "\n",
    "            # Custom forward pass function\n",
    "            non_linear_model = forward_custom(model, data, batch, num_bits, 3, 32, 32, 5, 1, 64, 64, device)\n",
    "            output = non_linear_model.forward_pass()\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            print(f\"ITERATION {i}: Accuracy = {correct/i:.2f} (cumulative = {correct})\")\n",
    "            i = i+1\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('Loss: {}  Accuracy: {}%)\\n'.format(\n",
    "        test_loss, 100 * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "# Test original/default model forward pass.\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('Loss: {}  Accuracy: {}%)\\n'.format(\n",
    "        test_loss, 100 * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "def main():\n",
    "    torch.set_default_dtype(torch.float32)\n",
    "    torch.manual_seed(0)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # INT8 weight, INT8 activations\n",
    "    num_bits = 8\n",
    "    # Make sure this matches quantization config from CIFAR10_CNN_Training\n",
    "    configure_list = [{\n",
    "        'quant_types': ['weight', 'input'],\n",
    "        'quant_bits': {'weight': num_bits, 'input': num_bits},\n",
    "        'quant_start_step': 2,\n",
    "        'op_names': ['conv1', 'conv2']\n",
    "    }, {\n",
    "        'quant_types': ['output'],\n",
    "        'quant_bits': {'output': num_bits},\n",
    "        'quant_start_step': 2,\n",
    "        'op_names': ['relu1', 'relu2', 'relu3']\n",
    "    }, {\n",
    "        'quant_types': ['output', 'weight', 'input'],\n",
    "        'quant_bits': {'output': num_bits, 'weight': num_bits, 'input': num_bits},\n",
    "        'quant_start_step': 2,\n",
    "        'op_names': ['fc1', 'fc2'],\n",
    "    }]\n",
    "\n",
    "    set_quant_scheme_dtype('weight', 'per_tensor_symmetric', 'int')\n",
    "    set_quant_scheme_dtype('output', 'per_tensor_symmetric', 'int')\n",
    "    set_quant_scheme_dtype('input', 'per_tensor_symmetric', 'int')\n",
    "\n",
    "    # Load CIFAR-10 dataset with train/test split sets.\n",
    "    trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR10('data', train=True, download=True, transform=trans),\n",
    "        batch_size=64, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR10('data', train=False, transform=trans),\n",
    "        batch_size=100, shuffle=True)\n",
    "\n",
    "\n",
    "    # Create a NaiveModel object and apply QAT_Quantizer setup\n",
    "    model_path = \"cifar_model.pth\"\n",
    "    qmodel = NaiveModel().to(device)\n",
    "    dummy_input = torch.randn(1, 3, 32, 32).to(device)\n",
    "    optimizer = torch.optim.SGD(qmodel.parameters(), lr=0.01, momentum=0.5)\n",
    "    # To enable batch normalization folding in the training process, you should\n",
    "    # pass dummy_input to the QAT_Quantizer.\n",
    "    quantizer = QAT_Quantizer(qmodel, configure_list, optimizer, dummy_input=dummy_input)\n",
    "    quantizer.compress()\n",
    "\n",
    "    # Load trained model (from CIFAR10_CNN_Training step).\n",
    "    state = torch.load(model_path, map_location=device)\n",
    "    qmodel.load_state_dict(state, strict=True)\n",
    "    qmodel.eval()\n",
    "\n",
    "    # Evaluate test accuracy with imported quantized model (qmodel) from cifar_model.pth.\n",
    "    test(qmodel, device, test_loader)\n",
    "\n",
    "    # Evaluate test accuracy of qmodel, with custom forward pass.\n",
    "    test_custom(qmodel, device, test_loader, num_bits)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class forward_custom:\n",
    "  def __init__(self, model, data, batch, num_bits, ifmap,\n",
    "               ofmap, idim, knl, stride, ncol, nrow, device):\n",
    "    self.model = model\n",
    "    self.data = data\n",
    "    self.batch = batch\n",
    "    self.num_bits = num_bits\n",
    "    self.ifmap = ifmap        # Image (colour CIFAR-10)\n",
    "    self.ofmap = ofmap        # Output maps (from applying ofmap knl x knl weight kernels)\n",
    "    self.idim = idim          # Input image dimensions\n",
    "    self.knl = knl            # Kernel dimensions\n",
    "    self.stride = stride      # Convolutional stride\n",
    "    self.ncol = ncol          # Hardware matrix-vector-multiplication (MVM) columns (corresponds to number of output ADCs)\n",
    "    self.nrow = nrow          # Hardware MVM rows (corresponds to number of input DACs)\n",
    "                              # In-memory computing hardware therefore,\n",
    "                              # has ncol*nrow weights.\n",
    "    self.nl_mult = self.read_non_idealities('../HardwareSpec/8x8_Mac_result_final.xlsx')\n",
    "    self.device = device\n",
    "\n",
    "\n",
    "  def read_non_idealities(self, file_name):\n",
    "    return pd.read_excel(file_name, index_col=0).values\n",
    "\n",
    "  def forward_pass(self):\n",
    "    # Scale input\n",
    "    s_in, x = self.scale_quant(self.data, self.num_bits)\n",
    "\n",
    "    # Convolutional layer 1\n",
    "    x = self.conv2d(x, self.model.conv1.module, s_in, self.idim, self.ifmap, self.ofmap)\n",
    "\n",
    "    # ReLU 1\n",
    "    x = self.relu6(x, 0, 6, self.model.relu1.module)\n",
    "\n",
    "    # Maxpool layer 1\n",
    "    x = self.maxpool2d(x, self.idim - self.knl + 1, self.ofmap, 2)\n",
    "\n",
    "    # Scale for convolutional layer 2\n",
    "    sin_conv2 = self.model.conv2.module.input_scale\n",
    "    x = self.noscale_quant(x, sin_conv2.cpu(), 0, self.num_bits)\n",
    "    x = self.conv2d(x, self.model.conv2.module, sin_conv2, (self.idim-self.knl+1)/2, self.ofmap, 64)\n",
    "\n",
    "    # ReLU 2\n",
    "    x = self.relu6(x, 0, 6, self.model.relu2.module)\n",
    "\n",
    "    # Maxpool layer 2\n",
    "    x = self.maxpool2d(x, ((self.idim - self.knl+1)/2) - self.knl + 1, 64, 2)\n",
    "\n",
    "    # Fully connected layer 1\n",
    "    x = x.view(-1, x.size()[1:].numel())  # Flatten outputs of out_maxpool2 layer to feed to FC layers.\n",
    "    x = self.fc(x, self.model.fc1.module, 128) # 128 outputs (32*4*4 = 1600 - > 128)\n",
    "\n",
    "    # ReLU 3\n",
    "    x = self.relu6(x, 0, 6, self.model.relu3.module)\n",
    "\n",
    "    # Fully connected layer 2\n",
    "    x = self.fc(x.cpu(), self.model.fc2.module, 10)\n",
    "\n",
    "    # Softmax layer\n",
    "    return F.log_softmax(x, dim=1)\n",
    "\n",
    "  def conv2d(self, x, conv, s_in, idim, ifmap, ofmap):\n",
    "    # Scale input image, conv weight and bias to bits used\n",
    "    sw_conv, filters_conv = self.scale_quant(conv.weight.cpu(), self.num_bits)\n",
    "    bias_conv = conv.bias / (s_in * sw_conv)\n",
    "\n",
    "    # CONV layer (WSAB dataflow - see convolve2D_wsab function)\n",
    "    out_conv = self.convolve2D_wsab(x, filters_conv, bias_conv, 0, self.stride, self.batch, ifmap, ofmap, idim, self.knl, self.ncol, self.nrow)\n",
    "    out_conv = torch.from_numpy(out_conv)\n",
    "    out_conv = out_conv.to(self.device)\n",
    "\n",
    "    # Applying scaling normalization s_in*sw_conv (convert back to float)\n",
    "    return out_conv * s_in * sw_conv\n",
    "\n",
    "  def convolve2D_wsab(self, image, kernel, bias, padding, strides, batch, ifmap, ofmap, idim, knl, ncol, nrow):\n",
    "    xKernShape = knl\n",
    "    yKernShape = knl\n",
    "    xImgShape = idim\n",
    "    yImgShape = idim\n",
    "\n",
    "    # Number of weight block partitions to fit into hardware\n",
    "    block_col = int(np.ceil(ofmap/ncol))\n",
    "    block_row = int(np.ceil(knl*knl*ifmap/nrow))\n",
    "    kernel_flat = np.zeros((block_col*ncol, block_row*nrow))\n",
    "\n",
    "    # Shape of Output Convolution\n",
    "    xOutput = int(((xImgShape - xKernShape + 2 * padding) / strides) + 1)\n",
    "    yOutput = int(((yImgShape - yKernShape + 2 * padding) / strides) + 1)\n",
    "    output = np.zeros((batch, ofmap, xOutput, yOutput))\n",
    "\n",
    "    kernel_flat[0:ofmap,0:ifmap*knl*knl] = torch.reshape(kernel, [ofmap, ifmap*knl*knl]).cpu().numpy()\n",
    "\n",
    "    # Process image by image\n",
    "    for b in range(batch):\n",
    "        # Apply Equal Padding to All Sides\n",
    "        if padding != 0:\n",
    "            imagePadded = np.zeros((idim + padding * 2, idim + padding * 2))\n",
    "            imagePadded[int(padding):int(-1 * padding), int(padding):int(-1 * padding)] = image[b]\n",
    "        else:\n",
    "            imagePadded = image[b]\n",
    "\n",
    "            # Iterate through image\n",
    "\n",
    "        image_block = np.zeros((block_row * nrow, xOutput, yOutput))\n",
    "        otemp = torch.zeros(block_row, block_col*ncol, xOutput, yOutput)\n",
    "        for bc in range(block_col):\n",
    "            for br in range(block_row):\n",
    "                ktemp = torch.zeros(ncol,nrow)\n",
    "                ktemp = kernel_flat[ncol*bc:(bc+1)*ncol, nrow*br:(br+1)*nrow]\n",
    "                ktemp = torch.from_numpy(ktemp)\n",
    "\n",
    "                for y in range(yOutput):\n",
    "                    for x in range(xOutput):\n",
    "                        # Fetch image section x,y, bc,br\n",
    "                        image_block[0:knl * knl * ifmap, x, y] = imagePadded[0:ifmap,\n",
    "                                                                 strides * x: strides * x + xKernShape,\n",
    "                                                                 strides * y: strides * y + yKernShape].reshape(\n",
    "                            knl * knl * ifmap).cpu().numpy()\n",
    "\n",
    "                        itemp = image_block[br*nrow:(br+1)*nrow, x, y].reshape(1, nrow)\n",
    "                        itemp = torch.from_numpy(itemp)\n",
    "\n",
    "\n",
    "                        # Replace this line with Arduino SPI function call\n",
    "                        #otemp[br, bc*ncol:(bc+1)*ncol, x, y] = torch.sum(ktemp*itemp, dim=1)\n",
    "                        otemp[br, bc*ncol:(bc+1)*ncol, x, y] = self.non_linear_mult(ktemp, itemp[0])\n",
    "                        #print(\"Batch:%d,BROW:%d,BCOL:%d,X:%d,Y:%d\" % (b,br,bc,x,y))\n",
    "        output[b] = torch.sum(otemp[:, 0:ofmap, :, :], dim=0) + (bias.reshape(ofmap, 1, 1).cpu()*torch.ones(xOutput, yOutput)).detach().numpy()\n",
    "\n",
    "    return output\n",
    "\n",
    "  def relu6(self, x, min_val, max_val, relu_module):\n",
    "    i = (x >= min_val) * x\n",
    "    out_relu = (i <= max_val) * (i - max_val) + max_val\n",
    "    so_relu = relu_module.output_scale\n",
    "\n",
    "    # Apply fake quantization to relu1 output.\n",
    "    out_relu = self.noscale_quant(out_relu, so_relu, 0, self.num_bits)\n",
    "    return self.dequantize(out_relu, so_relu, 0)\n",
    "\n",
    "  def maxpool2d(self, x, idim, ofmap, knl):\n",
    "    # Maxpool layer - downsample by knl x knl with maxpool (no quantization required for max function)\n",
    "    return torch.from_numpy(self.maxpool2D_wsa(self.batch, x, idim, ofmap, knl))\n",
    "\n",
    "  # Maxpool layer (apply to all ofmaps simultaneously - faster!)\n",
    "  def maxpool2D_wsa(self, batch, image, idim, ofmap, knl):\n",
    "    xKernShape = knl\n",
    "    yKernShape = knl\n",
    "    xImgShape = idim\n",
    "    yImgShape = idim\n",
    "    strides = knl\n",
    "    padding = 0\n",
    "    # Shape of Output Convolution\n",
    "    xOutput = int(((xImgShape - xKernShape + 2 * padding) / strides) + 1)\n",
    "    yOutput = int(((yImgShape - yKernShape + 2 * padding) / strides) + 1)\n",
    "    output = np.zeros((batch, ofmap, xOutput, yOutput))\n",
    "\n",
    "    for b in range(batch):\n",
    "        # Apply Equal Padding to All Sides\n",
    "        if padding != 0:\n",
    "            imagePadded = np.zeros((idim + padding * 2, idim + padding * 2))\n",
    "            imagePadded[int(padding):int(-1 * padding), int(padding):int(-1 * padding)] = image[b]\n",
    "        else:\n",
    "            imagePadded = image[b]\n",
    "\n",
    "        # Iterate through image\n",
    "        for y in range(yOutput):\n",
    "            for x in range(xOutput):\n",
    "                output[b, :, x, y] = torch.amax(\n",
    "                        imagePadded[:, strides * x: strides * x + xKernShape, strides * y: strides * y + yKernShape], dim=(1,2)).detach().cpu().numpy()\n",
    "    return output\n",
    "\n",
    "  def fc(self, x, module, odim):\n",
    "    sin_fc = module.input_scale.cpu()\n",
    "    in_fcs = self.noscale_quant(x, sin_fc, 0, self.num_bits)\n",
    "    sw_fc, filters_fc = self.scale_quant(module.weight.cpu(), self.num_bits)\n",
    "    bias_fc = module.bias.cpu() / (sw_fc * sin_fc)\n",
    "\n",
    "\n",
    "    # FC layer using (WSA dataflow)\n",
    "    # N.B. Need to implement WSAB function to implement 'hardware acceleration' of FC layer.\n",
    "    out_fc = torch.from_numpy(self.fc_custom_wsa(in_fcs, filters_fc, bias_fc, self.batch, odim))\n",
    "\n",
    "    # FC output scaling\n",
    "    out_fcs = out_fc * sin_fc * sw_fc\n",
    "\n",
    "    so_fc = module.output_scale.cpu()\n",
    "    # FC output fake quantization\n",
    "    return self.dequantize(self.noscale_quant(out_fcs, so_fc, 0, self.num_bits), so_fc, 0).to(self.device)\n",
    "\n",
    "  # 'Fake' Quantization function [Jacob et. al]\n",
    "  def quantize(self, real_value, scale, zero_point, qmin, qmax):\n",
    "    transformed_val = zero_point + real_value / scale\n",
    "    clamped_val = torch.clamp(transformed_val, qmin, qmax)\n",
    "    quantized_val = torch.round(clamped_val)\n",
    "    return quantized_val\n",
    "\n",
    "  # 'Fake' Dequantization function [Jacob et. al]\n",
    "  def dequantize(self, quantized_val, scale, zero_point):\n",
    "    real_val = scale * (quantized_val - zero_point)\n",
    "    return real_val\n",
    "\n",
    "  # Scaling function (Jacob et. al)\n",
    "  def scale_quant(self, real_value, num_bits):\n",
    "    qmin = -(2 ** (num_bits - 1) - 1)\n",
    "    qmax = 2 ** (num_bits - 1) - 1\n",
    "    abs_max = torch.abs(real_value).max()\n",
    "    scale = abs_max / (float(qmax - qmin) / 2)\n",
    "    zero_point = 0\n",
    "    quant = self.quantize(real_value, scale, zero_point, qmin, qmax)\n",
    "    return scale, quant\n",
    "\n",
    "  # Scaling function (Jacob et. al)\n",
    "  def noscale_quant(self, real_value, scale, zero_point, num_bits):\n",
    "    qmin = -(2 ** (num_bits - 1) - 1)\n",
    "    qmax = 2 ** (num_bits - 1) - 1\n",
    "    quant = self.quantize(real_value, scale, zero_point, qmin, qmax)\n",
    "    return quant\n",
    "\n",
    "  # Custom FC layer (vector multiplication style - much faster than for loop implementation)\n",
    "  def fc_custom_wsa(self, fc_input, filters, bias, batch, ofmap):\n",
    "    output = np.zeros((batch, ofmap))\n",
    "    for b in range(batch):\n",
    "        outputi = np.zeros(ofmap)\n",
    "        input_batch = fc_input[b]\n",
    "        outputi = torch.sum((input_batch * filters),dim=1) + bias\n",
    "        output[b, :] = outputi.detach().numpy()\n",
    "    return output\n",
    "\n",
    "\n",
    "  # Non-linear jazz\n",
    "  def non_linear_conv(self, kernel, image):\n",
    "    sum = 0\n",
    "    for i in range(len(kernel)):\n",
    "      sum = sum + self.nl_mult[int(kernel[i]+15)][int(image[i]+15)]\n",
    "    return sum\n",
    "\n",
    "  def non_linear_mult(self, kernel, image):\n",
    "    output = np.empty(len(kernel))\n",
    "\n",
    "    for i in range(len(kernel)):\n",
    "      output[i] = self.non_linear_conv(kernel[i, :].tolist(), image.tolist())\n",
    "\n",
    "    return torch.from_numpy(output)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
