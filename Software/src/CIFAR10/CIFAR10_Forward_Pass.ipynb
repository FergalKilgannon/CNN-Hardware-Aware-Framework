{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10 FORWARD PASS\n",
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from naive_cifar import NaiveModel\n",
    "from nni.algorithms.compression.pytorch.quantization import QAT_Quantizer\n",
    "from nni.compression.pytorch.quantization.settings import set_quant_scheme_dtype\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Pass Class\n",
    "\n",
    "Note the function *forward_pass()*. This contains our CNN design, and is generalized numerically. Later in the notebook, it is possible to customize some aspects of this CNN (number of kernels used, size of the kernels, fully connected node counts, etc.), but the overall structure will need to be recoded. For instance, if you wanted a third convolutional layer, you will add that function wherever in the order you require it, but also will need to take note of the numerical values being fed into and out of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class forward_custom:\n",
    "  def __init__(self, model, data, batch, num_bits, ifmap,\n",
    "               ofmap, idim, knl, stride, conv2kernels, maxpool_dim, \n",
    "               fc1_nodes, fc2_nodes, non_ideal_file, ncol, nrow, device):\n",
    "    self.model = model\n",
    "    self.data = data\n",
    "    self.batch = batch\n",
    "    self.num_bits = num_bits\n",
    "    self.ifmap = ifmap                # Image (RGB CIFAR-10)\n",
    "    self.ofmap = ofmap                # Output maps (from applying ofmap knl x knl weight kernels)\n",
    "    self.idim = idim                  # Input image dimensions\n",
    "    self.knl = knl                    # Kernel dimensions\n",
    "    self.stride = stride              # Convolutional stride\n",
    "    self.conv2kernels = conv2kernels  # Output channels to the second conv layer\n",
    "    self.maxpool_dim = maxpool_dim    # Maxpool area\n",
    "    self.fc1_nodes = fc1_nodes        # Fully connected layer 2 output nodes\n",
    "    self.fc2_nodes = fc2_nodes        # Fully connected layer 2 output nodes\n",
    "    self.ncol = ncol                  # Hardware matrix-vector-multiplication (MVM) columns (corresponds to number of output ADCs)\n",
    "    self.nrow = nrow                  # Hardware MVM rows (corresponds to number of input DACs)\n",
    "                                      # In-memory computing hardware therefore has ncol*nrow weights.\n",
    "\n",
    "    self.nl_mult = self.read_non_idealities(non_ideal_file)\n",
    "    self.device = device\n",
    "\n",
    "\n",
    "  def read_non_idealities(self, file_name):\n",
    "    return pd.read_excel(file_name, index_col=0).values\n",
    "\n",
    "  def forward_pass(self):\n",
    "    # Scale input\n",
    "    s_in, x = self.scale_quant(self.data, self.num_bits)\n",
    "\n",
    "    # Convolutional layer 1\n",
    "    x = self.conv2d(x, self.model.conv1.module, s_in, self.idim, self.ifmap, self.ofmap)\n",
    "\n",
    "    # ReLU 1\n",
    "    x = self.relu6(x, 0, 6, self.model.relu1.module)\n",
    "\n",
    "    # Maxpool layer 1\n",
    "    x = self.maxpool2d(x, self.idim - self.knl + 1, self.ofmap, self.maxpool_dim)\n",
    "\n",
    "    # Scale for convolutional layer 2\n",
    "    sin_conv2 = self.model.conv2.module.input_scale\n",
    "    x = self.noscale_quant(x, sin_conv2.cpu(), 0, self.num_bits)\n",
    "    x = self.conv2d(x, self.model.conv2.module, sin_conv2, (self.idim-self.knl+1)/2, self.ofmap, self.conv2kernels)\n",
    "\n",
    "    # ReLU 2\n",
    "    x = self.relu6(x, 0, 6, self.model.relu2.module)\n",
    "\n",
    "    # Maxpool layer 2\n",
    "    x = self.maxpool2d(x, ((self.idim - self.knl+1)/2) - self.knl + 1, self.conv2kernels, self.maxpool_dim)\n",
    "\n",
    "    # Fully connected layer 1\n",
    "    x = x.view(-1, x.size()[1:].numel())  # Flatten outputs of out_maxpool2 layer to feed to FC layers.\n",
    "    x = self.fc(x, self.model.fc1.module, self.fc1_nodes)\n",
    "\n",
    "    # ReLU 3\n",
    "    x = self.relu6(x, 0, 6, self.model.relu3.module)\n",
    "\n",
    "    # Fully connected layer 2\n",
    "    x = self.fc(x.cpu(), self.model.fc2.module, self.fc2_nodes)\n",
    "\n",
    "    # Softmax layer\n",
    "    return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "  def conv2d(self, x, conv, s_in, idim, ifmap, ofmap):\n",
    "    # Scale input image, conv weight and bias to bits used\n",
    "    sw_conv, filters_conv = self.scale_quant(conv.weight.cpu(), self.num_bits)\n",
    "    bias_conv = conv.bias / (s_in * sw_conv)\n",
    "\n",
    "    # CONV layer (WSAB dataflow - see convolve2D_wsab function)\n",
    "    out_conv = self.convolve2D_wsab(x, filters_conv, bias_conv, 0, self.stride, self.batch, ifmap, ofmap, idim, self.knl, self.ncol, self.nrow)\n",
    "    out_conv = torch.from_numpy(out_conv)\n",
    "    out_conv = out_conv.to(self.device)\n",
    "\n",
    "    # Applying scaling normalization s_in*sw_conv (convert back to float)\n",
    "    return out_conv * s_in * sw_conv\n",
    "\n",
    "\n",
    "  def convolve2D_wsab(self, image, kernel, bias, padding, strides, batch, ifmap, ofmap, idim, knl, ncol, nrow):\n",
    "    xKernShape = knl\n",
    "    yKernShape = knl\n",
    "    xImgShape = idim\n",
    "    yImgShape = idim\n",
    "\n",
    "    # Number of weight block partitions to fit into hardware\n",
    "    block_col = int(np.ceil(ofmap/ncol))\n",
    "    block_row = int(np.ceil(knl*knl*ifmap/nrow))\n",
    "    kernel_flat = np.zeros((block_col*ncol, block_row*nrow))\n",
    "\n",
    "    # Shape of Output Convolution\n",
    "    xOutput = int(((xImgShape - xKernShape + 2 * padding) / strides) + 1)\n",
    "    yOutput = int(((yImgShape - yKernShape + 2 * padding) / strides) + 1)\n",
    "    output = np.zeros((batch, ofmap, xOutput, yOutput))\n",
    "\n",
    "    kernel_flat[0:ofmap,0:ifmap*knl*knl] = torch.reshape(kernel, [ofmap, ifmap*knl*knl]).cpu().numpy()\n",
    "\n",
    "    # Process image by image\n",
    "    for b in range(batch):\n",
    "        # Apply Equal Padding to All Sides\n",
    "        if padding != 0:\n",
    "            imagePadded = np.zeros((idim + padding * 2, idim + padding * 2))\n",
    "            imagePadded[int(padding):int(-1 * padding), int(padding):int(-1 * padding)] = image[b]\n",
    "        else:\n",
    "            imagePadded = image[b]\n",
    "\n",
    "            # Iterate through image\n",
    "\n",
    "        image_block = np.zeros((block_row * nrow, xOutput, yOutput))\n",
    "        otemp = torch.zeros(block_row, block_col*ncol, xOutput, yOutput)\n",
    "        for bc in range(block_col):\n",
    "            for br in range(block_row):\n",
    "                ktemp = torch.zeros(ncol,nrow)\n",
    "                ktemp = kernel_flat[ncol*bc:(bc+1)*ncol, nrow*br:(br+1)*nrow]\n",
    "                ktemp = torch.from_numpy(ktemp)\n",
    "\n",
    "                for y in range(yOutput):\n",
    "                    for x in range(xOutput):\n",
    "                        # Fetch image section x,y, bc,br\n",
    "                        image_block[0:knl * knl * ifmap, x, y] = imagePadded[0:ifmap,\n",
    "                                                                 strides * x: strides * x + xKernShape,\n",
    "                                                                 strides * y: strides * y + yKernShape].reshape(\n",
    "                            knl * knl * ifmap).cpu().numpy()\n",
    "\n",
    "                        itemp = image_block[br*nrow:(br+1)*nrow, x, y].reshape(1, nrow)\n",
    "                        itemp = torch.from_numpy(itemp)\n",
    "\n",
    "\n",
    "                        # Replace this line with Arduino SPI function call\n",
    "                        #otemp[br, bc*ncol:(bc+1)*ncol, x, y] = torch.sum(ktemp*itemp, dim=1)\n",
    "                        otemp[br, bc*ncol:(bc+1)*ncol, x, y] = self.non_linear_mult(ktemp, itemp[0])\n",
    "                        #print(\"Batch:%d,BROW:%d,BCOL:%d,X:%d,Y:%d\" % (b,br,bc,x,y))\n",
    "        output[b] = torch.sum(otemp[:, 0:ofmap, :, :], dim=0) + (bias.reshape(ofmap, 1, 1).cpu()*torch.ones(xOutput, yOutput)).detach().numpy()\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "  def relu6(self, x, min_val, max_val, relu_module):\n",
    "    i = (x >= min_val) * x\n",
    "    out_relu = (i <= max_val) * (i - max_val) + max_val\n",
    "    so_relu = relu_module.output_scale\n",
    "\n",
    "    # Apply fake quantization to relu1 output.\n",
    "    out_relu = self.noscale_quant(out_relu, so_relu, 0, self.num_bits)\n",
    "    return self.dequantize(out_relu, so_relu, 0)\n",
    "\n",
    "\n",
    "  def maxpool2d(self, x, idim, ofmap, knl):\n",
    "    # Maxpool layer - downsample by knl x knl with maxpool (no quantization required for max function)\n",
    "    return torch.from_numpy(self.maxpool2D_wsa(self.batch, x, idim, ofmap, knl))\n",
    "\n",
    "\n",
    "  # Maxpool layer (apply to all ofmaps simultaneously - faster!)\n",
    "  def maxpool2D_wsa(self, batch, image, idim, ofmap, knl):\n",
    "    xKernShape = knl\n",
    "    yKernShape = knl\n",
    "    xImgShape = idim\n",
    "    yImgShape = idim\n",
    "    strides = knl\n",
    "    padding = 0\n",
    "    # Shape of Output Convolution\n",
    "    xOutput = int(((xImgShape - xKernShape + 2 * padding) / strides) + 1)\n",
    "    yOutput = int(((yImgShape - yKernShape + 2 * padding) / strides) + 1)\n",
    "    output = np.zeros((batch, ofmap, xOutput, yOutput))\n",
    "\n",
    "    for b in range(batch):\n",
    "        # Apply Equal Padding to All Sides\n",
    "        if padding != 0:\n",
    "            imagePadded = np.zeros((idim + padding * 2, idim + padding * 2))\n",
    "            imagePadded[int(padding):int(-1 * padding), int(padding):int(-1 * padding)] = image[b]\n",
    "        else:\n",
    "            imagePadded = image[b]\n",
    "\n",
    "        # Iterate through image\n",
    "        for y in range(yOutput):\n",
    "            for x in range(xOutput):\n",
    "                output[b, :, x, y] = torch.amax(\n",
    "                        imagePadded[:, strides * x: strides * x + xKernShape, strides * y: strides * y + yKernShape], dim=(1,2)).detach().cpu().numpy()\n",
    "    return output\n",
    "\n",
    "\n",
    "  def fc(self, x, module, odim):\n",
    "    sin_fc = module.input_scale.cpu()\n",
    "    in_fcs = self.noscale_quant(x, sin_fc, 0, self.num_bits)\n",
    "    sw_fc, filters_fc = self.scale_quant(module.weight.cpu(), self.num_bits)\n",
    "    bias_fc = module.bias.cpu() / (sw_fc * sin_fc)\n",
    "\n",
    "    # FC layer using (WSA dataflow)\n",
    "    # N.B. Need to implement WSAB function to implement 'hardware acceleration' of FC layer.\n",
    "    out_fc = torch.from_numpy(self.fc_custom_wsa(in_fcs, filters_fc, bias_fc, self.batch, odim))\n",
    "\n",
    "    # FC output scaling\n",
    "    out_fcs = out_fc * sin_fc * sw_fc\n",
    "\n",
    "    so_fc = module.output_scale.cpu()\n",
    "    # FC output fake quantization\n",
    "    return self.dequantize(self.noscale_quant(out_fcs, so_fc, 0, self.num_bits), so_fc, 0).to(self.device)\n",
    "\n",
    "\n",
    "  # 'Fake' Quantization function [Jacob et. al]\n",
    "  def quantize(self, real_value, scale, zero_point, qmin, qmax):\n",
    "    transformed_val = zero_point + real_value / scale\n",
    "    clamped_val = torch.clamp(transformed_val, qmin, qmax)\n",
    "    quantized_val = torch.round(clamped_val)\n",
    "    return quantized_val\n",
    "\n",
    "\n",
    "  # 'Fake' Dequantization function [Jacob et. al]\n",
    "  def dequantize(self, quantized_val, scale, zero_point):\n",
    "    real_val = scale * (quantized_val - zero_point)\n",
    "    return real_val\n",
    "\n",
    "\n",
    "  # Scaling function (Jacob et. al)\n",
    "  def scale_quant(self, real_value, num_bits):\n",
    "    qmin = -(2 ** (num_bits - 1) - 1)\n",
    "    qmax = 2 ** (num_bits - 1) - 1\n",
    "    abs_max = torch.abs(real_value).max()\n",
    "    scale = abs_max / (float(qmax - qmin) / 2)\n",
    "    zero_point = 0\n",
    "    quant = self.quantize(real_value, scale, zero_point, qmin, qmax)\n",
    "    return scale, quant\n",
    "\n",
    "\n",
    "  # Scaling function (Jacob et. al)\n",
    "  def noscale_quant(self, real_value, scale, zero_point, num_bits):\n",
    "    qmin = -(2 ** (num_bits - 1) - 1)\n",
    "    qmax = 2 ** (num_bits - 1) - 1\n",
    "    quant = self.quantize(real_value, scale, zero_point, qmin, qmax)\n",
    "    return quant\n",
    "\n",
    "\n",
    "  # Custom FC layer (vector multiplication style - much faster than for loop implementation)\n",
    "  def fc_custom_wsa(self, fc_input, filters, bias, batch, ofmap):\n",
    "    output = np.zeros((batch, ofmap))\n",
    "    for b in range(batch):\n",
    "        outputi = np.zeros(ofmap)\n",
    "        input_batch = fc_input[b]\n",
    "        outputi = torch.sum((input_batch * filters),dim=1) + bias\n",
    "        output[b, :] = outputi.detach().numpy()\n",
    "    return output\n",
    "\n",
    "\n",
    "  # Non-linear lookup\n",
    "  def non_linear_conv(self, kernel, image):\n",
    "    sum = 0\n",
    "    for i in range(len(kernel)):\n",
    "      sum = sum + self.nl_mult[int(kernel[i]+(2**(self.num_bits-1))-1)][int(image[i]+(2**(self.num_bits-1))-1)]\n",
    "    return sum\n",
    "\n",
    "  def non_linear_mult(self, kernel, image):\n",
    "    output = np.empty(len(kernel))\n",
    "\n",
    "    for i in range(len(kernel)):\n",
    "      output[i] = self.non_linear_conv(kernel[i, :].tolist(), image.tolist())\n",
    "\n",
    "    return torch.from_numpy(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel dimensions (used in conv layers)\n",
    "knl = 5\n",
    "\n",
    "# Kernel stride\n",
    "stride = 1\n",
    "\n",
    "# Output Feature Maps (same as kernels in conv layer 1)\n",
    "ofmap = 32\n",
    "\n",
    "# Kernels in conv layer 2\n",
    "conv2kernels = 64\n",
    "\n",
    "# Maxpool dimensions (assuming square area)\n",
    "maxpool_dim = 2\n",
    "\n",
    "# Fully Connected 1 output nodes\n",
    "fc1_nodes = 128\n",
    "\n",
    "# Hardware MVM columns\n",
    "ncol = 16\n",
    "\n",
    "# Hardware MVM rows\n",
    "nrow = 32\n",
    "\n",
    "# Non-Idealities File\n",
    "non_ideal_file = '../HardwareSpec/8x8_Mac_result_final.xlsx'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom test model function\n",
    "# Only implements forward pass of neural network - no training done.\n",
    "def test_custom(model, device, test_loader, num_bits, idim, ifmap, ofmap, knl, stride,\n",
    "                conv2kernels, maxpool_dim, fc1_nodes, fc2_nodes, non_ideal_file, ncol, nrow):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    i = 1\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            batch = data.size()[0]\n",
    "\n",
    "            # Custom forward pass function\n",
    "            non_linear_model = forward_custom(model, data, batch, num_bits, ifmap, ofmap, idim, knl, stride, conv2kernels,\n",
    "                                              maxpool_dim, fc1_nodes, fc2_nodes, non_ideal_file, ncol, nrow, device)\n",
    "            output = non_linear_model.forward_pass()\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            print(f\"ITERATION {i}: Accuracy = {correct/i:.2f}% (cumulative = {correct})\")\n",
    "            i = i+1\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print(f\"Loss: {test_loss}   Accuracy: {100 * correct / len(test_loader.dataset)}%\\n\")\n",
    "\n",
    "\n",
    "# Test original/default model forward pass.\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print(f\"Loss: {test_loss}   Accuracy: {100 * correct / len(test_loader.dataset)}%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization and Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "torch.set_default_dtype(torch.float32)\n",
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# INT9 weight, INT9 activations\n",
    "num_bits = 9\n",
    "# Make sure this matches quantization config from CIFAR10_CNN_Training\n",
    "configure_list = [{\n",
    "    'quant_types': ['weight', 'input'],\n",
    "    'quant_bits': {'weight': num_bits, 'input': num_bits},\n",
    "    'quant_start_step': 2,\n",
    "    'op_names': ['conv1', 'conv2']\n",
    "}, {\n",
    "    'quant_types': ['output'],\n",
    "    'quant_bits': {'output': num_bits},\n",
    "    'quant_start_step': 2,\n",
    "    'op_names': ['relu1', 'relu2', 'relu3']\n",
    "}, {\n",
    "    'quant_types': ['output', 'weight', 'input'],\n",
    "    'quant_bits': {'output': num_bits, 'weight': num_bits, 'input': num_bits},\n",
    "    'quant_start_step': 2,\n",
    "    'op_names': ['fc1', 'fc2'],\n",
    "}]\n",
    "\n",
    "set_quant_scheme_dtype('weight', 'per_tensor_symmetric', 'int')\n",
    "set_quant_scheme_dtype('output', 'per_tensor_symmetric', 'int')\n",
    "set_quant_scheme_dtype('input', 'per_tensor_symmetric', 'int')\n",
    "\n",
    "# Load CIFAR-10 dataset with train/test split sets.\n",
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('data', train=True, download=True, transform=trans),\n",
    "    batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('data', train=False, transform=trans),\n",
    "    batch_size=100, shuffle=True)\n",
    "\n",
    "idim = next(iter(train_loader))[0][0].size()[1]\n",
    "ifmap = next(iter(train_loader))[0][0].size()[0]\n",
    "fc2_nodes = len(set(train_loader.dataset.targets))\n",
    "\n",
    "\n",
    "# Create a NaiveModel object and apply QAT_Quantizer setup\n",
    "model_path = \"models/cifar_model_4bit.pth\"\n",
    "qmodel = NaiveModel().to(device)\n",
    "dummy_input = torch.randn(1, ifmap, idim, idim).to(device)\n",
    "optimizer = torch.optim.SGD(qmodel.parameters(), lr=0.01, momentum=0.5)\n",
    "# To enable batch normalization folding in the training process, you should\n",
    "# pass dummy_input to the QAT_Quantizer.\n",
    "quantizer = QAT_Quantizer(qmodel, configure_list, optimizer, dummy_input=dummy_input)\n",
    "quantizer.compress()\n",
    "\n",
    "# Load trained model (from CIFAR10_CNN_Training step).\n",
    "state = torch.load(model_path, map_location=device)\n",
    "qmodel.load_state_dict(state, strict=True)\n",
    "qmodel.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.6597601303100586   Accuracy: 39.94%\n",
      "\n",
      "ITERATION 1: Accuracy = 50.00% (cumulative = 50)\n",
      "ITERATION 2: Accuracy = 45.00% (cumulative = 90)\n",
      "ITERATION 3: Accuracy = 40.67% (cumulative = 122)\n",
      "ITERATION 4: Accuracy = 40.75% (cumulative = 163)\n",
      "ITERATION 5: Accuracy = 42.60% (cumulative = 213)\n",
      "ITERATION 6: Accuracy = 41.67% (cumulative = 250)\n",
      "ITERATION 7: Accuracy = 40.86% (cumulative = 286)\n",
      "ITERATION 8: Accuracy = 41.00% (cumulative = 328)\n",
      "ITERATION 9: Accuracy = 40.44% (cumulative = 364)\n",
      "ITERATION 10: Accuracy = 40.20% (cumulative = 402)\n",
      "ITERATION 11: Accuracy = 39.64% (cumulative = 436)\n",
      "ITERATION 12: Accuracy = 39.67% (cumulative = 476)\n",
      "ITERATION 13: Accuracy = 40.15% (cumulative = 522)\n",
      "ITERATION 14: Accuracy = 39.93% (cumulative = 559)\n",
      "ITERATION 15: Accuracy = 39.47% (cumulative = 592)\n",
      "ITERATION 16: Accuracy = 39.38% (cumulative = 630)\n",
      "ITERATION 17: Accuracy = 39.88% (cumulative = 678)\n",
      "ITERATION 18: Accuracy = 40.11% (cumulative = 722)\n",
      "ITERATION 19: Accuracy = 40.32% (cumulative = 766)\n",
      "ITERATION 20: Accuracy = 40.15% (cumulative = 803)\n",
      "ITERATION 21: Accuracy = 40.24% (cumulative = 845)\n",
      "ITERATION 22: Accuracy = 40.41% (cumulative = 889)\n",
      "ITERATION 23: Accuracy = 40.09% (cumulative = 922)\n",
      "ITERATION 24: Accuracy = 40.42% (cumulative = 970)\n",
      "ITERATION 25: Accuracy = 39.96% (cumulative = 999)\n",
      "ITERATION 26: Accuracy = 39.92% (cumulative = 1038)\n",
      "ITERATION 27: Accuracy = 40.04% (cumulative = 1081)\n",
      "ITERATION 28: Accuracy = 39.93% (cumulative = 1118)\n",
      "ITERATION 29: Accuracy = 39.69% (cumulative = 1151)\n",
      "ITERATION 30: Accuracy = 39.83% (cumulative = 1195)\n",
      "ITERATION 31: Accuracy = 39.77% (cumulative = 1233)\n",
      "ITERATION 32: Accuracy = 39.69% (cumulative = 1270)\n",
      "ITERATION 33: Accuracy = 39.70% (cumulative = 1310)\n",
      "ITERATION 34: Accuracy = 39.74% (cumulative = 1351)\n",
      "ITERATION 35: Accuracy = 39.86% (cumulative = 1395)\n",
      "ITERATION 36: Accuracy = 40.22% (cumulative = 1448)\n",
      "ITERATION 37: Accuracy = 40.24% (cumulative = 1489)\n",
      "ITERATION 38: Accuracy = 40.24% (cumulative = 1529)\n",
      "ITERATION 39: Accuracy = 40.31% (cumulative = 1572)\n",
      "ITERATION 40: Accuracy = 40.50% (cumulative = 1620)\n",
      "ITERATION 41: Accuracy = 40.27% (cumulative = 1651)\n",
      "ITERATION 42: Accuracy = 40.36% (cumulative = 1695)\n",
      "ITERATION 43: Accuracy = 40.33% (cumulative = 1734)\n",
      "ITERATION 44: Accuracy = 40.30% (cumulative = 1773)\n",
      "ITERATION 45: Accuracy = 40.16% (cumulative = 1807)\n",
      "ITERATION 46: Accuracy = 40.24% (cumulative = 1851)\n",
      "ITERATION 47: Accuracy = 40.06% (cumulative = 1883)\n",
      "ITERATION 48: Accuracy = 40.08% (cumulative = 1924)\n",
      "ITERATION 49: Accuracy = 40.10% (cumulative = 1965)\n",
      "ITERATION 50: Accuracy = 40.12% (cumulative = 2006)\n",
      "ITERATION 51: Accuracy = 40.18% (cumulative = 2049)\n",
      "ITERATION 52: Accuracy = 40.13% (cumulative = 2087)\n",
      "ITERATION 53: Accuracy = 40.23% (cumulative = 2132)\n",
      "ITERATION 54: Accuracy = 40.35% (cumulative = 2179)\n",
      "ITERATION 55: Accuracy = 40.42% (cumulative = 2223)\n",
      "ITERATION 56: Accuracy = 40.36% (cumulative = 2260)\n",
      "ITERATION 57: Accuracy = 40.53% (cumulative = 2310)\n",
      "ITERATION 58: Accuracy = 40.57% (cumulative = 2353)\n",
      "ITERATION 59: Accuracy = 40.54% (cumulative = 2392)\n",
      "ITERATION 60: Accuracy = 40.53% (cumulative = 2432)\n",
      "ITERATION 61: Accuracy = 40.64% (cumulative = 2479)\n",
      "ITERATION 62: Accuracy = 40.61% (cumulative = 2518)\n",
      "ITERATION 63: Accuracy = 40.59% (cumulative = 2557)\n",
      "ITERATION 64: Accuracy = 40.62% (cumulative = 2600)\n",
      "ITERATION 65: Accuracy = 40.77% (cumulative = 2650)\n",
      "ITERATION 66: Accuracy = 40.86% (cumulative = 2697)\n",
      "ITERATION 67: Accuracy = 40.87% (cumulative = 2738)\n",
      "ITERATION 68: Accuracy = 40.97% (cumulative = 2786)\n",
      "ITERATION 69: Accuracy = 41.06% (cumulative = 2833)\n",
      "ITERATION 70: Accuracy = 40.99% (cumulative = 2869)\n",
      "ITERATION 71: Accuracy = 40.94% (cumulative = 2907)\n",
      "ITERATION 72: Accuracy = 40.97% (cumulative = 2950)\n",
      "ITERATION 73: Accuracy = 40.92% (cumulative = 2987)\n",
      "ITERATION 74: Accuracy = 40.91% (cumulative = 3027)\n",
      "ITERATION 75: Accuracy = 40.79% (cumulative = 3059)\n",
      "ITERATION 76: Accuracy = 40.75% (cumulative = 3097)\n",
      "ITERATION 77: Accuracy = 40.69% (cumulative = 3133)\n",
      "ITERATION 78: Accuracy = 40.74% (cumulative = 3178)\n",
      "ITERATION 79: Accuracy = 40.72% (cumulative = 3217)\n",
      "ITERATION 80: Accuracy = 40.76% (cumulative = 3261)\n",
      "ITERATION 81: Accuracy = 40.74% (cumulative = 3300)\n",
      "ITERATION 82: Accuracy = 40.70% (cumulative = 3337)\n",
      "ITERATION 83: Accuracy = 40.75% (cumulative = 3382)\n",
      "ITERATION 84: Accuracy = 40.80% (cumulative = 3427)\n",
      "ITERATION 85: Accuracy = 40.81% (cumulative = 3469)\n",
      "ITERATION 86: Accuracy = 40.78% (cumulative = 3507)\n",
      "ITERATION 87: Accuracy = 40.78% (cumulative = 3548)\n",
      "ITERATION 88: Accuracy = 40.82% (cumulative = 3592)\n",
      "ITERATION 89: Accuracy = 40.84% (cumulative = 3635)\n",
      "ITERATION 90: Accuracy = 40.92% (cumulative = 3683)\n",
      "ITERATION 91: Accuracy = 40.97% (cumulative = 3728)\n",
      "ITERATION 92: Accuracy = 40.91% (cumulative = 3764)\n",
      "ITERATION 93: Accuracy = 40.94% (cumulative = 3807)\n",
      "ITERATION 94: Accuracy = 40.96% (cumulative = 3850)\n",
      "ITERATION 95: Accuracy = 41.04% (cumulative = 3899)\n",
      "ITERATION 96: Accuracy = 40.99% (cumulative = 3935)\n",
      "ITERATION 97: Accuracy = 41.01% (cumulative = 3978)\n",
      "ITERATION 98: Accuracy = 41.05% (cumulative = 4023)\n",
      "ITERATION 99: Accuracy = 40.98% (cumulative = 4057)\n",
      "ITERATION 100: Accuracy = 40.98% (cumulative = 4098)\n",
      "Loss: 1.6862476070374894   Accuracy: 40.98%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate test accuracy with imported quantized model (qmodel) from cifar_model.pth.\n",
    "test(qmodel, device, test_loader)\n",
    "\n",
    "# Evaluate test accuracy of qmodel, with custom forward pass.\n",
    "test_custom(qmodel, device, test_loader, num_bits, idim, ifmap, ofmap, knl, stride,\n",
    "            conv2kernels, maxpool_dim, fc1_nodes, fc2_nodes, non_ideal_file, ncol, nrow)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
