{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST FORWARD PASS\n",
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ferga\\anaconda3\\Lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from naive_cifar import NaiveModel\n",
    "from nni.algorithms.compression.pytorch.quantization import QAT_Quantizer\n",
    "from nni.compression.pytorch.quantization.settings import set_quant_scheme_dtype\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Pass Class\n",
    "\n",
    "Note the function *forward_pass()*. This contains our CNN design, and is generalized numerically. Later in the notebook, it is possible to customize some aspects of this CNN (number of kernels used, size of the kernels, fully connected node counts, etc.), but the overall structure will need to be recoded. For instance, if you wanted a third convolutional layer, you will add that function wherever in the order you require it, but also will need to take note of the numerical values being fed into and out of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class forward_custom:\n",
    "  def __init__(self, model, data, batch, num_bits, ifmap,\n",
    "               ofmap, idim, knl, stride, conv2kernels, maxpool_dim, \n",
    "               fc1_nodes, fc2_nodes, non_ideal_file, ncol, nrow, device):\n",
    "    self.model = model\n",
    "    self.data = data\n",
    "    self.batch = batch\n",
    "    self.num_bits = num_bits\n",
    "    self.ifmap = ifmap                # Image (RGB CIFAR-10)\n",
    "    self.ofmap = ofmap                # Output maps (from applying ofmap knl x knl weight kernels)\n",
    "    self.idim = idim                  # Input image dimensions\n",
    "    self.knl = knl                    # Kernel dimensions\n",
    "    self.stride = stride              # Convolutional stride\n",
    "    self.conv2kernels = conv2kernels  # Output channels to the second conv layer\n",
    "    self.maxpool_dim = maxpool_dim    # Maxpool area\n",
    "    self.fc1_nodes = fc1_nodes        # Fully connected layer 2 output nodes\n",
    "    self.fc2_nodes = fc2_nodes        # Fully connected layer 2 output nodes\n",
    "    self.ncol = ncol                  # Hardware matrix-vector-multiplication (MVM) columns (corresponds to number of output ADCs)\n",
    "    self.nrow = nrow                  # Hardware MVM rows (corresponds to number of input DACs)\n",
    "                                      # In-memory computing hardware therefore has ncol*nrow weights.\n",
    "\n",
    "    self.nl_mult = self.read_non_idealities(non_ideal_file)\n",
    "    self.device = device\n",
    "\n",
    "\n",
    "  def read_non_idealities(self, file_name):\n",
    "    return pd.read_excel(file_name, index_col=0).values\n",
    "\n",
    "  def forward_pass(self):\n",
    "    # Scale input\n",
    "    s_in, x = self.scale_quant(self.data, self.num_bits)\n",
    "\n",
    "    # Convolutional layer 1\n",
    "    x = self.conv2d(x, self.model.conv1.module, s_in, self.idim, self.ifmap, self.ofmap)\n",
    "\n",
    "    # ReLU 1\n",
    "    x = self.relu6(x, 0, 6, self.model.relu1.module)\n",
    "\n",
    "    # Maxpool layer 1\n",
    "    x = self.maxpool2d(x, self.idim - self.knl + 1, self.ofmap, self.maxpool_dim)\n",
    "\n",
    "    # Scale for convolutional layer 2\n",
    "    sin_conv2 = self.model.conv2.module.input_scale\n",
    "    x = self.noscale_quant(x, sin_conv2.cpu(), 0, self.num_bits)\n",
    "    x = self.conv2d(x, self.model.conv2.module, sin_conv2, (self.idim-self.knl+1)/2, self.ofmap, self.conv2kernels)\n",
    "\n",
    "    # ReLU 2\n",
    "    x = self.relu6(x, 0, 6, self.model.relu2.module)\n",
    "\n",
    "    # Maxpool layer 2\n",
    "    x = self.maxpool2d(x, ((self.idim - self.knl+1)/2) - self.knl + 1, self.conv2kernels, self.maxpool_dim)\n",
    "\n",
    "    # Fully connected layer 1\n",
    "    x = x.view(-1, x.size()[1:].numel())  # Flatten outputs of out_maxpool2 layer to feed to FC layers.\n",
    "    x = self.fc(x, self.model.fc1.module, self.fc1_nodes)\n",
    "\n",
    "    # ReLU 3\n",
    "    x = self.relu6(x, 0, 6, self.model.relu3.module)\n",
    "\n",
    "    # Fully connected layer 2\n",
    "    x = self.fc(x.cpu(), self.model.fc2.module, self.fc2_nodes)\n",
    "\n",
    "    # Softmax layer\n",
    "    return F.log_softmax(x, dim=1)\n",
    "\n",
    "  def conv2d(self, x, conv, s_in, idim, ifmap, ofmap):\n",
    "    # Scale input image, conv weight and bias to bits used\n",
    "    sw_conv, filters_conv = self.scale_quant(conv.weight.cpu(), self.num_bits)\n",
    "    bias_conv = conv.bias / (s_in * sw_conv)\n",
    "\n",
    "    # CONV layer (WSAB dataflow - see convolve2D_wsab function)\n",
    "    out_conv = self.convolve2D_wsab(x, filters_conv, bias_conv, 0, self.stride, self.batch, ifmap, ofmap, idim, self.knl, self.ncol, self.nrow)\n",
    "    out_conv = torch.from_numpy(out_conv)\n",
    "    out_conv = out_conv.to(self.device)\n",
    "\n",
    "    # Applying scaling normalization s_in*sw_conv (convert back to float)\n",
    "    return out_conv * s_in * sw_conv\n",
    "\n",
    "  def convolve2D_wsab(self, image, kernel, bias, padding, strides, batch, ifmap, ofmap, idim, knl, ncol, nrow):\n",
    "    xKernShape = knl\n",
    "    yKernShape = knl\n",
    "    xImgShape = idim\n",
    "    yImgShape = idim\n",
    "\n",
    "    # Number of weight block partitions to fit into hardware\n",
    "    block_col = int(np.ceil(ofmap/ncol))\n",
    "    block_row = int(np.ceil(knl*knl*ifmap/nrow))\n",
    "    kernel_flat = np.zeros((block_col*ncol, block_row*nrow))\n",
    "\n",
    "    # Shape of Output Convolution\n",
    "    xOutput = int(((xImgShape - xKernShape + 2 * padding) / strides) + 1)\n",
    "    yOutput = int(((yImgShape - yKernShape + 2 * padding) / strides) + 1)\n",
    "    output = np.zeros((batch, ofmap, xOutput, yOutput))\n",
    "\n",
    "    kernel_flat[0:ofmap,0:ifmap*knl*knl] = torch.reshape(kernel, [ofmap, ifmap*knl*knl]).cpu().numpy()\n",
    "\n",
    "    # Process image by image\n",
    "    for b in range(batch):\n",
    "        # Apply Equal Padding to All Sides\n",
    "        if padding != 0:\n",
    "            imagePadded = np.zeros((idim + padding * 2, idim + padding * 2))\n",
    "            imagePadded[int(padding):int(-1 * padding), int(padding):int(-1 * padding)] = image[b]\n",
    "        else:\n",
    "            imagePadded = image[b]\n",
    "\n",
    "            # Iterate through image\n",
    "\n",
    "        image_block = np.zeros((block_row * nrow, xOutput, yOutput))\n",
    "        otemp = torch.zeros(block_row, block_col*ncol, xOutput, yOutput)\n",
    "        for bc in range(block_col):\n",
    "            for br in range(block_row):\n",
    "                ktemp = torch.zeros(ncol,nrow)\n",
    "                ktemp = kernel_flat[ncol*bc:(bc+1)*ncol, nrow*br:(br+1)*nrow]\n",
    "                ktemp = torch.from_numpy(ktemp)\n",
    "\n",
    "                for y in range(yOutput):\n",
    "                    for x in range(xOutput):\n",
    "                        # Fetch image section x,y, bc,br\n",
    "                        image_block[0:knl * knl * ifmap, x, y] = imagePadded[0:ifmap,\n",
    "                                                                 strides * x: strides * x + xKernShape,\n",
    "                                                                 strides * y: strides * y + yKernShape].reshape(\n",
    "                            knl * knl * ifmap).cpu().numpy()\n",
    "\n",
    "                        itemp = image_block[br*nrow:(br+1)*nrow, x, y].reshape(1, nrow)\n",
    "                        itemp = torch.from_numpy(itemp)\n",
    "\n",
    "\n",
    "                        # Replace this line with Arduino SPI function call\n",
    "                        #otemp[br, bc*ncol:(bc+1)*ncol, x, y] = torch.sum(ktemp*itemp, dim=1)\n",
    "                        otemp[br, bc*ncol:(bc+1)*ncol, x, y] = self.non_linear_mult(ktemp, itemp[0])\n",
    "                        #print(\"Batch:%d,BROW:%d,BCOL:%d,X:%d,Y:%d\" % (b,br,bc,x,y))\n",
    "        output[b] = torch.sum(otemp[:, 0:ofmap, :, :], dim=0) + (bias.reshape(ofmap, 1, 1).cpu()*torch.ones(xOutput, yOutput)).detach().numpy()\n",
    "\n",
    "    return output\n",
    "\n",
    "  def relu6(self, x, min_val, max_val, relu_module):\n",
    "    i = (x >= min_val) * x\n",
    "    out_relu = (i <= max_val) * (i - max_val) + max_val\n",
    "    so_relu = relu_module.output_scale\n",
    "\n",
    "    # Apply fake quantization to relu1 output.\n",
    "    out_relu = self.noscale_quant(out_relu, so_relu, 0, self.num_bits)\n",
    "    return self.dequantize(out_relu, so_relu, 0)\n",
    "\n",
    "  def maxpool2d(self, x, idim, ofmap, knl):\n",
    "    # Maxpool layer - downsample by knl x knl with maxpool (no quantization required for max function)\n",
    "    return torch.from_numpy(self.maxpool2D_wsa(self.batch, x, idim, ofmap, knl))\n",
    "\n",
    "  # Maxpool layer (apply to all ofmaps simultaneously - faster!)\n",
    "  def maxpool2D_wsa(self, batch, image, idim, ofmap, knl):\n",
    "    xKernShape = knl\n",
    "    yKernShape = knl\n",
    "    xImgShape = idim\n",
    "    yImgShape = idim\n",
    "    strides = knl\n",
    "    padding = 0\n",
    "    # Shape of Output Convolution\n",
    "    xOutput = int(((xImgShape - xKernShape + 2 * padding) / strides) + 1)\n",
    "    yOutput = int(((yImgShape - yKernShape + 2 * padding) / strides) + 1)\n",
    "    output = np.zeros((batch, ofmap, xOutput, yOutput))\n",
    "\n",
    "    for b in range(batch):\n",
    "        # Apply Equal Padding to All Sides\n",
    "        if padding != 0:\n",
    "            imagePadded = np.zeros((idim + padding * 2, idim + padding * 2))\n",
    "            imagePadded[int(padding):int(-1 * padding), int(padding):int(-1 * padding)] = image[b]\n",
    "        else:\n",
    "            imagePadded = image[b]\n",
    "\n",
    "        # Iterate through image\n",
    "        for y in range(yOutput):\n",
    "            for x in range(xOutput):\n",
    "                output[b, :, x, y] = torch.amax(\n",
    "                        imagePadded[:, strides * x: strides * x + xKernShape, strides * y: strides * y + yKernShape], dim=(1,2)).detach().cpu().numpy()\n",
    "    return output\n",
    "\n",
    "  def fc(self, x, module, odim):\n",
    "    sin_fc = module.input_scale.cpu()\n",
    "    in_fcs = self.noscale_quant(x, sin_fc, 0, self.num_bits)\n",
    "    sw_fc, filters_fc = self.scale_quant(module.weight.cpu(), self.num_bits)\n",
    "    bias_fc = module.bias.cpu() / (sw_fc * sin_fc)\n",
    "\n",
    "\n",
    "    # FC layer using (WSA dataflow)\n",
    "    # N.B. Need to implement WSAB function to implement 'hardware acceleration' of FC layer.\n",
    "    out_fc = torch.from_numpy(self.fc_custom_wsa(in_fcs, filters_fc, bias_fc, self.batch, odim))\n",
    "\n",
    "    # FC output scaling\n",
    "    out_fcs = out_fc * sin_fc * sw_fc\n",
    "\n",
    "    so_fc = module.output_scale.cpu()\n",
    "    # FC output fake quantization\n",
    "    return self.dequantize(self.noscale_quant(out_fcs, so_fc, 0, self.num_bits), so_fc, 0).to(self.device)\n",
    "\n",
    "  # 'Fake' Quantization function [Jacob et. al]\n",
    "  def quantize(self, real_value, scale, zero_point, qmin, qmax):\n",
    "    transformed_val = zero_point + real_value / scale\n",
    "    clamped_val = torch.clamp(transformed_val, qmin, qmax)\n",
    "    quantized_val = torch.round(clamped_val)\n",
    "    return quantized_val\n",
    "\n",
    "  # 'Fake' Dequantization function [Jacob et. al]\n",
    "  def dequantize(self, quantized_val, scale, zero_point):\n",
    "    real_val = scale * (quantized_val - zero_point)\n",
    "    return real_val\n",
    "\n",
    "  # Scaling function (Jacob et. al)\n",
    "  def scale_quant(self, real_value, num_bits):\n",
    "    qmin = -(2 ** (num_bits - 1) - 1)\n",
    "    qmax = 2 ** (num_bits - 1) - 1\n",
    "    abs_max = torch.abs(real_value).max()\n",
    "    scale = abs_max / (float(qmax - qmin) / 2)\n",
    "    zero_point = 0\n",
    "    quant = self.quantize(real_value, scale, zero_point, qmin, qmax)\n",
    "    return scale, quant\n",
    "\n",
    "  # Scaling function (Jacob et. al)\n",
    "  def noscale_quant(self, real_value, scale, zero_point, num_bits):\n",
    "    qmin = -(2 ** (num_bits - 1) - 1)\n",
    "    qmax = 2 ** (num_bits - 1) - 1\n",
    "    quant = self.quantize(real_value, scale, zero_point, qmin, qmax)\n",
    "    return quant\n",
    "\n",
    "  # Custom FC layer (vector multiplication style - much faster than for loop implementation)\n",
    "  def fc_custom_wsa(self, fc_input, filters, bias, batch, ofmap):\n",
    "    output = np.zeros((batch, ofmap))\n",
    "    for b in range(batch):\n",
    "        outputi = np.zeros(ofmap)\n",
    "        input_batch = fc_input[b]\n",
    "        outputi = torch.sum((input_batch * filters),dim=1) + bias\n",
    "        output[b, :] = outputi.detach().numpy()\n",
    "    return output\n",
    "\n",
    "\n",
    "  # Non-linear jazz\n",
    "  def non_linear_conv(self, kernel, image):\n",
    "    sum = 0\n",
    "    for i in range(len(kernel)):\n",
    "      sum = sum + self.nl_mult[int(kernel[i]+15)][int(image[i]+15)]\n",
    "    return sum\n",
    "\n",
    "  def non_linear_mult(self, kernel, image):\n",
    "    output = np.empty(len(kernel))\n",
    "\n",
    "    for i in range(len(kernel)):\n",
    "      output[i] = self.non_linear_conv(kernel[i, :].tolist(), image.tolist())\n",
    "\n",
    "    return torch.from_numpy(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel dimensions (used in conv layers)\n",
    "knl = 5\n",
    "\n",
    "# Kernel stride\n",
    "stride = 1\n",
    "\n",
    "# Output Feature Maps (same as kernels in conv layer 1)\n",
    "ofmap = 32\n",
    "\n",
    "# Kernels in conv layer 2\n",
    "conv2kernels = 64\n",
    "\n",
    "# Maxpool dimensions (assuming square area)\n",
    "maxpool_dim = 2\n",
    "\n",
    "# Fully Connected 1 output nodes\n",
    "fc1_nodes = 128\n",
    "\n",
    "# Hardware MVM columns\n",
    "ncol = 64\n",
    "\n",
    "# Hardware MVM rows\n",
    "nrow = 64\n",
    "\n",
    "# Non-Idealities File\n",
    "non_ideal_file = '../HardwareSpec/8x8_Mac_result_final.xlsx'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Loss: 1.0538166458129883  Accuracy: 62.22%)\n",
      "\n",
      "32\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 354\u001b[0m\n\u001b[0;32m    351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(output)\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 354\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 104\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    101\u001b[0m test(qmodel, device, test_loader)\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# Evaluate test accuracy of qmodel, with custom forward pass.\u001b[39;00m\n\u001b[1;32m--> 104\u001b[0m \u001b[43mtest_custom\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bits\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 15\u001b[0m, in \u001b[0;36mtest_custom\u001b[1;34m(model, device, test_loader, num_bits)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Custom forward pass function\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m non_linear_model \u001b[38;5;241m=\u001b[39m \u001b[43mforward_custom\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m output \u001b[38;5;241m=\u001b[39m non_linear_model\u001b[38;5;241m.\u001b[39mforward_pass()\n\u001b[0;32m     17\u001b[0m test_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnll_loss(output, target, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mitem()\n",
      "Cell \u001b[1;32mIn[11], line 125\u001b[0m, in \u001b[0;36mforward_custom.__init__\u001b[1;34m(self, model, data, batch, num_bits, ifmap, ofmap, idim, knl, stride, ncol, nrow, device)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnrow \u001b[38;5;241m=\u001b[39m nrow          \u001b[38;5;66;03m# Hardware MVM rows (corresponds to number of input DACs)\u001b[39;00m\n\u001b[0;32m    123\u001b[0m                           \u001b[38;5;66;03m# In-memory computing hardware therefore,\u001b[39;00m\n\u001b[0;32m    124\u001b[0m                           \u001b[38;5;66;03m# has ncol*nrow weights.\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnl_mult \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_non_idealities\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../HardwareSpec/8x8_Mac_result_final.xlsx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m device\n",
      "Cell \u001b[1;32mIn[11], line 130\u001b[0m, in \u001b[0;36mforward_custom.read_non_idealities\u001b[1;34m(self, file_name)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_non_idealities\u001b[39m(\u001b[38;5;28mself\u001b[39m, file_name):\n\u001b[1;32m--> 130\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mvalues\n",
      "File \u001b[1;32mc:\\Users\\ferga\\anaconda3\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:486\u001b[0m, in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    480\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    481\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    482\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    483\u001b[0m     )\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrue_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrue_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfalse_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfalse_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mna_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_default_na\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_default_na\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mna_filter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_filter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdate_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_parser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthousands\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthousands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    506\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecimal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecimal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskipfooter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipfooter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    509\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;66;03m# make sure to close opened file handles\u001b[39;00m\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m should_close:\n",
      "File \u001b[1;32mc:\\Users\\ferga\\anaconda3\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1551\u001b[0m, in \u001b[0;36mExcelFile.parse\u001b[1;34m(self, sheet_name, header, names, index_col, usecols, converters, true_values, false_values, skiprows, nrows, na_values, parse_dates, date_parser, date_format, thousands, comment, skipfooter, dtype_backend, **kwds)\u001b[0m\n\u001b[0;32m   1518\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\n\u001b[0;32m   1519\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1520\u001b[0m     sheet_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1538\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds,\n\u001b[0;32m   1539\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, DataFrame] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mint\u001b[39m, DataFrame]:\n\u001b[0;32m   1540\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1541\u001b[0m \u001b[38;5;124;03m    Parse specified sheet(s) into a DataFrame.\u001b[39;00m\n\u001b[0;32m   1542\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1549\u001b[0m \u001b[38;5;124;03m        DataFrame from the passed in Excel file.\u001b[39;00m\n\u001b[0;32m   1550\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1552\u001b[0m \u001b[43m        \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1553\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1555\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1556\u001b[0m \u001b[43m        \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1558\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrue_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrue_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfalse_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfalse_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1561\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1562\u001b[0m \u001b[43m        \u001b[49m\u001b[43mna_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1563\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdate_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_parser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1566\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthousands\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthousands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1567\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1568\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskipfooter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipfooter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1569\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1570\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1571\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ferga\\anaconda3\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:751\u001b[0m, in \u001b[0;36mBaseExcelReader.parse\u001b[1;34m(self, sheet_name, header, names, index_col, usecols, dtype, true_values, false_values, skiprows, nrows, na_values, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, dtype_backend, **kwds)\u001b[0m\n\u001b[0;32m    748\u001b[0m     sheet \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_sheet_by_index(asheetname)\n\u001b[0;32m    750\u001b[0m file_rows_needed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calc_rows(header, index_col, skiprows, nrows)\n\u001b[1;32m--> 751\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_sheet_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43msheet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_rows_needed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(sheet, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclose\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    753\u001b[0m     \u001b[38;5;66;03m# pyxlsb opens two TemporaryFiles\u001b[39;00m\n\u001b[0;32m    754\u001b[0m     sheet\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\ferga\\anaconda3\\Lib\\site-packages\\pandas\\io\\excel\\_openpyxl.py:602\u001b[0m, in \u001b[0;36mOpenpyxlReader.get_sheet_data\u001b[1;34m(self, sheet, file_rows_needed)\u001b[0m\n\u001b[0;32m    600\u001b[0m data: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[Scalar]] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    601\u001b[0m last_row_with_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 602\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow_number\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msheet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrows\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    603\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconverted_row\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_cell\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    604\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwhile\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconverted_row\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconverted_row\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[0;32m    605\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# trim trailing empty elements\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Users\\ferga\\anaconda3\\Lib\\site-packages\\openpyxl\\worksheet\\_read_only.py:79\u001b[0m, in \u001b[0;36mReadOnlyWorksheet._cells_by_row\u001b[1;34m(self, min_col, min_row, max_col, max_row, values_only)\u001b[0m\n\u001b[0;32m     75\u001b[0m src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_source()\n\u001b[0;32m     76\u001b[0m parser \u001b[38;5;241m=\u001b[39m WorkSheetParser(src, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shared_strings,\n\u001b[0;32m     77\u001b[0m                          data_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mdata_only, epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mepoch,\n\u001b[0;32m     78\u001b[0m                          date_formats\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39m_date_formats)\n\u001b[1;32m---> 79\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmax_row\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_row\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mbreak\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Users\\ferga\\anaconda3\\Lib\\site-packages\\openpyxl\\worksheet\\_reader.py:144\u001b[0m, in \u001b[0;36mparse\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\ferga\\anaconda3\\Lib\\xml\\etree\\ElementTree.py:1251\u001b[0m, in \u001b[0;36miterparse.<locals>.iterator\u001b[1;34m(source)\u001b[0m\n\u001b[0;32m   1249\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m pullparser\u001b[38;5;241m.\u001b[39mread_events()\n\u001b[0;32m   1250\u001b[0m \u001b[38;5;66;03m# load event buffer\u001b[39;00m\n\u001b[1;32m-> 1251\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43msource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[0;32m   1253\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ferga\\anaconda3\\Lib\\zipfile.py:955\u001b[0m, in \u001b[0;36mZipExtFile.read\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m    953\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_offset \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    954\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m n \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eof:\n\u001b[1;32m--> 955\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    956\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(data):\n\u001b[0;32m    957\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_readbuffer \u001b[38;5;241m=\u001b[39m data\n",
      "File \u001b[1;32mc:\\Users\\ferga\\anaconda3\\Lib\\zipfile.py:1031\u001b[0m, in \u001b[0;36mZipExtFile._read1\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m   1029\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compress_type \u001b[38;5;241m==\u001b[39m ZIP_DEFLATED:\n\u001b[0;32m   1030\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(n, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMIN_READ_SIZE)\n\u001b[1;32m-> 1031\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decompressor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecompress\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1032\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eof \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decompressor\u001b[38;5;241m.\u001b[39meof \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1033\u001b[0m                  \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compress_left \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m   1034\u001b[0m                  \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decompressor\u001b[38;5;241m.\u001b[39munconsumed_tail)\n\u001b[0;32m   1035\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eof:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Custom test model function\n",
    "# Only implements forward pass of neural network - no training done.\n",
    "def test_custom(model, device, test_loader, num_bits, ofmap, knl, stride,\n",
    "                conv2kernels, maxpool_dim, fc1_nodes, non_ideal_file, ncol, nrow):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    i = 1\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            batch = data.size()[0]\n",
    "            idim = data[0].size()[1]\n",
    "            ifmap = data[0].size()[0]\n",
    "            fc2_nodes = len(torch.unique(data))\n",
    "\n",
    "            # Custom forward pass function\n",
    "            non_linear_model = forward_custom(model, data, batch, num_bits, ifmap, ofmap, idim, knl, stride, conv2kernels,\n",
    "                                              maxpool_dim, fc1_nodes, fc2_nodes, non_ideal_file, ncol, nrow, device)\n",
    "            output = non_linear_model.forward_pass()\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            print(f\"ITERATION {i}: Accuracy = {correct/i:.2f}% (cumulative = {correct})\")\n",
    "            i = i+1\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('Loss: {}  Accuracy: {}%)\\n'.format(\n",
    "        test_loss, 100 * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "# Test original/default model forward pass.\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('Loss: {}  Accuracy: {}%)\\n'.format(\n",
    "        test_loss, 100 * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization and Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float32)\n",
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# INT8 weight, INT8 activations\n",
    "num_bits = 8\n",
    "# Make sure this matches quantization config from CIFAR10_CNN_Training\n",
    "configure_list = [{\n",
    "    'quant_types': ['weight', 'input'],\n",
    "    'quant_bits': {'weight': num_bits, 'input': num_bits},\n",
    "    'quant_start_step': 2,\n",
    "    'op_names': ['conv1', 'conv2']\n",
    "}, {\n",
    "    'quant_types': ['output'],\n",
    "    'quant_bits': {'output': num_bits},\n",
    "    'quant_start_step': 2,\n",
    "    'op_names': ['relu1', 'relu2', 'relu3']\n",
    "}, {\n",
    "    'quant_types': ['output', 'weight', 'input'],\n",
    "    'quant_bits': {'output': num_bits, 'weight': num_bits, 'input': num_bits},\n",
    "    'quant_start_step': 2,\n",
    "    'op_names': ['fc1', 'fc2'],\n",
    "}]\n",
    "\n",
    "set_quant_scheme_dtype('weight', 'per_tensor_symmetric', 'int')\n",
    "set_quant_scheme_dtype('output', 'per_tensor_symmetric', 'int')\n",
    "set_quant_scheme_dtype('input', 'per_tensor_symmetric', 'int')\n",
    "\n",
    "# Load CIFAR-10 dataset with train/test split sets.\n",
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('data', train=True, download=True, transform=trans),\n",
    "    batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('data', train=False, transform=trans),\n",
    "    batch_size=100, shuffle=True)\n",
    "\n",
    "\n",
    "# Create a NaiveModel object and apply QAT_Quantizer setup\n",
    "model_path = \"cifar_model.pth\"\n",
    "qmodel = NaiveModel().to(device)\n",
    "dummy_input = torch.randn(1, 3, 32, 32).to(device)\n",
    "optimizer = torch.optim.SGD(qmodel.parameters(), lr=0.01, momentum=0.5)\n",
    "# To enable batch normalization folding in the training process, you should\n",
    "# pass dummy_input to the QAT_Quantizer.\n",
    "quantizer = QAT_Quantizer(qmodel, configure_list, optimizer, dummy_input=dummy_input)\n",
    "quantizer.compress()\n",
    "\n",
    "# Load trained model (from CIFAR10_CNN_Training step).\n",
    "state = torch.load(model_path, map_location=device)\n",
    "qmodel.load_state_dict(state, strict=True)\n",
    "qmodel.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate test accuracy with imported quantized model (qmodel) from cifar_model.pth.\n",
    "test(qmodel, device, test_loader)\n",
    "\n",
    "# Evaluate test accuracy of qmodel, with custom forward pass.\n",
    "test_custom(qmodel, device, test_loader, num_bits, ofmap, knl, stride,\n",
    "            conv2kernels, maxpool_dim, fc1_nodes, non_ideal_file, ncol, nrow)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
