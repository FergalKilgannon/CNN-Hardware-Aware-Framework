{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HARDWARE FP MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "import PIL\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from tkinter import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FP CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class forward_custom:\n",
    "  def __init__(self, model, data, batch, num_bits, ifmap,\n",
    "               ofmap, idim, knl, stride, conv2kernels, maxpool_dim, \n",
    "               fc1_nodes, fc2_nodes, non_ideal_file, ncol, nrow, device):\n",
    "    self.model = model\n",
    "    self.data = data\n",
    "    self.batch = batch\n",
    "    self.num_bits = num_bits\n",
    "    self.ifmap = ifmap                # Image (grayscale MNIST)\n",
    "    self.ofmap = ofmap                # Output maps (from applying ofmap knl x knl weight kernels)\n",
    "    self.idim = idim                  # Input image dimensions\n",
    "    self.knl = knl                    # Kernel dimensions\n",
    "    self.stride = stride              # Convolutional stride\n",
    "    self.conv2kernels = conv2kernels  # Output channels to the second conv layer\n",
    "    self.maxpool_dim = maxpool_dim    # Maxpool area\n",
    "    self.fc1_nodes = fc1_nodes        # Fully connected layer 2 output nodes\n",
    "    self.fc2_nodes = fc2_nodes        # Fully connected layer 2 output nodes\n",
    "    self.ncol = ncol                  # Hardware matrix-vector-multiplication (MVM) columns (corresponds to number of output ADCs)\n",
    "    self.nrow = nrow                  # Hardware MVM rows (corresponds to number of input DACs)\n",
    "                                      # In-memory computing hardware therefore has ncol*nrow weights.\n",
    "\n",
    "    self.nl_mult = self.read_non_idealities(non_ideal_file)\n",
    "    self.device = device\n",
    "    self.convWeightC = 1\n",
    "    self.convBiasC = 1\n",
    "    self.reluC = 1\n",
    "    self.fcWeightC = 1\n",
    "    self.fcBiasC = 1\n",
    "    self.fcOPC = 1\n",
    "    self.fcIPC = 1\n",
    "\n",
    "\n",
    "  def read_non_idealities(self, file_name):\n",
    "    return pd.read_excel(file_name, index_col=0).values\n",
    "\n",
    "  def forward_pass(self):\n",
    "    # Scale input\n",
    "    s_in, x = self.scale_quant(self.data, self.num_bits)\n",
    "    \n",
    "    # Convolutional layer 1\n",
    "    x = self.conv2d(x, s_in, self.idim, self.ifmap, self.ofmap)\n",
    "\n",
    "    # ReLU 1\n",
    "    x = self.relu6(x, 0, 6)\n",
    "\n",
    "    # Maxpool layer 1\n",
    "    x = self.maxpool2d(x, self.idim - self.knl + 1, self.ofmap, self.maxpool_dim)\n",
    "    \n",
    "    # Scale for convolutional layer 2\n",
    "    sin_conv2 = self.model[\"conv2IP\"]\n",
    "    x = self.noscale_quant(x, sin_conv2.cpu(), 0, self.num_bits)\n",
    "    x = self.conv2d(x, sin_conv2, (self.idim-self.knl+1)/2, self.ofmap, self.conv2kernels)\n",
    "\n",
    "    # ReLU 2\n",
    "    x = self.relu6(x, 0, 6)\n",
    "\n",
    "    # Maxpool layer 2\n",
    "    x = self.maxpool2d(x, ((self.idim - self.knl+1)/2) - self.knl + 1, self.conv2kernels, self.maxpool_dim)\n",
    "\n",
    "    # Fully connected layer 1\n",
    "    x = x.view(-1, x.size()[1:].numel())  # Flatten outputs of out_maxpool2 layer to feed to FC layers.\n",
    "    x = self.fc(x, self.fc1_nodes)\n",
    "\n",
    "    # ReLU 3\n",
    "    x = self.relu6(x, 0, 6)\n",
    "\n",
    "    # Fully connected layer 2\n",
    "    x = self.fc(x.cpu(), self.fc2_nodes)\n",
    "\n",
    "    # Softmax layer\n",
    "    return F.log_softmax(x, dim=1)\n",
    "\n",
    "  def conv2d(self, x, s_in, idim, ifmap, ofmap):\n",
    "    # Scale input image, conv weight and bias to bits used\n",
    "    sw_conv, filters_conv = self.scale_quant(self.model[f\"conv{self.convWeightC}Weight\"], self.num_bits)\n",
    "    bias_conv = self.model[f\"conv{self.convBiasC}Bias\"] / (s_in * sw_conv)\n",
    "    self.convWeightC += 1\n",
    "    self.convBiasC += 1\n",
    "\n",
    "    # CONV layer (WSAB dataflow - see convolve2D_wsab function)\n",
    "    out_conv = self.convolve2D_wsab(x, filters_conv, bias_conv, 0, self.stride, self.batch, ifmap, ofmap, idim, self.knl, self.ncol, self.nrow)\n",
    "    out_conv = torch.from_numpy(out_conv)\n",
    "    out_conv = out_conv.to(self.device)\n",
    "\n",
    "    # Applying scaling normalization s_in*sw_conv (convert back to float)\n",
    "    return out_conv * s_in * sw_conv\n",
    "\n",
    "\n",
    "  def convolve2D_wsab(self, image, kernel, bias, padding, strides, batch, ifmap, ofmap, idim, knl, ncol, nrow):\n",
    "    xKernShape = knl\n",
    "    yKernShape = knl\n",
    "    xImgShape = idim\n",
    "    yImgShape = idim\n",
    "\n",
    "    # Shape of Output Convolution\n",
    "    xOutput = int(((xImgShape - xKernShape + 2 * padding) / strides) + 1)\n",
    "    yOutput = int(((yImgShape - yKernShape + 2 * padding) / strides) + 1)\n",
    "    output = np.zeros((batch, ofmap, xOutput, yOutput))\n",
    "\n",
    "    # WEIGHT STATIONARY\n",
    "\n",
    "    # Number of weight block partitions to fit into hardware\n",
    "    block_col = int(np.ceil(ofmap/ncol))\n",
    "    block_row = int(np.ceil(knl*knl*ifmap/nrow))\n",
    "    kernel_flat = np.zeros((block_col*ncol, block_row*nrow), dtype='int64')\n",
    "\n",
    "    kernel_flat[0:ofmap,0:ifmap*knl*knl] = torch.reshape(kernel, [ofmap, ifmap*knl*knl]).cpu().numpy()\n",
    "    # data_moved_count = 0\n",
    "    # mac_completed_count = 0\n",
    "    # batch = 1\n",
    "    # Process image by image\n",
    "    for b in range(batch):\n",
    "      # Apply Equal Padding to All Sides\n",
    "      if padding != 0:\n",
    "          imagePadded = np.zeros((idim + padding * 2, idim + padding * 2))\n",
    "          imagePadded[int(padding):int(-1 * padding), int(padding):int(-1 * padding)] = image[b]\n",
    "      else:\n",
    "          imagePadded = image[b]\n",
    "\n",
    "      # Iterate through image\n",
    "\n",
    "      image_block = np.zeros((block_row * nrow, xOutput, yOutput), dtype='int64')\n",
    "      otemp = torch.zeros(block_row, block_col*ncol, xOutput, yOutput)\n",
    "      for bc in range(block_col):\n",
    "          for br in range(block_row):\n",
    "              # data_moved_count += nrow*ncol\n",
    "              ktemp = torch.zeros(ncol,nrow)\n",
    "              ktemp = kernel_flat[ncol*bc:(bc+1)*ncol, nrow*br:(br+1)*nrow]\n",
    "              ktemp = torch.from_numpy(ktemp)\n",
    "\n",
    "              for y in range(yOutput):\n",
    "                  for x in range(xOutput):\n",
    "                      # Fetch image section x,y, bc,br\n",
    "                      image_block[0:knl * knl * ifmap, x, y] = imagePadded[0:ifmap,\n",
    "                                                                strides * x: strides * x + xKernShape,\n",
    "                                                                strides * y: strides * y + yKernShape].reshape(\n",
    "                          knl * knl * ifmap).cpu().numpy()\n",
    "                      # data_moved_count += xKernShape*yKernShape\n",
    "                      # mac_completed_count += ncol\n",
    "                      # data_moved_count += nrow + 1\n",
    "\n",
    "                      itemp = image_block[br*nrow:(br+1)*nrow, x, y].reshape(1, nrow)\n",
    "                      itemp = torch.from_numpy(itemp)\n",
    "                      # Replace this line with Arduino SPI function call\n",
    "                      #otemp[br, bc*ncol:(bc+1)*ncol, x, y] = torch.sum(ktemp*itemp, dim=1)\n",
    "                      otemp[br, bc*ncol:(bc+1)*ncol, x, y] = self.non_linear_mult(ktemp, itemp[0])\n",
    "                      # print(\"Batch:%d,BROW:%d,BCOL:%d,X:%d,Y:%d\" % (b,br,bc,x,y))\n",
    "\n",
    "      output[b] = torch.sum(otemp[:, 0:ofmap, :, :], dim=0) + (bias.reshape(ofmap, 1, 1).cpu()*torch.ones(xOutput, yOutput)).detach().numpy()\n",
    "\n",
    "    # print(f\"Moved {data_moved_count} pieces of data, {mac_completed_count} MAC operations completed\")\n",
    "    # print(ktemp[1000])  # Workaround breakpoint\n",
    "\n",
    "    # # OUTPUT STATIONARY\n",
    "    # # Number of output block partitions to fit into hardware\n",
    "    # block_col = int(np.ceil(xOutput/ncol))\n",
    "    # block_row = int(np.ceil(knl*knl*ifmap/nrow))\n",
    "\n",
    "    # kernel_flat = np.zeros((block_col*ncol, block_row*nrow))\n",
    "    # kernel_flat[0:ofmap,0:ifmap*knl*knl] = torch.reshape(kernel, [ofmap, ifmap*knl*knl]).cpu().numpy()\n",
    "\n",
    "    # data_moved_count = 0\n",
    "    # mac_completed_count = 0\n",
    "    # batch = 1\n",
    "\n",
    "    # # Process image by image\n",
    "    # for b in range(batch):\n",
    "    #   # Apply Equal Padding to All Sides\n",
    "    #   if padding != 0:\n",
    "    #       imagePadded = np.zeros((idim + padding * 2, idim + padding * 2))\n",
    "    #       imagePadded[int(padding):int(-1 * padding), int(padding):int(-1 * padding)] = image[b]\n",
    "    #   else:\n",
    "    #       imagePadded = image[b]\n",
    "\n",
    "    #   image_block = np.zeros((nrow * block_row, ncol*block_col, yOutput))\n",
    "    #   otemp = torch.zeros((nrow * block_row, ofmap, ncol*block_col, yOutput))\n",
    "\n",
    "    #   for y in range(yOutput):\n",
    "    #      for x in range(xOutput):\n",
    "    #         image_block[0:knl * knl * ifmap, x, y] = imagePadded[0:ifmap,\n",
    "    #                                                       strides * x: strides * x + xKernShape,\n",
    "    #                                                       strides * y: strides * y + yKernShape].reshape(\n",
    "    #                 knl * knl * ifmap).cpu().numpy()\n",
    "\n",
    "    #   # Iterate through image\n",
    "    #   for y in range(yOutput):\n",
    "    #       for bc in range(block_col):\n",
    "    #         for br in range(block_row):\n",
    "    #           data_moved_count += nrow*ncol\n",
    "    #           for kerCol in range(ofmap):\n",
    "    #             data_moved_count += nrow\n",
    "    #             ktemp = kernel_flat[kerCol, br*nrow:(br+1)*nrow]\n",
    "    #             ktemp = torch.from_numpy(ktemp)\n",
    "    #             for x in range(ncol):\n",
    "    #                 mac_completed_count += 1\n",
    "    #                 data_moved_count += 1\n",
    "    #                 # Fetch image section x,y, bc,br\n",
    "    #                 itemp = image_block[br*nrow:(br+1)*nrow, (bc*ncol)+x, y].reshape(1, nrow)\n",
    "    #                 itemp = torch.from_numpy(itemp)\n",
    "    #                 # Replace this line with Arduino SPI function call\n",
    "    #                 otemp[br, kerCol, (bc*ncol)+x, y] = torch.sum(ktemp*itemp, dim=1)\n",
    "    #                 # otemp[kerCol, x, y] = self.non_linear_mult(ktemp, itemp[0])\n",
    "    #                 # print(\"Batch:%d,BROW:%d,BCOL:%d,X:%d,Y:%d\" % (b,br,bc,x,y))\n",
    "    #   output[b] = torch.sum(otemp[0:block_row, 0:ofmap, 0:xOutput, 0:yOutput], dim=0) + (bias.reshape(ofmap, 1, 1).cpu()*torch.ones(xOutput, yOutput)).detach().numpy()\n",
    "    #   print(f\"Moved {data_moved_count} pieces of data, {mac_completed_count} MAC operations completed\")\n",
    "    #   print(ktemp[1000]) # Workaround breakpoint\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "  def relu6(self, x, min_val, max_val):\n",
    "    i = (x >= min_val) * x\n",
    "    out_relu = (i <= max_val) * (i - max_val) + max_val\n",
    "    so_relu = self.model[f\"relu{self.reluC}\"]\n",
    "    self.reluC += 1\n",
    "\n",
    "    # Apply fake quantization to relu1 output.\n",
    "    out_relu = self.noscale_quant(out_relu, so_relu, 0, self.num_bits)\n",
    "    return self.dequantize(out_relu, so_relu, 0)\n",
    "\n",
    "\n",
    "  def maxpool2d(self, x, idim, ofmap, knl):\n",
    "    # Maxpool layer - downsample by knl x knl with maxpool (no quantization required for max function)\n",
    "    return torch.from_numpy(self.maxpool2D_wsa(self.batch, x, idim, ofmap, knl))\n",
    "\n",
    "\n",
    "  # Maxpool layer (apply to all ofmaps simultaneously - faster!)\n",
    "  def maxpool2D_wsa(self, batch, image, idim, ofmap, knl):\n",
    "    xKernShape = knl\n",
    "    yKernShape = knl\n",
    "    xImgShape = idim\n",
    "    yImgShape = idim\n",
    "    strides = knl\n",
    "    padding = 0\n",
    "    # Shape of Output Convolution\n",
    "    xOutput = int(((xImgShape - xKernShape + 2 * padding) / strides) + 1)\n",
    "    yOutput = int(((yImgShape - yKernShape + 2 * padding) / strides) + 1)\n",
    "    output = np.zeros((batch, ofmap, xOutput, yOutput))\n",
    "\n",
    "    for b in range(batch):\n",
    "        # Apply Equal Padding to All Sides\n",
    "        if padding != 0:\n",
    "            imagePadded = np.zeros((idim + padding * 2, idim + padding * 2))\n",
    "            imagePadded[int(padding):int(-1 * padding), int(padding):int(-1 * padding)] = image[b]\n",
    "        else:\n",
    "            imagePadded = image[b]\n",
    "\n",
    "        # Iterate through image\n",
    "        for y in range(yOutput):\n",
    "            for x in range(xOutput):\n",
    "                output[b, :, x, y] = torch.amax(\n",
    "                        imagePadded[:, strides * x: strides * x + xKernShape, strides * y: strides * y + yKernShape], dim=(1,2)).detach().cpu().numpy()\n",
    "    return output\n",
    "\n",
    "\n",
    "  def fc(self, x, odim):\n",
    "    sin_fc = self.model[f\"fc{self.fcIPC}IP\"]\n",
    "    in_fcs = self.noscale_quant(x, sin_fc, 0, self.num_bits)\n",
    "    sw_fc, filters_fc = self.scale_quant(self.model[f\"fc{self.fcWeightC}Weight\"], self.num_bits)\n",
    "    bias_fc = self.model[f\"fc{self.fcBiasC}Bias\"] / (sw_fc * sin_fc)\n",
    "\n",
    "\n",
    "    # FC layer using (WSA dataflow)\n",
    "    # N.B. Need to implement WSAB function to implement 'hardware acceleration' of FC layer.\n",
    "    out_fc = torch.from_numpy(self.fc_custom_wsa(in_fcs, filters_fc, bias_fc, self.batch, odim))\n",
    "\n",
    "    # FC output scaling\n",
    "    out_fcs = out_fc * sin_fc * sw_fc\n",
    "\n",
    "    so_fc = self.model[f\"fc{self.fcOPC}OP\"]\n",
    "    self.fcIPC += 1\n",
    "    self.fcOPC += 1\n",
    "    self.fcWeightC += 1\n",
    "    self.fcBiasC += 1\n",
    "    # FC output fake quantization\n",
    "    return self.dequantize(self.noscale_quant(out_fcs, so_fc, 0, self.num_bits), so_fc, 0).to(self.device)\n",
    "\n",
    "\n",
    "  # 'Fake' Quantization function [Jacob et. al]\n",
    "  def quantize(self, real_value, scale, zero_point, qmin, qmax):\n",
    "    transformed_val = zero_point + real_value / scale\n",
    "    clamped_val = torch.clamp(transformed_val, qmin, qmax)\n",
    "    quantized_val = torch.round(clamped_val)\n",
    "    return quantized_val\n",
    "\n",
    "\n",
    "  # 'Fake' Dequantization function [Jacob et. al]\n",
    "  def dequantize(self, quantized_val, scale, zero_point):\n",
    "    real_val = scale * (quantized_val - zero_point)\n",
    "    return real_val\n",
    "\n",
    "\n",
    "  # Scaling function (Jacob et. al)\n",
    "  def scale_quant(self, real_value, num_bits):\n",
    "    qmin = -(2 ** (num_bits - 1) - 1)\n",
    "    qmax = 2 ** (num_bits - 1) - 1\n",
    "    abs_max = torch.abs(real_value).max()\n",
    "    scale = abs_max / (float(qmax - qmin) / 2)\n",
    "    zero_point = 0\n",
    "    quant = self.quantize(real_value, scale, zero_point, qmin, qmax)\n",
    "    return scale, quant\n",
    "\n",
    "\n",
    "  # Scaling function (Jacob et. al)\n",
    "  def noscale_quant(self, real_value, scale, zero_point, num_bits):\n",
    "    qmin = -(2 ** (num_bits - 1) - 1)\n",
    "    qmax = 2 ** (num_bits - 1) - 1\n",
    "    quant = self.quantize(real_value, scale, zero_point, qmin, qmax)\n",
    "    return quant\n",
    "\n",
    "\n",
    "  # Custom FC layer (vector multiplication style - much faster than for loop implementation)\n",
    "  def fc_custom_wsa(self, fc_input, filters, bias, batch, ofmap):\n",
    "    output = np.zeros((batch, ofmap))\n",
    "    for b in range(batch):\n",
    "        outputi = np.zeros(ofmap)\n",
    "        input_batch = fc_input[b]\n",
    "        outputi = torch.sum((input_batch * filters),dim=1) + bias\n",
    "        output[b, :] = outputi.detach().numpy()\n",
    "    return output\n",
    "\n",
    "\n",
    "  # Non-linear look-up\n",
    "  def non_linear_conv(self, kernel, image):\n",
    "    sum = np.int64(0)\n",
    "    for i in range(len(kernel)):\n",
    "      sum = sum + self.nl_mult[int(kernel[i]+(2**(self.num_bits-1))-1)][int(image[i]+(2**(self.num_bits-1))-1)]\n",
    "      #sum = sum + (kernel[i]*image[i] + 20*kernel[i]*np.sin(np.pi*kernel[i]/5)*np.sin(np.pi*image[i]/5))\n",
    "    return sum\n",
    "\n",
    "  def non_linear_mult(self, kernel, image):\n",
    "    output = np.empty(len(kernel))\n",
    "    if len(kernel.shape) == 2:\n",
    "      for i in range(len(kernel)):\n",
    "        output[i] = self.non_linear_conv(kernel[i, :].tolist(), image.tolist())\n",
    "      return torch.from_numpy(output)\n",
    "    else:\n",
    "      output = self.non_linear_conv(kernel.tolist(), image.tolist())\n",
    "      return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel dimensions (used in conv layers)\n",
    "knl = 5\n",
    "\n",
    "# Kernel stride\n",
    "stride = 1\n",
    "\n",
    "# Output Feature Maps (same as kernels in conv layer 1)\n",
    "ofmap = 16\n",
    "\n",
    "# Kernels in conv layer 2\n",
    "conv2kernels = 32\n",
    "\n",
    "# Maxpool dimensions (assuming square area)\n",
    "maxpool_dim = 2\n",
    "\n",
    "# Fully Connected 1 output nodes\n",
    "fc1_nodes = 50\n",
    "\n",
    "# Hardware MVM columns\n",
    "ncol = 16\n",
    "\n",
    "# Hardware MVM rows\n",
    "nrow = 32\n",
    "\n",
    "# Non-Idealities File\n",
    "non_ideal_file = '../HardwareSpec/8x8_Mac_result_final.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class customDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, file_path, transform=None):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "\n",
    "        self.number = PIL.Image.open(file_path)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        sample = self.number\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_input():\n",
    "    width = 300  # canvas width\n",
    "    height = 300 # canvas height\n",
    "    black = (0, 0, 0) # canvas back\n",
    "\n",
    "    def save_and_exit():\n",
    "        # save image to hard drive\n",
    "        filename = \"data/user_input.jpg\"\n",
    "        output_image.save(filename)\n",
    "        master.destroy()\n",
    "\n",
    "    def paint(event):\n",
    "        x1, y1 = (event.x - 1), (event.y - 1)\n",
    "        x2, y2 = (event.x + 1), (event.y + 1)\n",
    "        canvas.create_oval(x1, y1, x2, y2, fill=\"white\",width=40)\n",
    "        draw.line([x1, y1, x2, y2],fill=\"white\",width=40)\n",
    "\n",
    "    master = Tk()\n",
    "\n",
    "    # create a tkinter canvas to draw on\n",
    "    canvas = Canvas(master, width=width, height=height, bg='black')\n",
    "    canvas.pack()\n",
    "\n",
    "    # create an empty PIL image and draw object to draw on\n",
    "    output_image = PIL.Image.new(\"RGB\", (width, height), black)\n",
    "    draw = PIL.ImageDraw.Draw(output_image)\n",
    "    canvas.pack(expand=YES, fill=BOTH)\n",
    "    canvas.bind(\"<B1-Motion>\", paint)\n",
    "\n",
    "    # add a button to save the image\n",
    "    button=Button(text=\"save\",command=save_and_exit)\n",
    "    button.pack()\n",
    "\n",
    "    master.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_CNN():\n",
    "    torch.set_default_dtype(torch.float32)\n",
    "    torch.manual_seed(0)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # INT9 weight, INT9 activations\n",
    "    num_bits = 9\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    user_image = PIL.Image.open('data/user_input.jpg')\n",
    "    user_image = user_image.resize((28,28), PIL.Image.LANCZOS).convert('L')\n",
    "    tensor_image = transform(user_image)\n",
    "\n",
    "    #tensor_image = transform(PIL.Image.open('data/eight.jpg'))\n",
    "\n",
    "    idim = tensor_image.size()[1]\n",
    "    ifmap = tensor_image.size()[0]\n",
    "    fc2_nodes = 10\n",
    "\n",
    "    # Create a NaiveModel object and apply QAT_Quantizer setup\n",
    "    model_path = \"9bit_quantised_model.pt\"\n",
    "\n",
    "    # Load trained model (from MNIST_CNN_Training step).\n",
    "    qmodel = torch.load(model_path, map_location=device)\n",
    "\n",
    "    return tensor_image, device, qmodel, num_bits, ifmap, idim, fc2_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number written is 3\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "capture_input()\n",
    "tensor_image, device, model, num_bits, ifmap, idim, fc2_nodes = setup_CNN()\n",
    "plt.imshow(tensor_image.permute(1, 2, 0))\n",
    "\n",
    "with torch.no_grad():\n",
    "        data = tensor_image.unsqueeze(0)\n",
    "        data = data.to(device)\n",
    "        batch = 1\n",
    "\n",
    "        # Custom forward pass function\n",
    "        non_linear_model = forward_custom(model, data, batch, num_bits, ifmap, ofmap, idim, knl, stride, conv2kernels,\n",
    "                                            maxpool_dim, fc1_nodes, fc2_nodes, non_ideal_file, ncol, nrow, device)\n",
    "        output = non_linear_model.forward_pass()\n",
    "        pred = output.argmax(dim=1, keepdim=True)[0][0]\n",
    "        print(f\"Number written is {pred}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
