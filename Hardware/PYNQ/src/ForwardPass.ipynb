{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HARDWARE FP MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from naive_mnistAVGPOOL import NaiveModel\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "from nni.algorithms.compression.pytorch.quantization import QAT_Quantizer\n",
    "from nni.compression.pytorch.quantization.settings import set_quant_scheme_dtype\n",
    "import PIL\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from tkinter import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FP CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class forward_custom:\n",
    "  def __init__(self, model, data, batch, num_bits, ifmap,\n",
    "               ofmap, idim, knl, stride, conv2kernels, maxpool_dim, \n",
    "               fc1_nodes, fc2_nodes, non_ideal_file, ncol, nrow, device):\n",
    "    self.model = model\n",
    "    self.data = data\n",
    "    self.batch = batch\n",
    "    self.num_bits = num_bits\n",
    "    self.ifmap = ifmap                # Image (grayscale MNIST)\n",
    "    self.ofmap = ofmap                # Output maps (from applying ofmap knl x knl weight kernels)\n",
    "    self.idim = idim                  # Input image dimensions\n",
    "    self.knl = knl                    # Kernel dimensions\n",
    "    self.stride = stride              # Convolutional stride\n",
    "    self.conv2kernels = conv2kernels  # Output channels to the second conv layer\n",
    "    self.maxpool_dim = maxpool_dim    # Maxpool area\n",
    "    self.fc1_nodes = fc1_nodes        # Fully connected layer 2 output nodes\n",
    "    self.fc2_nodes = fc2_nodes        # Fully connected layer 2 output nodes\n",
    "    self.ncol = ncol                  # Hardware matrix-vector-multiplication (MVM) columns (corresponds to number of output ADCs)\n",
    "    self.nrow = nrow                  # Hardware MVM rows (corresponds to number of input DACs)\n",
    "                                      # In-memory computing hardware therefore has ncol*nrow weights.\n",
    "\n",
    "    self.nl_mult = self.read_non_idealities(non_ideal_file)\n",
    "    self.device = device\n",
    "\n",
    "\n",
    "  def read_non_idealities(self, file_name):\n",
    "    return pd.read_excel(file_name, index_col=0).values\n",
    "\n",
    "  def forward_pass(self):\n",
    "    # Scale input\n",
    "    s_in, x = self.scale_quant(self.data, self.num_bits)\n",
    "    \n",
    "    # Convolutional layer 1\n",
    "    x = self.conv2d(x, self.model.conv1.module, s_in, self.idim, self.ifmap, self.ofmap)\n",
    "\n",
    "    # ReLU 1\n",
    "    x = self.relu6(x, 0, 6, self.model.relu1.module)\n",
    "\n",
    "    # Maxpool layer 1\n",
    "    x = self.avgpool2d(x, self.idim - self.knl + 1, self.ofmap, self.maxpool_dim)\n",
    "    \n",
    "    # Scale for convolutional layer 2\n",
    "    sin_conv2 = self.model.conv2.module.input_scale\n",
    "    x = self.noscale_quant(x, sin_conv2.cpu(), 0, self.num_bits)\n",
    "    x = self.conv2d(x, self.model.conv2.module, sin_conv2, (self.idim-self.knl+1)/2, self.ofmap, self.conv2kernels)\n",
    "\n",
    "    # ReLU 2\n",
    "    x = self.relu6(x, 0, 6, self.model.relu2.module)\n",
    "\n",
    "    # Maxpool layer 2\n",
    "    x = self.avgpool2d(x, ((self.idim - self.knl+1)/2) - self.knl + 1, self.conv2kernels, self.maxpool_dim)\n",
    "\n",
    "    # Fully connected layer 1\n",
    "    x = x.view(-1, x.size()[1:].numel())  # Flatten outputs of out_maxpool2 layer to feed to FC layers.\n",
    "    x = self.fc(x, self.model.fc1.module, self.fc1_nodes)\n",
    "\n",
    "    # ReLU 3\n",
    "    x = self.relu6(x, 0, 6, self.model.relu3.module)\n",
    "\n",
    "    # Fully connected layer 2\n",
    "    x = self.fc(x.cpu(), self.model.fc2.module, self.fc2_nodes)\n",
    "\n",
    "    # Softmax layer\n",
    "    return F.log_softmax(x, dim=1)\n",
    "\n",
    "  def conv2d(self, x, conv, s_in, idim, ifmap, ofmap):\n",
    "    # Scale input image, conv weight and bias to bits used\n",
    "    sw_conv, filters_conv = self.scale_quant(conv.weight.cpu(), self.num_bits)\n",
    "    bias_conv = conv.bias / (s_in * sw_conv)\n",
    "\n",
    "    # CONV layer (WSAB dataflow - see convolve2D_wsab function)\n",
    "    out_conv = self.convolve2D_wsab(x, filters_conv, bias_conv, 0, self.stride, self.batch, ifmap, ofmap, idim, self.knl, self.ncol, self.nrow)\n",
    "    out_conv = torch.from_numpy(out_conv)\n",
    "    out_conv = out_conv.to(self.device)\n",
    "\n",
    "    # Applying scaling normalization s_in*sw_conv (convert back to float)\n",
    "    return out_conv * s_in * sw_conv\n",
    "\n",
    "\n",
    "  def convolve2D_wsab(self, image, kernel, bias, padding, strides, batch, ifmap, ofmap, idim, knl, ncol, nrow):\n",
    "    xKernShape = knl\n",
    "    yKernShape = knl\n",
    "    xImgShape = idim\n",
    "    yImgShape = idim\n",
    "\n",
    "    # Shape of Output Convolution\n",
    "    xOutput = int(((xImgShape - xKernShape + 2 * padding) / strides) + 1)\n",
    "    yOutput = int(((yImgShape - yKernShape + 2 * padding) / strides) + 1)\n",
    "    output = np.zeros((batch, ofmap, xOutput, yOutput))\n",
    "\n",
    "    # WEIGHT STATIONARY\n",
    "\n",
    "    # Number of weight block partitions to fit into hardware\n",
    "    block_col = int(np.ceil(ofmap/ncol))\n",
    "    block_row = int(np.ceil(knl*knl*ifmap/nrow))\n",
    "    kernel_flat = np.zeros((block_col*ncol, block_row*nrow), dtype='int64')\n",
    "\n",
    "    kernel_flat[0:ofmap,0:ifmap*knl*knl] = torch.reshape(kernel, [ofmap, ifmap*knl*knl]).cpu().numpy()\n",
    "    # data_moved_count = 0\n",
    "    # mac_completed_count = 0\n",
    "    # batch = 1\n",
    "    # Process image by image\n",
    "    for b in range(batch):\n",
    "      # Apply Equal Padding to All Sides\n",
    "      if padding != 0:\n",
    "          imagePadded = np.zeros((idim + padding * 2, idim + padding * 2))\n",
    "          imagePadded[int(padding):int(-1 * padding), int(padding):int(-1 * padding)] = image[b]\n",
    "      else:\n",
    "          imagePadded = image[b]\n",
    "\n",
    "      # Iterate through image\n",
    "\n",
    "      image_block = np.zeros((block_row * nrow, xOutput, yOutput), dtype='int64')\n",
    "      otemp = torch.zeros(block_row, block_col*ncol, xOutput, yOutput)\n",
    "      for bc in range(block_col):\n",
    "          for br in range(block_row):\n",
    "              # data_moved_count += nrow*ncol\n",
    "              ktemp = torch.zeros(ncol,nrow)\n",
    "              ktemp = kernel_flat[ncol*bc:(bc+1)*ncol, nrow*br:(br+1)*nrow]\n",
    "              ktemp = torch.from_numpy(ktemp)\n",
    "\n",
    "              for y in range(yOutput):\n",
    "                  for x in range(xOutput):\n",
    "                      # Fetch image section x,y, bc,br\n",
    "                      image_block[0:knl * knl * ifmap, x, y] = imagePadded[0:ifmap,\n",
    "                                                                strides * x: strides * x + xKernShape,\n",
    "                                                                strides * y: strides * y + yKernShape].reshape(\n",
    "                          knl * knl * ifmap).cpu().numpy()\n",
    "                      # data_moved_count += xKernShape*yKernShape\n",
    "                      # mac_completed_count += ncol\n",
    "                      # data_moved_count += nrow + 1\n",
    "\n",
    "                      itemp = image_block[br*nrow:(br+1)*nrow, x, y].reshape(1, nrow)\n",
    "                      itemp = torch.from_numpy(itemp)\n",
    "                      # Replace this line with Arduino SPI function call\n",
    "                      #otemp[br, bc*ncol:(bc+1)*ncol, x, y] = torch.sum(ktemp*itemp, dim=1)\n",
    "                      otemp[br, bc*ncol:(bc+1)*ncol, x, y] = self.non_linear_mult(ktemp, itemp[0])\n",
    "                      # print(\"Batch:%d,BROW:%d,BCOL:%d,X:%d,Y:%d\" % (b,br,bc,x,y))\n",
    "\n",
    "      output[b] = torch.sum(otemp[:, 0:ofmap, :, :], dim=0) + (bias.reshape(ofmap, 1, 1).cpu()*torch.ones(xOutput, yOutput)).detach().numpy()\n",
    "\n",
    "    # print(f\"Moved {data_moved_count} pieces of data, {mac_completed_count} MAC operations completed\")\n",
    "    # print(ktemp[1000])  # Workaround breakpoint\n",
    "\n",
    "    # # OUTPUT STATIONARY\n",
    "    # # Number of output block partitions to fit into hardware\n",
    "    # block_col = int(np.ceil(xOutput/ncol))\n",
    "    # block_row = int(np.ceil(knl*knl*ifmap/nrow))\n",
    "\n",
    "    # kernel_flat = np.zeros((block_col*ncol, block_row*nrow))\n",
    "    # kernel_flat[0:ofmap,0:ifmap*knl*knl] = torch.reshape(kernel, [ofmap, ifmap*knl*knl]).cpu().numpy()\n",
    "\n",
    "    # data_moved_count = 0\n",
    "    # mac_completed_count = 0\n",
    "    # batch = 1\n",
    "\n",
    "    # # Process image by image\n",
    "    # for b in range(batch):\n",
    "    #   # Apply Equal Padding to All Sides\n",
    "    #   if padding != 0:\n",
    "    #       imagePadded = np.zeros((idim + padding * 2, idim + padding * 2))\n",
    "    #       imagePadded[int(padding):int(-1 * padding), int(padding):int(-1 * padding)] = image[b]\n",
    "    #   else:\n",
    "    #       imagePadded = image[b]\n",
    "\n",
    "    #   image_block = np.zeros((nrow * block_row, ncol*block_col, yOutput))\n",
    "    #   otemp = torch.zeros((nrow * block_row, ofmap, ncol*block_col, yOutput))\n",
    "\n",
    "    #   for y in range(yOutput):\n",
    "    #      for x in range(xOutput):\n",
    "    #         image_block[0:knl * knl * ifmap, x, y] = imagePadded[0:ifmap,\n",
    "    #                                                       strides * x: strides * x + xKernShape,\n",
    "    #                                                       strides * y: strides * y + yKernShape].reshape(\n",
    "    #                 knl * knl * ifmap).cpu().numpy()\n",
    "\n",
    "    #   # Iterate through image\n",
    "    #   for y in range(yOutput):\n",
    "    #       for bc in range(block_col):\n",
    "    #         for br in range(block_row):\n",
    "    #           data_moved_count += nrow*ncol\n",
    "    #           for kerCol in range(ofmap):\n",
    "    #             data_moved_count += nrow\n",
    "    #             ktemp = kernel_flat[kerCol, br*nrow:(br+1)*nrow]\n",
    "    #             ktemp = torch.from_numpy(ktemp)\n",
    "    #             for x in range(ncol):\n",
    "    #                 mac_completed_count += 1\n",
    "    #                 data_moved_count += 1\n",
    "    #                 # Fetch image section x,y, bc,br\n",
    "    #                 itemp = image_block[br*nrow:(br+1)*nrow, (bc*ncol)+x, y].reshape(1, nrow)\n",
    "    #                 itemp = torch.from_numpy(itemp)\n",
    "    #                 # Replace this line with Arduino SPI function call\n",
    "    #                 otemp[br, kerCol, (bc*ncol)+x, y] = torch.sum(ktemp*itemp, dim=1)\n",
    "    #                 # otemp[kerCol, x, y] = self.non_linear_mult(ktemp, itemp[0])\n",
    "    #                 # print(\"Batch:%d,BROW:%d,BCOL:%d,X:%d,Y:%d\" % (b,br,bc,x,y))\n",
    "    #   output[b] = torch.sum(otemp[0:block_row, 0:ofmap, 0:xOutput, 0:yOutput], dim=0) + (bias.reshape(ofmap, 1, 1).cpu()*torch.ones(xOutput, yOutput)).detach().numpy()\n",
    "    #   print(f\"Moved {data_moved_count} pieces of data, {mac_completed_count} MAC operations completed\")\n",
    "    #   print(ktemp[1000]) # Workaround breakpoint\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "  def relu6(self, x, min_val, max_val, relu_module):\n",
    "    i = (x >= min_val) * x\n",
    "    out_relu = (i <= max_val) * (i - max_val) + max_val\n",
    "    so_relu = relu_module.output_scale\n",
    "\n",
    "    # Apply fake quantization to relu1 output.\n",
    "    out_relu = self.noscale_quant(out_relu, so_relu, 0, self.num_bits)\n",
    "    return self.dequantize(out_relu, so_relu, 0)\n",
    "\n",
    "\n",
    "  def maxpool2d(self, x, idim, ofmap, knl):\n",
    "    # Maxpool layer - downsample by knl x knl with maxpool (no quantization required for max function)\n",
    "    return torch.from_numpy(self.maxpool2D_wsa(self.batch, x, idim, ofmap, knl))\n",
    "\n",
    "\n",
    "  # Maxpool layer (apply to all ofmaps simultaneously - faster!)\n",
    "  def maxpool2D_wsa(self, batch, image, idim, ofmap, knl):\n",
    "    xKernShape = knl\n",
    "    yKernShape = knl\n",
    "    xImgShape = idim\n",
    "    yImgShape = idim\n",
    "    strides = knl\n",
    "    padding = 0\n",
    "    # Shape of Output Convolution\n",
    "    xOutput = int(((xImgShape - xKernShape + 2 * padding) / strides) + 1)\n",
    "    yOutput = int(((yImgShape - yKernShape + 2 * padding) / strides) + 1)\n",
    "    output = np.zeros((batch, ofmap, xOutput, yOutput))\n",
    "\n",
    "    for b in range(batch):\n",
    "        # Apply Equal Padding to All Sides\n",
    "        if padding != 0:\n",
    "            imagePadded = np.zeros((idim + padding * 2, idim + padding * 2))\n",
    "            imagePadded[int(padding):int(-1 * padding), int(padding):int(-1 * padding)] = image[b]\n",
    "        else:\n",
    "            imagePadded = image[b]\n",
    "\n",
    "        # Iterate through image\n",
    "        for y in range(yOutput):\n",
    "            for x in range(xOutput):\n",
    "                output[b, :, x, y] = torch.amax(\n",
    "                        imagePadded[:, strides * x: strides * x + xKernShape, strides * y: strides * y + yKernShape], dim=(1,2)).detach().cpu().numpy()\n",
    "    return output\n",
    "  \n",
    "\n",
    "  def avgpool2d(self, x, idim, ofmap, knl):\n",
    "    # Maxpool layer - downsample by knl x knl with maxpool (no quantization required for max function)\n",
    "    return torch.from_numpy(self.avgpool2D_wsa(self.batch, x, idim, ofmap, knl))\n",
    "  \n",
    "\n",
    "  # Maxpool layer (apply to all ofmaps simultaneously - faster!)\n",
    "  def avgpool2D_wsa(self, batch, image, idim, ofmap, knl):\n",
    "    xKernShape = knl\n",
    "    yKernShape = knl\n",
    "    xImgShape = idim\n",
    "    yImgShape = idim\n",
    "    strides = knl\n",
    "    padding = 0\n",
    "    # Shape of Output Convolution\n",
    "    xOutput = int(((xImgShape - xKernShape + 2 * padding) / strides) + 1)\n",
    "    yOutput = int(((yImgShape - yKernShape + 2 * padding) / strides) + 1)\n",
    "    output = np.zeros((batch, ofmap, xOutput, yOutput))\n",
    "\n",
    "    for b in range(batch):\n",
    "        # Apply Equal Padding to All Sides\n",
    "        if padding != 0:\n",
    "            imagePadded = np.zeros((idim + padding * 2, idim + padding * 2))\n",
    "            imagePadded[int(padding):int(-1 * padding), int(padding):int(-1 * padding)] = image[b]\n",
    "        else:\n",
    "            imagePadded = image[b]\n",
    "\n",
    "        # Iterate through image\n",
    "        for y in range(yOutput):\n",
    "            for x in range(xOutput):\n",
    "                for channel in range(ofmap):\n",
    "                  # output[b, channel, x, y] = torch.sum(\n",
    "                  #    imagePadded[channel, strides * x: strides * x + xKernShape, strides * y: strides * y + yKernShape].reshape(knl*knl) \n",
    "                  #    * torch.ones(xKernShape*yKernShape), dim=0)/(knl*knl)\n",
    "                  output[b, channel, x, y] = self.non_linear_mult(\n",
    "                        imagePadded[channel, strides * x: strides * x + xKernShape, strides * y: strides * y + yKernShape].reshape(knl*knl),\n",
    "                        torch.ones(xKernShape*yKernShape))/(knl*knl)\n",
    "    return output\n",
    "\n",
    "\n",
    "  def fc(self, x, module, odim):\n",
    "    sin_fc = module.input_scale.cpu()\n",
    "    in_fcs = self.noscale_quant(x, sin_fc, 0, self.num_bits)\n",
    "    sw_fc, filters_fc = self.scale_quant(module.weight.cpu(), self.num_bits)\n",
    "    bias_fc = module.bias.cpu() / (sw_fc * sin_fc)\n",
    "\n",
    "\n",
    "    # FC layer using (WSA dataflow)\n",
    "    # N.B. Need to implement WSAB function to implement 'hardware acceleration' of FC layer.\n",
    "    out_fc = torch.from_numpy(self.fc_custom_wsab(in_fcs, filters_fc, bias_fc, self.batch, odim, self.ncol, self.nrow))\n",
    "\n",
    "    # FC output scaling\n",
    "    out_fcs = out_fc * sin_fc * sw_fc\n",
    "\n",
    "    so_fc = module.output_scale.cpu()\n",
    "    # FC output fake quantization\n",
    "    return self.dequantize(self.noscale_quant(out_fcs, so_fc, 0, self.num_bits), so_fc, 0).to(self.device)\n",
    "\n",
    "\n",
    "  # 'Fake' Quantization function [Jacob et. al]\n",
    "  def quantize(self, real_value, scale, zero_point, qmin, qmax):\n",
    "    transformed_val = zero_point + real_value / scale\n",
    "    clamped_val = torch.clamp(transformed_val, qmin, qmax)\n",
    "    quantized_val = torch.round(clamped_val)\n",
    "    return quantized_val\n",
    "\n",
    "\n",
    "  # 'Fake' Dequantization function [Jacob et. al]\n",
    "  def dequantize(self, quantized_val, scale, zero_point):\n",
    "    real_val = scale * (quantized_val - zero_point)\n",
    "    return real_val\n",
    "\n",
    "\n",
    "  # Scaling function (Jacob et. al)\n",
    "  def scale_quant(self, real_value, num_bits):\n",
    "    qmin = -(2 ** (num_bits - 1) - 1)\n",
    "    qmax = 2 ** (num_bits - 1) - 1\n",
    "    abs_max = torch.abs(real_value).max()\n",
    "    scale = abs_max / (float(qmax - qmin) / 2)\n",
    "    zero_point = 0\n",
    "    quant = self.quantize(real_value, scale, zero_point, qmin, qmax)\n",
    "    return scale, quant\n",
    "\n",
    "\n",
    "  # Scaling function (Jacob et. al)\n",
    "  def noscale_quant(self, real_value, scale, zero_point, num_bits):\n",
    "    qmin = -(2 ** (num_bits - 1) - 1)\n",
    "    qmax = 2 ** (num_bits - 1) - 1\n",
    "    quant = self.quantize(real_value, scale, zero_point, qmin, qmax)\n",
    "    return quant\n",
    "\n",
    "\n",
    "  # Custom FC layer (vector multiplication style - much faster than for loop implementation)\n",
    "  def fc_custom_wsa(self, fc_input, filters, bias, batch, ofmap):\n",
    "    print(fc_input.size()[1])\n",
    "    print(fc_input)\n",
    "    output = np.zeros((batch, ofmap))\n",
    "    for b in range(batch):\n",
    "        outputi = np.zeros(ofmap)\n",
    "        input_batch = fc_input[b]\n",
    "        print((input_batch*filters).size())\n",
    "        outputi = torch.sum((input_batch * filters),dim=1) + bias\n",
    "        output[b, :] = outputi.detach().numpy()\n",
    "    return output\n",
    "  \n",
    "  \n",
    "  def fc_custom_wsab(self, fc_input, filters, bias, batch, ofmap, ncol, nrow):\n",
    "    output = np.zeros((batch, ofmap))\n",
    "    for b in range(batch):\n",
    "        # WEIGHT STATIONARY\n",
    "\n",
    "        # Iterate through image\n",
    "        block_col = int(np.ceil(ofmap/ncol))\n",
    "        block_row = int(np.ceil(fc_input.size()[1]/nrow))\n",
    "        filters_flat = np.zeros((block_col*ncol, block_row*nrow), dtype='int64')\n",
    "        image_col = np.zeros((block_row*nrow), dtype='int64')\n",
    "\n",
    "        filters_flat[0:ofmap,0:fc_input.size()[1]] = torch.reshape(filters, [ofmap, fc_input.size()[1]]).cpu().numpy()\n",
    "        image_col[0:fc_input.size()[1]] = fc_input[b, :]\n",
    "\n",
    "        otemp = torch.zeros(block_row, block_col*ncol)\n",
    "\n",
    "        for bc in range(block_col):\n",
    "            for br in range(block_row):\n",
    "                ftemp = torch.zeros(ncol,nrow)\n",
    "                ftemp = filters_flat[ncol*bc:(bc+1)*ncol, nrow*br:(br+1)*nrow]\n",
    "                ftemp = torch.from_numpy(ftemp)\n",
    "\n",
    "                itemp = image_col[br*nrow:(br+1)*nrow].reshape(1, nrow)\n",
    "                itemp = torch.from_numpy(itemp)\n",
    "\n",
    "                for col in range(ncol):\n",
    "                  # Replace this line with Arduino SPI function call\n",
    "                  otemp[br, (bc*ncol)+col] = torch.sum(ftemp[col, :]*itemp[0], dim=0)\n",
    "                  #otemp[br, (bc*ncol)+col] = self.non_linear_mult(ftemp[col, :], itemp[0])\n",
    "                  # print(\"Batch:%d,BROW:%d,BCOL:%d,X:%d,Y:%d\" % (b,br,bc,x,y))\n",
    "        output[b] = torch.sum(otemp[:, 0:ofmap], dim=0) + (bias.reshape(ofmap).cpu()*torch.ones(ofmap)).detach().numpy()\n",
    "    return output\n",
    "\n",
    "\n",
    "  # Non-linear look-up\n",
    "  def non_linear_conv(self, kernel, image):\n",
    "    sum = 0\n",
    "    if self.num_bits == 9 or self.num_bits == 8:\n",
    "       bit = 9\n",
    "    else:\n",
    "       bit = 5\n",
    "    for i in range(len(kernel)):\n",
    "      sum = sum + self.nl_mult[int(kernel[i]+(2**(bit-1))-1)][int(image[i]+(2**(bit-1))-1)]\n",
    "      #sum = sum + (kernel[i]*image[i] + 20*kernel[i]*np.sin(np.pi*kernel[i]/5)*np.sin(np.pi*image[i]/5))\n",
    "    return sum\n",
    "\n",
    "  def non_linear_mult(self, kernel, image):\n",
    "    output = np.empty(len(kernel))\n",
    "    if len(kernel.shape) == 2:\n",
    "      for i in range(len(kernel)):\n",
    "        output[i] = self.non_linear_conv(kernel[i, :].tolist(), image.tolist())\n",
    "      return torch.from_numpy(output)\n",
    "    else:\n",
    "      output = self.non_linear_conv(kernel.tolist(), image.tolist())\n",
    "      return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel dimensions (used in conv layers)\n",
    "knl = 5\n",
    "\n",
    "# Kernel stride\n",
    "stride = 1\n",
    "\n",
    "# Output Feature Maps (same as kernels in conv layer 1)\n",
    "ofmap = 16\n",
    "\n",
    "# Kernels in conv layer 2\n",
    "conv2kernels = 32\n",
    "\n",
    "# Maxpool dimensions (assuming square area)\n",
    "maxpool_dim = 2\n",
    "\n",
    "# Fully Connected 1 output nodes\n",
    "fc1_nodes = 50\n",
    "\n",
    "# Hardware MVM columns\n",
    "ncol = 16\n",
    "\n",
    "# Hardware MVM rows\n",
    "nrow = 32\n",
    "\n",
    "# Non-Idealities File\n",
    "non_ideal_file = '../HardwareSpec/4x4_Mac_result_final.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_input():\n",
    "    width = 300  # canvas width\n",
    "    height = 300 # canvas height\n",
    "    black = (0, 0, 0) # canvas back\n",
    "\n",
    "    def save_and_exit():\n",
    "        # save image to hard drive\n",
    "        filename = \"data/user_input.jpg\"\n",
    "        output_image.save(filename)\n",
    "        master.destroy()\n",
    "\n",
    "    def paint(event):\n",
    "        x1, y1 = (event.x - 1), (event.y - 1)\n",
    "        x2, y2 = (event.x + 1), (event.y + 1)\n",
    "        canvas.create_oval(x1, y1, x2, y2, fill=\"white\",width=40)\n",
    "        draw.line([x1, y1, x2, y2],fill=\"white\",width=40)\n",
    "\n",
    "    master = Tk()\n",
    "\n",
    "    # create a tkinter canvas to draw on\n",
    "    canvas = Canvas(master, width=width, height=height, bg='black')\n",
    "    canvas.pack()\n",
    "\n",
    "    # create an empty PIL image and draw object to draw on\n",
    "    output_image = PIL.Image.new(\"RGB\", (width, height), black)\n",
    "    draw = PIL.ImageDraw.Draw(output_image)\n",
    "    canvas.pack(expand=YES, fill=BOTH)\n",
    "    canvas.bind(\"<B1-Motion>\", paint)\n",
    "\n",
    "    # add a button to save the image\n",
    "    button=Button(text=\"save\",command=save_and_exit)\n",
    "    button.pack()\n",
    "\n",
    "    master.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_CNN():\n",
    "    torch.set_default_dtype(torch.float32)\n",
    "    torch.manual_seed(0)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # INT9 weight, INT9 activations\n",
    "    num_bits = 4\n",
    "    # Make sure this matches quantization config from MNIST_CNN_Training\n",
    "    configure_list = [{\n",
    "        'quant_types': ['weight', 'input'],\n",
    "        'quant_bits': {'weight': num_bits, 'input': num_bits},\n",
    "        'quant_start_step': 2,\n",
    "        'op_names': ['conv1', 'conv2']\n",
    "    }, {\n",
    "        'quant_types': ['output'],\n",
    "        'quant_bits': {'output': num_bits},\n",
    "        'quant_start_step': 2,\n",
    "        'op_names': ['relu1', 'relu2', 'relu3']\n",
    "    }, {\n",
    "        'quant_types': ['output', 'weight', 'input'],\n",
    "        'quant_bits': {'output': num_bits, 'weight': num_bits, 'input': num_bits},\n",
    "        'quant_start_step': 2,\n",
    "        'op_names': ['fc1', 'fc2'],\n",
    "    }]\n",
    "\n",
    "    set_quant_scheme_dtype('weight', 'per_tensor_symmetric', 'int')\n",
    "    set_quant_scheme_dtype('output', 'per_tensor_symmetric', 'int')\n",
    "    set_quant_scheme_dtype('input', 'per_tensor_symmetric', 'int')\n",
    "\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    user_image = PIL.Image.open('data/user_input.jpg')\n",
    "    user_image = user_image.resize((28,28), PIL.Image.LANCZOS).convert('L')\n",
    "    tensor_image = transform(user_image)\n",
    "\n",
    "    #tensor_image = transform(PIL.Image.open('data/eight.jpg'))\n",
    "\n",
    "    idim = tensor_image.size()[1]\n",
    "    ifmap = tensor_image.size()[0]\n",
    "    fc2_nodes = 10\n",
    "\n",
    "    # Create a NaiveModel object and apply QAT_Quantizer setup\n",
    "    model_path = \"models/mnist_model_4bit_AVGPOOL.pth\"\n",
    "    qmodel = NaiveModel().to(device)\n",
    "    optimizer = torch.optim.SGD(qmodel.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "    dummy_input = torch.randn(1, ifmap, idim, idim).to(device)\n",
    "    # To enable batch normalization folding in the training process, you should\n",
    "    # pass dummy_input to the QAT_Quantizer.\n",
    "    quantizer = QAT_Quantizer(qmodel, configure_list, optimizer, dummy_input=dummy_input)\n",
    "    quantizer.compress()\n",
    "\n",
    "\n",
    "    # Load trained model (from MNIST_CNN_Training step).\n",
    "    state = torch.load(model_path, map_location=device)\n",
    "    qmodel.load_state_dict(state, strict=True)\n",
    "    qmodel.eval()\n",
    "    return tensor_image, device, qmodel, num_bits, ifmap, idim, fc2_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number written is 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "capture_input()\n",
    "tensor_image, device, model, num_bits, ifmap, idim, fc2_nodes = setup_CNN()\n",
    "plt.imshow(tensor_image.permute(1, 2, 0))\n",
    "\n",
    "with torch.no_grad():\n",
    "        data = tensor_image.unsqueeze(0)\n",
    "        data = data.to(device)\n",
    "        batch = 1\n",
    "\n",
    "        # Custom forward pass function\n",
    "        non_linear_model = forward_custom(model, data, batch, num_bits, ifmap, ofmap, idim, knl, stride, conv2kernels,\n",
    "                                            maxpool_dim, fc1_nodes, fc2_nodes, non_ideal_file, ncol, nrow, device)\n",
    "        output = non_linear_model.forward_pass()\n",
    "        pred = output.argmax(dim=1, keepdim=True)[0][0]\n",
    "        print(f\"Number written is {pred}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
