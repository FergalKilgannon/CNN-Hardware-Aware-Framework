{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HARDWARE FP MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ferga\\anaconda3\\Lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from naive_mnist import NaiveModel\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "from nni.algorithms.compression.pytorch.quantization import QAT_Quantizer\n",
    "from nni.compression.pytorch.quantization.settings import set_quant_scheme_dtype\n",
    "import PIL\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from tkinter import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FP CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class forward_custom:\n",
    "  def __init__(self, model, data, batch, num_bits, ifmap,\n",
    "               ofmap, idim, knl, stride, conv2kernels, maxpool_dim, \n",
    "               fc1_nodes, fc2_nodes, non_ideal_file, ncol, nrow, device):\n",
    "    self.model = model\n",
    "    self.data = data\n",
    "    self.batch = batch\n",
    "    self.num_bits = num_bits\n",
    "    self.ifmap = ifmap                # Image (grayscale MNIST)\n",
    "    self.ofmap = ofmap                # Output maps (from applying ofmap knl x knl weight kernels)\n",
    "    self.idim = idim                  # Input image dimensions\n",
    "    self.knl = knl                    # Kernel dimensions\n",
    "    self.stride = stride              # Convolutional stride\n",
    "    self.conv2kernels = conv2kernels  # Output channels to the second conv layer\n",
    "    self.maxpool_dim = maxpool_dim    # Maxpool area\n",
    "    self.fc1_nodes = fc1_nodes        # Fully connected layer 2 output nodes\n",
    "    self.fc2_nodes = fc2_nodes        # Fully connected layer 2 output nodes\n",
    "    self.ncol = ncol                  # Hardware matrix-vector-multiplication (MVM) columns (corresponds to number of output ADCs)\n",
    "    self.nrow = nrow                  # Hardware MVM rows (corresponds to number of input DACs)\n",
    "                                      # In-memory computing hardware therefore has ncol*nrow weights.\n",
    "\n",
    "    self.nl_mult = self.read_non_idealities(non_ideal_file)\n",
    "    self.device = device\n",
    "\n",
    "\n",
    "  def read_non_idealities(self, file_name):\n",
    "    return pd.read_excel(file_name, index_col=0).values\n",
    "\n",
    "  def forward_pass(self):\n",
    "    # Scale input\n",
    "    s_in, x = self.scale_quant(self.data, self.num_bits)\n",
    "    \n",
    "    # Convolutional layer 1\n",
    "    x = self.conv2d(x, self.model.conv1.module, s_in, self.idim, self.ifmap, self.ofmap)\n",
    "\n",
    "    # ReLU 1\n",
    "    x = self.relu6(x, 0, 6, self.model.relu1.module)\n",
    "\n",
    "    # Maxpool layer 1\n",
    "    x = self.maxpool2d(x, self.idim - self.knl + 1, self.ofmap, self.maxpool_dim)\n",
    "    \n",
    "    # Scale for convolutional layer 2\n",
    "    sin_conv2 = self.model.conv2.module.input_scale\n",
    "    x = self.noscale_quant(x, sin_conv2.cpu(), 0, self.num_bits)\n",
    "    x = self.conv2d(x, self.model.conv2.module, sin_conv2, (self.idim-self.knl+1)/2, self.ofmap, self.conv2kernels)\n",
    "\n",
    "    # ReLU 2\n",
    "    x = self.relu6(x, 0, 6, self.model.relu2.module)\n",
    "\n",
    "    # Maxpool layer 2\n",
    "    x = self.maxpool2d(x, ((self.idim - self.knl+1)/2) - self.knl + 1, self.conv2kernels, self.maxpool_dim)\n",
    "\n",
    "    # Fully connected layer 1\n",
    "    x = x.view(-1, x.size()[1:].numel())  # Flatten outputs of out_maxpool2 layer to feed to FC layers.\n",
    "    x = self.fc(x, self.model.fc1.module, self.fc1_nodes)\n",
    "\n",
    "    # ReLU 3\n",
    "    x = self.relu6(x, 0, 6, self.model.relu3.module)\n",
    "\n",
    "    # Fully connected layer 2\n",
    "    x = self.fc(x.cpu(), self.model.fc2.module, self.fc2_nodes)\n",
    "\n",
    "    # Softmax layer\n",
    "    return F.log_softmax(x, dim=1)\n",
    "\n",
    "  def conv2d(self, x, conv, s_in, idim, ifmap, ofmap):\n",
    "    # Scale input image, conv weight and bias to bits used\n",
    "    sw_conv, filters_conv = self.scale_quant(conv.weight.cpu(), self.num_bits)\n",
    "    bias_conv = conv.bias / (s_in * sw_conv)\n",
    "\n",
    "    # CONV layer (WSAB dataflow - see convolve2D_wsab function)\n",
    "    out_conv = self.convolve2D_wsab(x, filters_conv, bias_conv, 0, self.stride, self.batch, ifmap, ofmap, idim, self.knl, self.ncol, self.nrow)\n",
    "    out_conv = torch.from_numpy(out_conv)\n",
    "    out_conv = out_conv.to(self.device)\n",
    "\n",
    "    # Applying scaling normalization s_in*sw_conv (convert back to float)\n",
    "    return out_conv * s_in * sw_conv\n",
    "\n",
    "\n",
    "  def convolve2D_wsab(self, image, kernel, bias, padding, strides, batch, ifmap, ofmap, idim, knl, ncol, nrow):\n",
    "    xKernShape = knl\n",
    "    yKernShape = knl\n",
    "    xImgShape = idim\n",
    "    yImgShape = idim\n",
    "\n",
    "    # Shape of Output Convolution\n",
    "    xOutput = int(((xImgShape - xKernShape + 2 * padding) / strides) + 1)\n",
    "    yOutput = int(((yImgShape - yKernShape + 2 * padding) / strides) + 1)\n",
    "    output = np.zeros((batch, ofmap, xOutput, yOutput))\n",
    "\n",
    "    # WEIGHT STATIONARY\n",
    "\n",
    "    # Number of weight block partitions to fit into hardware\n",
    "    block_col = int(np.ceil(ofmap/ncol))\n",
    "    block_row = int(np.ceil(knl*knl*ifmap/nrow))\n",
    "    kernel_flat = np.zeros((block_col*ncol, block_row*nrow), dtype='int64')\n",
    "\n",
    "    kernel_flat[0:ofmap,0:ifmap*knl*knl] = torch.reshape(kernel, [ofmap, ifmap*knl*knl]).cpu().numpy()\n",
    "    # data_moved_count = 0\n",
    "    # mac_completed_count = 0\n",
    "    # batch = 1\n",
    "    # Process image by image\n",
    "    for b in range(batch):\n",
    "      # Apply Equal Padding to All Sides\n",
    "      if padding != 0:\n",
    "          imagePadded = np.zeros((idim + padding * 2, idim + padding * 2))\n",
    "          imagePadded[int(padding):int(-1 * padding), int(padding):int(-1 * padding)] = image[b]\n",
    "      else:\n",
    "          imagePadded = image[b]\n",
    "\n",
    "      # Iterate through image\n",
    "\n",
    "      image_block = np.zeros((block_row * nrow, xOutput, yOutput), dtype='int64')\n",
    "      otemp = torch.zeros(block_row, block_col*ncol, xOutput, yOutput)\n",
    "      for bc in range(block_col):\n",
    "          for br in range(block_row):\n",
    "              # data_moved_count += nrow*ncol\n",
    "              ktemp = torch.zeros(ncol,nrow)\n",
    "              ktemp = kernel_flat[ncol*bc:(bc+1)*ncol, nrow*br:(br+1)*nrow]\n",
    "              ktemp = torch.from_numpy(ktemp)\n",
    "\n",
    "              for y in range(yOutput):\n",
    "                  for x in range(xOutput):\n",
    "                      # Fetch image section x,y, bc,br\n",
    "                      image_block[0:knl * knl * ifmap, x, y] = imagePadded[0:ifmap,\n",
    "                                                                strides * x: strides * x + xKernShape,\n",
    "                                                                strides * y: strides * y + yKernShape].reshape(\n",
    "                          knl * knl * ifmap).cpu().numpy()\n",
    "                      # data_moved_count += xKernShape*yKernShape\n",
    "                      # mac_completed_count += ncol\n",
    "                      # data_moved_count += nrow + 1\n",
    "\n",
    "                      itemp = image_block[br*nrow:(br+1)*nrow, x, y].reshape(1, nrow)\n",
    "                      itemp = torch.from_numpy(itemp)\n",
    "                      # Replace this line with Arduino SPI function call\n",
    "                      #otemp[br, bc*ncol:(bc+1)*ncol, x, y] = torch.sum(ktemp*itemp, dim=1)\n",
    "                      otemp[br, bc*ncol:(bc+1)*ncol, x, y] = self.non_linear_mult(ktemp, itemp[0])\n",
    "                      # print(\"Batch:%d,BROW:%d,BCOL:%d,X:%d,Y:%d\" % (b,br,bc,x,y))\n",
    "\n",
    "      output[b] = torch.sum(otemp[:, 0:ofmap, :, :], dim=0) + (bias.reshape(ofmap, 1, 1).cpu()*torch.ones(xOutput, yOutput)).detach().numpy()\n",
    "\n",
    "    # print(f\"Moved {data_moved_count} pieces of data, {mac_completed_count} MAC operations completed\")\n",
    "    # print(ktemp[1000])  # Workaround breakpoint\n",
    "\n",
    "    # # OUTPUT STATIONARY\n",
    "    # # Number of output block partitions to fit into hardware\n",
    "    # block_col = int(np.ceil(xOutput/ncol))\n",
    "    # block_row = int(np.ceil(knl*knl*ifmap/nrow))\n",
    "\n",
    "    # kernel_flat = np.zeros((block_col*ncol, block_row*nrow))\n",
    "    # kernel_flat[0:ofmap,0:ifmap*knl*knl] = torch.reshape(kernel, [ofmap, ifmap*knl*knl]).cpu().numpy()\n",
    "\n",
    "    # data_moved_count = 0\n",
    "    # mac_completed_count = 0\n",
    "    # batch = 1\n",
    "\n",
    "    # # Process image by image\n",
    "    # for b in range(batch):\n",
    "    #   # Apply Equal Padding to All Sides\n",
    "    #   if padding != 0:\n",
    "    #       imagePadded = np.zeros((idim + padding * 2, idim + padding * 2))\n",
    "    #       imagePadded[int(padding):int(-1 * padding), int(padding):int(-1 * padding)] = image[b]\n",
    "    #   else:\n",
    "    #       imagePadded = image[b]\n",
    "\n",
    "    #   image_block = np.zeros((nrow * block_row, ncol*block_col, yOutput))\n",
    "    #   otemp = torch.zeros((nrow * block_row, ofmap, ncol*block_col, yOutput))\n",
    "\n",
    "    #   for y in range(yOutput):\n",
    "    #      for x in range(xOutput):\n",
    "    #         image_block[0:knl * knl * ifmap, x, y] = imagePadded[0:ifmap,\n",
    "    #                                                       strides * x: strides * x + xKernShape,\n",
    "    #                                                       strides * y: strides * y + yKernShape].reshape(\n",
    "    #                 knl * knl * ifmap).cpu().numpy()\n",
    "\n",
    "    #   # Iterate through image\n",
    "    #   for y in range(yOutput):\n",
    "    #       for bc in range(block_col):\n",
    "    #         for br in range(block_row):\n",
    "    #           data_moved_count += nrow*ncol\n",
    "    #           for kerCol in range(ofmap):\n",
    "    #             data_moved_count += nrow\n",
    "    #             ktemp = kernel_flat[kerCol, br*nrow:(br+1)*nrow]\n",
    "    #             ktemp = torch.from_numpy(ktemp)\n",
    "    #             for x in range(ncol):\n",
    "    #                 mac_completed_count += 1\n",
    "    #                 data_moved_count += 1\n",
    "    #                 # Fetch image section x,y, bc,br\n",
    "    #                 itemp = image_block[br*nrow:(br+1)*nrow, (bc*ncol)+x, y].reshape(1, nrow)\n",
    "    #                 itemp = torch.from_numpy(itemp)\n",
    "    #                 # Replace this line with Arduino SPI function call\n",
    "    #                 otemp[br, kerCol, (bc*ncol)+x, y] = torch.sum(ktemp*itemp, dim=1)\n",
    "    #                 # otemp[kerCol, x, y] = self.non_linear_mult(ktemp, itemp[0])\n",
    "    #                 # print(\"Batch:%d,BROW:%d,BCOL:%d,X:%d,Y:%d\" % (b,br,bc,x,y))\n",
    "    #   output[b] = torch.sum(otemp[0:block_row, 0:ofmap, 0:xOutput, 0:yOutput], dim=0) + (bias.reshape(ofmap, 1, 1).cpu()*torch.ones(xOutput, yOutput)).detach().numpy()\n",
    "    #   print(f\"Moved {data_moved_count} pieces of data, {mac_completed_count} MAC operations completed\")\n",
    "    #   print(ktemp[1000]) # Workaround breakpoint\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "  def relu6(self, x, min_val, max_val, relu_module):\n",
    "    i = (x >= min_val) * x\n",
    "    out_relu = (i <= max_val) * (i - max_val) + max_val\n",
    "    so_relu = relu_module.output_scale\n",
    "\n",
    "    # Apply fake quantization to relu1 output.\n",
    "    out_relu = self.noscale_quant(out_relu, so_relu, 0, self.num_bits)\n",
    "    return self.dequantize(out_relu, so_relu, 0)\n",
    "\n",
    "\n",
    "  def maxpool2d(self, x, idim, ofmap, knl):\n",
    "    # Maxpool layer - downsample by knl x knl with maxpool (no quantization required for max function)\n",
    "    return torch.from_numpy(self.maxpool2D_wsa(self.batch, x, idim, ofmap, knl))\n",
    "\n",
    "\n",
    "  # Maxpool layer (apply to all ofmaps simultaneously - faster!)\n",
    "  def maxpool2D_wsa(self, batch, image, idim, ofmap, knl):\n",
    "    xKernShape = knl\n",
    "    yKernShape = knl\n",
    "    xImgShape = idim\n",
    "    yImgShape = idim\n",
    "    strides = knl\n",
    "    padding = 0\n",
    "    # Shape of Output Convolution\n",
    "    xOutput = int(((xImgShape - xKernShape + 2 * padding) / strides) + 1)\n",
    "    yOutput = int(((yImgShape - yKernShape + 2 * padding) / strides) + 1)\n",
    "    output = np.zeros((batch, ofmap, xOutput, yOutput))\n",
    "\n",
    "    for b in range(batch):\n",
    "        # Apply Equal Padding to All Sides\n",
    "        if padding != 0:\n",
    "            imagePadded = np.zeros((idim + padding * 2, idim + padding * 2))\n",
    "            imagePadded[int(padding):int(-1 * padding), int(padding):int(-1 * padding)] = image[b]\n",
    "        else:\n",
    "            imagePadded = image[b]\n",
    "\n",
    "        # Iterate through image\n",
    "        for y in range(yOutput):\n",
    "            for x in range(xOutput):\n",
    "                output[b, :, x, y] = torch.amax(\n",
    "                        imagePadded[:, strides * x: strides * x + xKernShape, strides * y: strides * y + yKernShape], dim=(1,2)).detach().cpu().numpy()\n",
    "    return output\n",
    "\n",
    "\n",
    "  def fc(self, x, module, odim):\n",
    "    sin_fc = module.input_scale.cpu()\n",
    "    in_fcs = self.noscale_quant(x, sin_fc, 0, self.num_bits)\n",
    "    sw_fc, filters_fc = self.scale_quant(module.weight.cpu(), self.num_bits)\n",
    "    bias_fc = module.bias.cpu() / (sw_fc * sin_fc)\n",
    "\n",
    "\n",
    "    # FC layer using (WSA dataflow)\n",
    "    # N.B. Need to implement WSAB function to implement 'hardware acceleration' of FC layer.\n",
    "    out_fc = torch.from_numpy(self.fc_custom_wsa(in_fcs, filters_fc, bias_fc, self.batch, odim))\n",
    "\n",
    "    # FC output scaling\n",
    "    out_fcs = out_fc * sin_fc * sw_fc\n",
    "\n",
    "    so_fc = module.output_scale.cpu()\n",
    "    # FC output fake quantization\n",
    "    return self.dequantize(self.noscale_quant(out_fcs, so_fc, 0, self.num_bits), so_fc, 0).to(self.device)\n",
    "\n",
    "\n",
    "  # 'Fake' Quantization function [Jacob et. al]\n",
    "  def quantize(self, real_value, scale, zero_point, qmin, qmax):\n",
    "    transformed_val = zero_point + real_value / scale\n",
    "    clamped_val = torch.clamp(transformed_val, qmin, qmax)\n",
    "    quantized_val = torch.round(clamped_val)\n",
    "    return quantized_val\n",
    "\n",
    "\n",
    "  # 'Fake' Dequantization function [Jacob et. al]\n",
    "  def dequantize(self, quantized_val, scale, zero_point):\n",
    "    real_val = scale * (quantized_val - zero_point)\n",
    "    return real_val\n",
    "\n",
    "\n",
    "  # Scaling function (Jacob et. al)\n",
    "  def scale_quant(self, real_value, num_bits):\n",
    "    qmin = -(2 ** (num_bits - 1) - 1)\n",
    "    qmax = 2 ** (num_bits - 1) - 1\n",
    "    abs_max = torch.abs(real_value).max()\n",
    "    scale = abs_max / (float(qmax - qmin) / 2)\n",
    "    zero_point = 0\n",
    "    quant = self.quantize(real_value, scale, zero_point, qmin, qmax)\n",
    "    return scale, quant\n",
    "\n",
    "\n",
    "  # Scaling function (Jacob et. al)\n",
    "  def noscale_quant(self, real_value, scale, zero_point, num_bits):\n",
    "    qmin = -(2 ** (num_bits - 1) - 1)\n",
    "    qmax = 2 ** (num_bits - 1) - 1\n",
    "    quant = self.quantize(real_value, scale, zero_point, qmin, qmax)\n",
    "    return quant\n",
    "\n",
    "\n",
    "  # Custom FC layer (vector multiplication style - much faster than for loop implementation)\n",
    "  def fc_custom_wsa(self, fc_input, filters, bias, batch, ofmap):\n",
    "    print(fc_input.size()[1])\n",
    "    output = np.zeros((batch, ofmap))\n",
    "    for b in range(batch):\n",
    "        outputi = np.zeros(ofmap)\n",
    "        input_batch = fc_input[b]\n",
    "        print((input_batch*filters).size())\n",
    "        outputi = torch.sum((input_batch * filters),dim=1) + bias\n",
    "        output[b, :] = outputi.detach().numpy()\n",
    "    return output\n",
    "  \n",
    "  \n",
    "  # def fc_custom_wsab(self, fc_input, filters, bias, batch, ofmap, ncol, nrow):\n",
    "  #   output = np.zeros((batch, ofmap))\n",
    "  #   for b in range(batch):\n",
    "  #       outputi = np.zeros(ofmap)\n",
    "  #       input_batch = fc_input[b]\n",
    "  #       outputi = torch.sum((input_batch * filters),dim=1) + bias\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  #       self, image, kernel, bias, padding, strides, batch, ifmap, ofmap, idim, knl, ncol, nrow):\n",
    "\n",
    "\n",
    "  #       # WEIGHT STATIONARY\n",
    "\n",
    "  #       # Iterate through image\n",
    "  #       block_col = int(np.ceil(ofmap/ncol))\n",
    "  #       block_row = int(np.ceil(fc_input.size()[1]/nrow))\n",
    "  #       filters_flat = np.zeros((block_col*ncol, block_row*nrow), dtype='int64')\n",
    "  #       image_col = np.zeros((block_row*nrow), dtype='int64')\n",
    "\n",
    "  #       filters_flat[0:ofmap,0:fc_input.size()[1]] = torch.reshape(filters, [ofmap, fc_input.size()[1]]).cpu().numpy()\n",
    "  #       image_col[0:fc_input.size()[1]] = fc_input\n",
    "\n",
    "  #       otemp = torch.zeros(block_row, block_col*ncol, xOutput, yOutput)\n",
    "  #       for bc in range(block_col):\n",
    "  #           for br in range(block_row):\n",
    "  #               # data_moved_count += nrow*ncol\n",
    "  #               ktemp = torch.zeros(ncol,nrow)\n",
    "  #               ktemp = kernel_flat[ncol*bc:(bc+1)*ncol, nrow*br:(br+1)*nrow]\n",
    "  #               ktemp = torch.from_numpy(ktemp)\n",
    "\n",
    "  #               for y in range(yOutput):\n",
    "  #                   for x in range(xOutput):\n",
    "  #                       # Fetch image section x,y, bc,br\n",
    "  #                       image_block[0:knl * knl * ifmap, x, y] = imagePadded[0:ifmap,\n",
    "  #                                                                 strides * x: strides * x + xKernShape,\n",
    "  #                                                                 strides * y: strides * y + yKernShape].reshape(\n",
    "  #                           knl * knl * ifmap).cpu().numpy()\n",
    "  #                       # data_moved_count += xKernShape*yKernShape\n",
    "  #                       # mac_completed_count += ncol\n",
    "  #                       # data_moved_count += nrow + 1\n",
    "\n",
    "  #                       itemp = image_block[br*nrow:(br+1)*nrow, x, y].reshape(1, nrow)\n",
    "  #                       itemp = torch.from_numpy(itemp)\n",
    "  #                       # Replace this line with Arduino SPI function call\n",
    "  #                       #otemp[br, bc*ncol:(bc+1)*ncol, x, y] = torch.sum(ktemp*itemp, dim=1)\n",
    "  #                       otemp[br, bc*ncol:(bc+1)*ncol, x, y] = self.non_linear_mult(ktemp, itemp[0])\n",
    "  #                       # print(\"Batch:%d,BROW:%d,BCOL:%d,X:%d,Y:%d\" % (b,br,bc,x,y))\n",
    "\n",
    "  #       output[b] = torch.sum(otemp[:, 0:ofmap, :, :], dim=0) + (bias.reshape(ofmap, 1, 1).cpu()*torch.ones(xOutput, yOutput)).detach().numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  #       output[b, :] = outputi.detach().numpy()\n",
    "  #   return output\n",
    "\n",
    "\n",
    "  # Non-linear look-up\n",
    "  def non_linear_conv(self, kernel, image):\n",
    "    sum = np.int64(0)\n",
    "    for i in range(len(kernel)):\n",
    "      #sum = sum + self.nl_mult[int(kernel[i]+(2**(self.num_bits-1))-1)][int(image[i]+(2**(self.num_bits-1))-1)]\n",
    "      sum = sum + (kernel[i]*image[i] + 20*kernel[i]*np.sin(np.pi*kernel[i]/5)*np.sin(np.pi*image[i]/5))\n",
    "    return sum\n",
    "\n",
    "  def non_linear_mult(self, kernel, image):\n",
    "    output = np.empty(len(kernel))\n",
    "    if len(kernel.shape) == 2:\n",
    "      for i in range(len(kernel)):\n",
    "        output[i] = self.non_linear_conv(kernel[i, :].tolist(), image.tolist())\n",
    "      return torch.from_numpy(output)\n",
    "    else:\n",
    "      output = self.non_linear_conv(kernel.tolist(), image.tolist())\n",
    "      return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel dimensions (used in conv layers)\n",
    "knl = 5\n",
    "\n",
    "# Kernel stride\n",
    "stride = 1\n",
    "\n",
    "# Output Feature Maps (same as kernels in conv layer 1)\n",
    "ofmap = 16\n",
    "\n",
    "# Kernels in conv layer 2\n",
    "conv2kernels = 32\n",
    "\n",
    "# Maxpool dimensions (assuming square area)\n",
    "maxpool_dim = 2\n",
    "\n",
    "# Fully Connected 1 output nodes\n",
    "fc1_nodes = 50\n",
    "\n",
    "# Hardware MVM columns\n",
    "ncol = 16\n",
    "\n",
    "# Hardware MVM rows\n",
    "nrow = 32\n",
    "\n",
    "# Non-Idealities File\n",
    "non_ideal_file = '../HardwareSpec/8x8_Mac_result_final.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_input():\n",
    "    width = 300  # canvas width\n",
    "    height = 300 # canvas height\n",
    "    black = (0, 0, 0) # canvas back\n",
    "\n",
    "    def save_and_exit():\n",
    "        # save image to hard drive\n",
    "        filename = \"data/user_input.jpg\"\n",
    "        output_image.save(filename)\n",
    "        master.destroy()\n",
    "\n",
    "    def paint(event):\n",
    "        x1, y1 = (event.x - 1), (event.y - 1)\n",
    "        x2, y2 = (event.x + 1), (event.y + 1)\n",
    "        canvas.create_oval(x1, y1, x2, y2, fill=\"white\",width=40)\n",
    "        draw.line([x1, y1, x2, y2],fill=\"white\",width=40)\n",
    "\n",
    "    master = Tk()\n",
    "\n",
    "    # create a tkinter canvas to draw on\n",
    "    canvas = Canvas(master, width=width, height=height, bg='black')\n",
    "    canvas.pack()\n",
    "\n",
    "    # create an empty PIL image and draw object to draw on\n",
    "    output_image = PIL.Image.new(\"RGB\", (width, height), black)\n",
    "    draw = PIL.ImageDraw.Draw(output_image)\n",
    "    canvas.pack(expand=YES, fill=BOTH)\n",
    "    canvas.bind(\"<B1-Motion>\", paint)\n",
    "\n",
    "    # add a button to save the image\n",
    "    button=Button(text=\"save\",command=save_and_exit)\n",
    "    button.pack()\n",
    "\n",
    "    master.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_CNN():\n",
    "    torch.set_default_dtype(torch.float32)\n",
    "    torch.manual_seed(0)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # INT9 weight, INT9 activations\n",
    "    num_bits = 9\n",
    "    # Make sure this matches quantization config from MNIST_CNN_Training\n",
    "    configure_list = [{\n",
    "        'quant_types': ['weight', 'input'],\n",
    "        'quant_bits': {'weight': num_bits, 'input': num_bits},\n",
    "        'quant_start_step': 2,\n",
    "        'op_names': ['conv1', 'conv2']\n",
    "    }, {\n",
    "        'quant_types': ['output'],\n",
    "        'quant_bits': {'output': num_bits},\n",
    "        'quant_start_step': 2,\n",
    "        'op_names': ['relu1', 'relu2', 'relu3']\n",
    "    }, {\n",
    "        'quant_types': ['output', 'weight', 'input'],\n",
    "        'quant_bits': {'output': num_bits, 'weight': num_bits, 'input': num_bits},\n",
    "        'quant_start_step': 2,\n",
    "        'op_names': ['fc1', 'fc2'],\n",
    "    }]\n",
    "\n",
    "    set_quant_scheme_dtype('weight', 'per_tensor_symmetric', 'int')\n",
    "    set_quant_scheme_dtype('output', 'per_tensor_symmetric', 'int')\n",
    "    set_quant_scheme_dtype('input', 'per_tensor_symmetric', 'int')\n",
    "\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    user_image = PIL.Image.open('data/user_input.jpg')\n",
    "    user_image = user_image.resize((28,28), PIL.Image.LANCZOS).convert('L')\n",
    "    tensor_image = transform(user_image)\n",
    "\n",
    "    #tensor_image = transform(PIL.Image.open('data/eight.jpg'))\n",
    "\n",
    "    idim = tensor_image.size()[1]\n",
    "    ifmap = tensor_image.size()[0]\n",
    "    fc2_nodes = 10\n",
    "\n",
    "    # Create a NaiveModel object and apply QAT_Quantizer setup\n",
    "    model_path = \"models/mnist_model_9bit.pth\"\n",
    "    qmodel = NaiveModel().to(device)\n",
    "    optimizer = torch.optim.SGD(qmodel.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "    dummy_input = torch.randn(1, ifmap, idim, idim).to(device)\n",
    "    # To enable batch normalization folding in the training process, you should\n",
    "    # pass dummy_input to the QAT_Quantizer.\n",
    "    quantizer = QAT_Quantizer(qmodel, configure_list, optimizer, dummy_input=dummy_input)\n",
    "    quantizer.compress()\n",
    "\n",
    "\n",
    "    # Load trained model (from MNIST_CNN_Training step).\n",
    "    state = torch.load(model_path, map_location=device)\n",
    "    qmodel.load_state_dict(state, strict=True)\n",
    "    qmodel.eval()\n",
    "    return tensor_image, device, qmodel, num_bits, ifmap, idim, fc2_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n",
      "torch.Size([50, 512])\n",
      "50\n",
      "torch.Size([10, 50])\n",
      "Number written is 2\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "capture_input()\n",
    "tensor_image, device, model, num_bits, ifmap, idim, fc2_nodes = setup_CNN()\n",
    "plt.imshow(tensor_image.permute(1, 2, 0))\n",
    "\n",
    "with torch.no_grad():\n",
    "        data = tensor_image.unsqueeze(0)\n",
    "        data = data.to(device)\n",
    "        batch = 1\n",
    "\n",
    "        # Custom forward pass function\n",
    "        non_linear_model = forward_custom(model, data, batch, num_bits, ifmap, ofmap, idim, knl, stride, conv2kernels,\n",
    "                                            maxpool_dim, fc1_nodes, fc2_nodes, non_ideal_file, ncol, nrow, device)\n",
    "        output = non_linear_model.forward_pass()\n",
    "        pred = output.argmax(dim=1, keepdim=True)[0][0]\n",
    "        print(f\"Number written is {pred}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
