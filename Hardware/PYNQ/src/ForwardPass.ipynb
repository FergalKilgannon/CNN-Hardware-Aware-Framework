{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HARDWARE FP MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from naive_mnist import NaiveModel\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "from nni.algorithms.compression.pytorch.quantization import QAT_Quantizer\n",
    "from nni.compression.pytorch.quantization.settings import set_quant_scheme_dtype\n",
    "import PIL\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from tkinter import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FP CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class forward_custom:\n",
    "  def __init__(self, model, data, batch, num_bits, ifmap,\n",
    "               ofmap, idim, knl, stride, conv2kernels, maxpool_dim, \n",
    "               fc1_nodes, fc2_nodes, non_ideal_file, ncol, nrow, device):\n",
    "    self.model = model\n",
    "    self.data = data\n",
    "    self.batch = batch\n",
    "    self.num_bits = num_bits\n",
    "    self.ifmap = ifmap                # Image (grayscale MNIST)\n",
    "    self.ofmap = ofmap                # Output maps (from applying ofmap knl x knl weight kernels)\n",
    "    self.idim = idim                  # Input image dimensions\n",
    "    self.knl = knl                    # Kernel dimensions\n",
    "    self.stride = stride              # Convolutional stride\n",
    "    self.conv2kernels = conv2kernels  # Output channels to the second conv layer\n",
    "    self.maxpool_dim = maxpool_dim    # Maxpool area\n",
    "    self.fc1_nodes = fc1_nodes        # Fully connected layer 2 output nodes\n",
    "    self.fc2_nodes = fc2_nodes        # Fully connected layer 2 output nodes\n",
    "    self.ncol = ncol                  # Hardware matrix-vector-multiplication (MVM) columns (corresponds to number of output ADCs)\n",
    "    self.nrow = nrow                  # Hardware MVM rows (corresponds to number of input DACs)\n",
    "                                      # In-memory computing hardware therefore has ncol*nrow weights.\n",
    "\n",
    "    self.nl_mult = self.read_non_idealities(non_ideal_file)\n",
    "    self.device = device\n",
    "\n",
    "\n",
    "  def read_non_idealities(self, file_name):\n",
    "    return pd.read_excel(file_name, index_col=0).values\n",
    "\n",
    "  def forward_pass(self):\n",
    "    # Scale input\n",
    "    s_in, x = self.scale_quant(self.data, self.num_bits)\n",
    "    \n",
    "    # Convolutional layer 1\n",
    "    x = self.conv2d(x, self.model.conv1.module, s_in, self.idim, self.ifmap, self.ofmap)\n",
    "\n",
    "    # ReLU 1\n",
    "    x = self.relu6(x, 0, 6, self.model.relu1.module)\n",
    "\n",
    "    # Maxpool layer 1\n",
    "    x = self.maxpool2d(x, self.idim - self.knl + 1, self.ofmap, self.maxpool_dim)\n",
    "    \n",
    "    # Scale for convolutional layer 2\n",
    "    sin_conv2 = self.model.conv2.module.input_scale\n",
    "    x = self.noscale_quant(x, sin_conv2.cpu(), 0, self.num_bits)\n",
    "    x = self.conv2d(x, self.model.conv2.module, sin_conv2, (self.idim-self.knl+1)/2, self.ofmap, self.conv2kernels)\n",
    "\n",
    "    # ReLU 2\n",
    "    x = self.relu6(x, 0, 6, self.model.relu2.module)\n",
    "\n",
    "    # Maxpool layer 2\n",
    "    x = self.maxpool2d(x, ((self.idim - self.knl+1)/2) - self.knl + 1, self.conv2kernels, self.maxpool_dim)\n",
    "\n",
    "    # Fully connected layer 1\n",
    "    x = x.view(-1, x.size()[1:].numel())  # Flatten outputs of out_maxpool2 layer to feed to FC layers.\n",
    "    x = self.fc(x, self.model.fc1.module, self.fc1_nodes)\n",
    "\n",
    "    # ReLU 3\n",
    "    x = self.relu6(x, 0, 6, self.model.relu3.module)\n",
    "\n",
    "    # Fully connected layer 2\n",
    "    x = self.fc(x.cpu(), self.model.fc2.module, self.fc2_nodes)\n",
    "\n",
    "    # Softmax layer\n",
    "    return F.log_softmax(x, dim=1)\n",
    "\n",
    "  def conv2d(self, x, conv, s_in, idim, ifmap, ofmap):\n",
    "    # Scale input image, conv weight and bias to bits used\n",
    "    sw_conv, filters_conv = self.scale_quant(conv.weight.cpu(), self.num_bits)\n",
    "    bias_conv = conv.bias / (s_in * sw_conv)\n",
    "\n",
    "    # CONV layer (WSAB dataflow - see convolve2D_wsab function)\n",
    "    out_conv = self.convolve2D_wsab(x, filters_conv, bias_conv, 0, self.stride, self.batch, ifmap, ofmap, idim, self.knl, self.ncol, self.nrow)\n",
    "    out_conv = torch.from_numpy(out_conv)\n",
    "    out_conv = out_conv.to(self.device)\n",
    "\n",
    "    # Applying scaling normalization s_in*sw_conv (convert back to float)\n",
    "    return out_conv * s_in * sw_conv\n",
    "\n",
    "\n",
    "  def convolve2D_wsab(self, image, kernel, bias, padding, strides, batch, ifmap, ofmap, idim, knl, ncol, nrow):\n",
    "    xKernShape = knl\n",
    "    yKernShape = knl\n",
    "    xImgShape = idim\n",
    "    yImgShape = idim\n",
    "\n",
    "    # Shape of Output Convolution\n",
    "    xOutput = int(((xImgShape - xKernShape + 2 * padding) / strides) + 1)\n",
    "    yOutput = int(((yImgShape - yKernShape + 2 * padding) / strides) + 1)\n",
    "    output = np.zeros((batch, ofmap, xOutput, yOutput))\n",
    "\n",
    "    # # WEIGHT STATIONARY\n",
    "\n",
    "    # # Number of weight block partitions to fit into hardware\n",
    "    # block_col = int(np.ceil(ofmap/ncol))\n",
    "    # block_row = int(np.ceil(knl*knl*ifmap/nrow))\n",
    "    # kernel_flat = np.zeros((block_col*ncol, block_row*nrow), dtype='int64')\n",
    "\n",
    "    # kernel_flat[0:ofmap,0:ifmap*knl*knl] = torch.reshape(kernel, [ofmap, ifmap*knl*knl]).cpu().numpy()\n",
    "    # data_moved_count = 0\n",
    "    # mac_completed_count = 0\n",
    "    # digital = 0\n",
    "\n",
    "    # # Process image by image\n",
    "    # for b in range(batch):\n",
    "    #   # Apply Equal Padding to All Sides\n",
    "    #   if padding != 0:\n",
    "    #       imagePadded = np.zeros((idim + padding * 2, idim + padding * 2))\n",
    "    #       imagePadded[int(padding):int(-1 * padding), int(padding):int(-1 * padding)] = image[b]\n",
    "    #   else:\n",
    "    #       imagePadded = image[b]\n",
    "\n",
    "    #   # Iterate through image\n",
    "\n",
    "    #   image_block = np.zeros((block_row * nrow, xOutput, yOutput), dtype='int64')\n",
    "    #   otemp = torch.zeros(block_row, block_col*ncol, xOutput, yOutput)\n",
    "    #   for bc in range(block_col):\n",
    "    #       for br in range(block_row):\n",
    "    #           data_moved_count += nrow*ncol*2\n",
    "    #           ktemp = kernel_flat[ncol*bc:(bc+1)*ncol, nrow*br:(br+1)*nrow]\n",
    "    #           ktemp = torch.from_numpy(ktemp)\n",
    "\n",
    "    #           for y in range(yOutput):\n",
    "    #               for x in range(xOutput):\n",
    "    #                   # Fetch image section x,y, bc,br\n",
    "    #                   image_block[0:knl * knl * ifmap, x, y] = imagePadded[0:ifmap,\n",
    "    #                                                             strides * x: strides * x + xKernShape,\n",
    "    #                                                             strides * y: strides * y + yKernShape].reshape(\n",
    "    #                       knl * knl * ifmap).cpu().numpy()\n",
    "    #                   data_moved_count += xKernShape*yKernShape*2\n",
    "    #                   mac_completed_count += ncol\n",
    "    #                   # mac_completed_count += (ncol*4)\n",
    "    #                   # data_moved_count += (ncol*4)\n",
    "    #                   data_moved_count += ncol*4.625\n",
    "    #                   # digital += 6\n",
    "\n",
    "    #                   itemp = image_block[br*nrow:(br+1)*nrow, x, y].reshape(nrow)\n",
    "    #                   itemp = torch.from_numpy(itemp)\n",
    "    #                   # Replace this line with Arduino SPI function call\n",
    "    #                   otemp[br, bc*ncol:(bc+1)*ncol, x, y] = self.non_linear_test(ktemp, itemp, dim=1)\n",
    "    #                   #otemp[br, bc*ncol:(bc+1)*ncol, x, y] = self.scaled_MAC(ktemp, itemp, dim=1)\n",
    "    #                   #otemp[br, bc*ncol:(bc+1)*ncol, x, y] = self.non_linear_mult(ktemp, itemp)\n",
    "    #                   # print(\"Batch:%d,BROW:%d,BCOL:%d,X:%d,Y:%d\" % (b,br,bc,x,y))\n",
    "\n",
    "    #   output[b] = torch.sum(otemp[:, 0:ofmap, :, :], dim=0) + (bias.reshape(ofmap, 1, 1).cpu()*torch.ones(xOutput, yOutput)).detach().numpy()\n",
    "\n",
    "    # OUTPUT STATIONARY\n",
    "    # Number of output block partitions to fit into hardware\n",
    "    block_col = int(np.ceil(xOutput/ncol))\n",
    "    block_row = int(np.ceil(knl*knl*ifmap/nrow))\n",
    "\n",
    "    kernel_flat = np.zeros((ofmap, block_row*nrow))\n",
    "    kernel_flat[0:ofmap, 0:ifmap*knl*knl] = torch.reshape(kernel, [ofmap, ifmap*knl*knl]).cpu().numpy()\n",
    "\n",
    "    data_moved_count = 0\n",
    "    mac_completed_count = 0\n",
    "    digital = 0\n",
    "\n",
    "    # Process image by image\n",
    "    for b in range(batch):\n",
    "      # Apply Equal Padding to All Sides\n",
    "      if padding != 0:\n",
    "          imagePadded = np.zeros((idim + padding * 2, idim + padding * 2))\n",
    "          imagePadded[int(padding):int(-1 * padding), int(padding):int(-1 * padding)] = image[b]\n",
    "      else:\n",
    "          imagePadded = image[b]\n",
    "\n",
    "      image_block = np.zeros((nrow*block_row, ncol*block_col, yOutput))\n",
    "      otemp = torch.zeros((block_row, ofmap, ncol*block_col, yOutput))\n",
    "\n",
    "      for y in range(yOutput):\n",
    "         for x in range(xOutput):\n",
    "            image_block[0:knl * knl * ifmap, x, y] = imagePadded[0:ifmap,\n",
    "                                                          strides * x: strides * x + xKernShape,\n",
    "                                                          strides * y: strides * y + yKernShape].reshape(\n",
    "                    knl * knl * ifmap).cpu().numpy()\n",
    "\n",
    "      # Iterate through image\n",
    "      for y in range(yOutput):\n",
    "          for bc in range(block_col):\n",
    "            for br in range(block_row):\n",
    "              data_moved_count += nrow*ncol*2\n",
    "              for kerCol in range(ofmap):\n",
    "                data_moved_count += nrow*2\n",
    "                ktemp = kernel_flat[kerCol, br*nrow:(br+1)*nrow]\n",
    "                ktemp = torch.from_numpy(ktemp)\n",
    "                for x in range(ncol):\n",
    "                    mac_completed_count += 1\n",
    "                    data_moved_count += 1*4.625\n",
    "                    # mac_completed_count += 4\n",
    "                    # data_moved_count += 4\n",
    "                    # digital += 6\n",
    "                    # Fetch image section x,y, bc,br\n",
    "                    itemp = image_block[br*nrow:(br+1)*nrow, (bc*ncol)+x, y].reshape(1, nrow)\n",
    "                    itemp = torch.from_numpy(itemp)\n",
    "                    # Replace this line with Arduino SPI function call\n",
    "                    otemp[br, kerCol, (bc*ncol)+x, y] = torch.sum(ktemp*itemp, dim=1)\n",
    "                    # otemp[kerCol, x, y] = self.non_linear_mult(ktemp, itemp[0])\n",
    "                    # print(\"Batch:%d,BROW:%d,BCOL:%d,X:%d,Y:%d\" % (b,br,bc,x,y))\n",
    "      output[b] = torch.sum(otemp[:, :, 0:xOutput, :], dim=0) + (bias.reshape(ofmap, 1, 1).cpu()*torch.ones(xOutput, yOutput)).detach().numpy()\n",
    "\n",
    "    print(f\"Completed {mac_completed_count} MAC operations\")\n",
    "    print(f\"{data_moved_count} bytes of data moved\")\n",
    "    print(f\"{block_row-1 + digital} digital operations completed\")\n",
    "    return output\n",
    "\n",
    "\n",
    "  def relu6(self, x, min_val, max_val, relu_module):\n",
    "    i = (x >= min_val) * x\n",
    "    out_relu = (i <= max_val) * (i - max_val) + max_val\n",
    "    so_relu = relu_module.output_scale\n",
    "\n",
    "    # Apply fake quantization to relu1 output.\n",
    "    out_relu = self.noscale_quant(out_relu, so_relu, 0, self.num_bits)\n",
    "    return self.dequantize(out_relu, so_relu, 0)\n",
    "\n",
    "\n",
    "  def maxpool2d(self, x, idim, ofmap, knl):\n",
    "    # Maxpool layer - downsample by knl x knl with maxpool (no quantization required for max function)\n",
    "    return torch.from_numpy(self.maxpool2D_wsa(self.batch, x, idim, ofmap, knl))\n",
    "\n",
    "\n",
    "  # Maxpool layer (apply to all ofmaps simultaneously - faster!)\n",
    "  def maxpool2D_wsa(self, batch, image, idim, ofmap, knl):\n",
    "    xKernShape = knl\n",
    "    yKernShape = knl\n",
    "    xImgShape = idim\n",
    "    yImgShape = idim\n",
    "    strides = knl\n",
    "    padding = 0\n",
    "    # Shape of Output Convolution\n",
    "    xOutput = int(((xImgShape - xKernShape + 2 * padding) / strides) + 1)\n",
    "    yOutput = int(((yImgShape - yKernShape + 2 * padding) / strides) + 1)\n",
    "    output = np.zeros((batch, ofmap, xOutput, yOutput))\n",
    "\n",
    "    for b in range(batch):\n",
    "        # Apply Equal Padding to All Sides\n",
    "        if padding != 0:\n",
    "            imagePadded = np.zeros((idim + padding * 2, idim + padding * 2))\n",
    "            imagePadded[int(padding):int(-1 * padding), int(padding):int(-1 * padding)] = image[b]\n",
    "        else:\n",
    "            imagePadded = image[b]\n",
    "\n",
    "        # Iterate through image\n",
    "        for y in range(yOutput):\n",
    "            for x in range(xOutput):\n",
    "                output[b, :, x, y] = torch.amax(\n",
    "                        imagePadded[:, strides * x: strides * x + xKernShape, strides * y: strides * y + yKernShape], dim=(1,2)).detach().cpu().numpy()\n",
    "    return output\n",
    "  \n",
    "\n",
    "  def avgpool2d(self, x, idim, ofmap, knl):\n",
    "    # Maxpool layer - downsample by knl x knl with maxpool (no quantization required for max function)\n",
    "    return torch.from_numpy(self.avgpool2D_wsa(self.batch, x, idim, ofmap, knl))\n",
    "  \n",
    "\n",
    "  # Maxpool layer (apply to all ofmaps simultaneously - faster!)\n",
    "  def avgpool2D_wsa(self, batch, image, idim, ofmap, knl):\n",
    "    xKernShape = knl\n",
    "    yKernShape = knl\n",
    "    xImgShape = idim\n",
    "    yImgShape = idim\n",
    "    strides = knl\n",
    "    padding = 0\n",
    "    # Shape of Output Convolution\n",
    "    xOutput = int(((xImgShape - xKernShape + 2 * padding) / strides) + 1)\n",
    "    yOutput = int(((yImgShape - yKernShape + 2 * padding) / strides) + 1)\n",
    "    output = np.zeros((batch, ofmap, xOutput, yOutput))\n",
    "\n",
    "    for b in range(batch):\n",
    "        # Apply Equal Padding to All Sides\n",
    "        if padding != 0:\n",
    "            imagePadded = np.zeros((idim + padding * 2, idim + padding * 2))\n",
    "            imagePadded[int(padding):int(-1 * padding), int(padding):int(-1 * padding)] = image[b]\n",
    "        else:\n",
    "            imagePadded = image[b]\n",
    "\n",
    "        # Iterate through image\n",
    "        for y in range(yOutput):\n",
    "            for x in range(xOutput):\n",
    "                for channel in range(ofmap):\n",
    "                  output[b, channel, x, y] = torch.sum(\n",
    "                     imagePadded[channel, strides * x: strides * x + xKernShape, strides * y: strides * y + yKernShape].reshape(knl*knl) \n",
    "                     * torch.ones(xKernShape*yKernShape), dim=0)/(knl*knl)\n",
    "                  # output[b, channel, x, y] = self.non_linear_mult(\n",
    "                  #       imagePadded[channel, strides * x: strides * x + xKernShape, strides * y: strides * y + yKernShape].reshape(knl*knl),\n",
    "                  #       torch.ones(xKernShape*yKernShape))/(knl*knl)\n",
    "    return output\n",
    "\n",
    "\n",
    "  def fc(self, x, module, odim):\n",
    "    sin_fc = module.input_scale.cpu()\n",
    "    in_fcs = self.noscale_quant(x, sin_fc, 0, self.num_bits)\n",
    "    sw_fc, filters_fc = self.scale_quant(module.weight.cpu(), self.num_bits)\n",
    "    bias_fc = module.bias.cpu() / (sw_fc * sin_fc)\n",
    "\n",
    "\n",
    "    # FC layer using (WSA dataflow)\n",
    "    # N.B. Need to implement WSAB function to implement 'hardware acceleration' of FC layer.\n",
    "    out_fc = torch.from_numpy(self.fc_custom_wsab(in_fcs, filters_fc, bias_fc, self.batch, odim, self.ncol, self.nrow))\n",
    "\n",
    "    # FC output scaling\n",
    "    out_fcs = out_fc * sin_fc * sw_fc\n",
    "\n",
    "    so_fc = module.output_scale.cpu()\n",
    "    # FC output fake quantization\n",
    "    return self.dequantize(self.noscale_quant(out_fcs, so_fc, 0, self.num_bits), so_fc, 0).to(self.device)\n",
    "\n",
    "\n",
    "  # 'Fake' Quantization function [Jacob et. al]\n",
    "  def quantize(self, real_value, scale, zero_point, qmin, qmax):\n",
    "    transformed_val = zero_point + real_value / scale\n",
    "    clamped_val = torch.clamp(transformed_val, qmin, qmax)\n",
    "    quantized_val = torch.round(clamped_val)\n",
    "    return quantized_val\n",
    "\n",
    "\n",
    "  # 'Fake' Dequantization function [Jacob et. al]\n",
    "  def dequantize(self, quantized_val, scale, zero_point):\n",
    "    real_val = scale * (quantized_val - zero_point)\n",
    "    return real_val\n",
    "\n",
    "\n",
    "  # Scaling function (Jacob et. al)\n",
    "  def scale_quant(self, real_value, num_bits):\n",
    "    qmin = -(2 ** (num_bits - 1) - 1)\n",
    "    qmax = 2 ** (num_bits - 1) - 1\n",
    "    abs_max = torch.abs(real_value).max()\n",
    "    scale = abs_max / (float(qmax - qmin) / 2)\n",
    "    zero_point = 0\n",
    "    quant = self.quantize(real_value, scale, zero_point, qmin, qmax)\n",
    "    return scale, quant\n",
    "\n",
    "\n",
    "  # Scaling function (Jacob et. al)\n",
    "  def noscale_quant(self, real_value, scale, zero_point, num_bits):\n",
    "    qmin = -(2 ** (num_bits - 1) - 1)\n",
    "    qmax = 2 ** (num_bits - 1) - 1\n",
    "    quant = self.quantize(real_value, scale, zero_point, qmin, qmax)\n",
    "    return quant\n",
    "\n",
    "\n",
    "  # Custom FC layer (vector multiplication style - much faster than for loop implementation)\n",
    "  def fc_custom_wsa(self, fc_input, filters, bias, batch, ofmap):\n",
    "    output = np.zeros((batch, ofmap))\n",
    "    for b in range(batch):\n",
    "        outputi = np.zeros(ofmap)\n",
    "        input_batch = fc_input[b]\n",
    "        outputi = torch.sum((input_batch * filters),dim=1) + bias\n",
    "        output[b, :] = outputi.detach().numpy()\n",
    "    return output\n",
    "  \n",
    "  \n",
    "  def fc_custom_wsab(self, fc_input, filters, bias, batch, ofmap, ncol, nrow):\n",
    "    output = np.zeros((batch, ofmap))\n",
    "\n",
    "    block_col = int(np.ceil(ofmap/ncol))\n",
    "    block_row = int(np.ceil(fc_input.size()[1]/nrow))\n",
    "    filters_flat = np.zeros((block_col*ncol, block_row*nrow), dtype='int64')\n",
    "    filters_flat[0:ofmap,0:fc_input.size()[1]] = torch.reshape(filters, [ofmap, fc_input.size()[1]]).cpu().numpy()\n",
    "    \n",
    "    # Iterate through image\n",
    "    for b in range(batch):\n",
    "      image_col = np.zeros((block_row*nrow), dtype='int64')\n",
    "      image_col[0:fc_input.size()[1]] = fc_input[b, :]\n",
    "\n",
    "      otemp = torch.zeros(block_row, block_col*ncol)\n",
    "\n",
    "      # WEIGHT STATIONARY\n",
    "      for bc in range(block_col):\n",
    "          for br in range(block_row):\n",
    "              ftemp = torch.zeros(ncol,nrow)\n",
    "              ftemp = filters_flat[ncol*bc:(bc+1)*ncol, nrow*br:(br+1)*nrow]\n",
    "              ftemp = torch.from_numpy(ftemp)\n",
    "\n",
    "              itemp = image_col[br*nrow:(br+1)*nrow].reshape(nrow)\n",
    "              itemp = torch.from_numpy(itemp)\n",
    "\n",
    "              for col in range(ncol):\n",
    "                # Replace this line with Arduino SPI function call\n",
    "                #otemp[br, (bc*ncol)+col] = self.scaled_MAC(ftemp[col, :], itemp)\n",
    "                otemp[br, (bc*ncol)+col] = self.non_linear_test(ftemp[col, :], itemp)\n",
    "                #otemp[br, (bc*ncol)+col] = self.non_linear_mult(ftemp[col, :], itemp[0])\n",
    "                # print(\"Batch:%d,BROW:%d,BCOL:%d,X:%d,Y:%d\" % (b,br,bc,x,y))\n",
    "      output[b] = torch.sum(otemp[:, 0:ofmap], dim=0) + (bias.reshape(ofmap).cpu()*torch.ones(ofmap)).detach().numpy()\n",
    "    return output\n",
    "\n",
    "\n",
    "  # Non-linear look-up\n",
    "  def non_linear_conv(self, kernel, image):\n",
    "    sum = 0\n",
    "    if self.num_bits == 9 or self.num_bits == 8:\n",
    "       bit = 9\n",
    "    else:\n",
    "       bit = 5\n",
    "    for i in range(len(kernel)):\n",
    "      sum = sum + self.nl_mult[int(kernel[i]+(2**(bit-1))-1)][int(image[i]+(2**(bit-1))-1)]\n",
    "      #sum = sum + (kernel[i]*image[i] + 20*kernel[i]*np.sin(np.pi*kernel[i]/5)*np.sin(np.pi*image[i]/5))\n",
    "    return sum\n",
    "\n",
    "  def non_linear_mult(self, kernel, image):\n",
    "    output = np.empty(len(kernel))\n",
    "    if len(kernel.shape) == 2:\n",
    "      for i in range(len(kernel)):\n",
    "        output[i] = self.non_linear_conv(kernel[i, :].tolist(), image.tolist())\n",
    "      return torch.from_numpy(output)\n",
    "    else:\n",
    "      output = self.non_linear_conv(kernel.tolist(), image.tolist())\n",
    "      return output\n",
    "    \n",
    "  def non_linear_test(self, kernel, image, dim=0):\n",
    "    #noiseFactor = 500\n",
    "    # if len(kernel.shape) == 2:\n",
    "    #   noise = np.random.normal(0, noiseFactor, (self.ncol, self.nrow))\n",
    "    # else:\n",
    "    #    noise = np.random.normal(0, noiseFactor, (self.nrow))\n",
    "    # return torch.sum(kernel*image + noise, dim=dim, dtype=torch.int64)\n",
    "    return torch.sum(kernel*image, dim=dim, dtype=torch.int64)\n",
    "      \n",
    "  def slice_input(self, array):\n",
    "    up_array = np.sign(array)*(abs(array)&np.int16(240))>>4\n",
    "    low_array = np.sign(array)*(abs(array)&np.int16(15))\n",
    "    return up_array, low_array\n",
    "      \n",
    "  \n",
    "  def scaled_MAC(self, kernel, image, dim=0):\n",
    "\n",
    "    if len(kernel.shape) == 2:\n",
    "        kernel_l = torch.empty((kernel.shape[0], kernel.shape[1]))\n",
    "        kernel_u = torch.empty((kernel.shape[0], kernel.shape[1]))\n",
    "        i = 0\n",
    "        for channel in kernel:\n",
    "          kernel_u[i], kernel_l[i] = self.slice_input(channel)\n",
    "          i += 1\n",
    "    else:\n",
    "      kernel_u, kernel_l = self.slice_input(kernel)\n",
    "\n",
    "    image_u, image_l = self.slice_input(image)\n",
    "    \n",
    "    result = torch.add(torch.add(self.non_linear_test(kernel_l, image_l, dim=dim), (self.non_linear_test(kernel_u, image_l, dim=dim)\n",
    "                                                      + self.non_linear_test(kernel_l, image_u, dim=dim))<<4), (self.non_linear_test(kernel_u, image_u, dim=dim))<<8)\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel dimensions (used in conv layers)\n",
    "knl = 5\n",
    "\n",
    "# Kernel stride\n",
    "stride = 1\n",
    "\n",
    "# Output Feature Maps (same as kernels in conv layer 1)\n",
    "ofmap = 16\n",
    "\n",
    "# Kernels in conv layer 2\n",
    "conv2kernels = 32\n",
    "\n",
    "# Maxpool dimensions (assuming square area)\n",
    "maxpool_dim = 2\n",
    "\n",
    "# Fully Connected 1 output nodes\n",
    "fc1_nodes = 50\n",
    "\n",
    "# Hardware MVM columns\n",
    "ncol = 64\n",
    "\n",
    "# Hardware MVM rows\n",
    "nrow = 32\n",
    "\n",
    "# Non-Idealities File\n",
    "non_ideal_file = '../HardwareSpec/4x4_Mac_result_final.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_input():\n",
    "    width = 300  # canvas width\n",
    "    height = 300 # canvas height\n",
    "    black = (0, 0, 0) # canvas back\n",
    "\n",
    "    def save_and_exit():\n",
    "        # save image to hard drive\n",
    "        filename = \"data/user_input.jpg\"\n",
    "        output_image.save(filename)\n",
    "        master.destroy()\n",
    "\n",
    "    def paint(event):\n",
    "        x1, y1 = (event.x - 1), (event.y - 1)\n",
    "        x2, y2 = (event.x + 1), (event.y + 1)\n",
    "        canvas.create_oval(x1, y1, x2, y2, fill=\"white\",width=40)\n",
    "        draw.line([x1, y1, x2, y2],fill=\"white\",width=40)\n",
    "\n",
    "    master = Tk()\n",
    "\n",
    "    # create a tkinter canvas to draw on\n",
    "    canvas = Canvas(master, width=width, height=height, bg='black')\n",
    "    canvas.pack()\n",
    "\n",
    "    # create an empty PIL image and draw object to draw on\n",
    "    output_image = PIL.Image.new(\"RGB\", (width, height), black)\n",
    "    draw = PIL.ImageDraw.Draw(output_image)\n",
    "    canvas.pack(expand=YES, fill=BOTH)\n",
    "    canvas.bind(\"<B1-Motion>\", paint)\n",
    "\n",
    "    # add a button to save the image\n",
    "    button=Button(text=\"save\",command=save_and_exit)\n",
    "    button.pack()\n",
    "\n",
    "    master.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_CNN():\n",
    "    torch.set_default_dtype(torch.float32)\n",
    "    torch.manual_seed(0)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # INT9 weight, INT9 activations\n",
    "    num_bits = 8\n",
    "    # Make sure this matches quantization config from MNIST_CNN_Training\n",
    "    configure_list = [{\n",
    "        'quant_types': ['weight', 'input'],\n",
    "        'quant_bits': {'weight': num_bits, 'input': num_bits},\n",
    "        'quant_start_step': 2,\n",
    "        'op_names': ['conv1', 'conv2']\n",
    "    }, {\n",
    "        'quant_types': ['output'],\n",
    "        'quant_bits': {'output': num_bits},\n",
    "        'quant_start_step': 2,\n",
    "        'op_names': ['relu1', 'relu2', 'relu3']\n",
    "    }, {\n",
    "        'quant_types': ['output', 'weight', 'input'],\n",
    "        'quant_bits': {'output': num_bits, 'weight': num_bits, 'input': num_bits},\n",
    "        'quant_start_step': 2,\n",
    "        'op_names': ['fc1', 'fc2'],\n",
    "    }]\n",
    "\n",
    "    set_quant_scheme_dtype('weight', 'per_tensor_symmetric', 'int')\n",
    "    set_quant_scheme_dtype('output', 'per_tensor_symmetric', 'int')\n",
    "    set_quant_scheme_dtype('input', 'per_tensor_symmetric', 'int')\n",
    "\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    user_image = PIL.Image.open('data/user_input.jpg')\n",
    "    user_image = user_image.resize((28,28), PIL.Image.LANCZOS).convert('L')\n",
    "    tensor_image = transform(user_image)\n",
    "\n",
    "    #tensor_image = transform(PIL.Image.open('data/eight.jpg'))\n",
    "\n",
    "    idim = tensor_image.size()[1]\n",
    "    ifmap = tensor_image.size()[0]\n",
    "    fc2_nodes = 10\n",
    "\n",
    "    # Create a NaiveModel object and apply QAT_Quantizer setup\n",
    "    model_path = \"models/mnist_model_8bit.pth\"\n",
    "    qmodel = NaiveModel().to(device)\n",
    "    optimizer = torch.optim.SGD(qmodel.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "    dummy_input = torch.randn(1, ifmap, idim, idim).to(device)\n",
    "    # To enable batch normalization folding in the training process, you should\n",
    "    # pass dummy_input to the QAT_Quantizer.\n",
    "    quantizer = QAT_Quantizer(qmodel, configure_list, optimizer, dummy_input=dummy_input)\n",
    "    quantizer.compress()\n",
    "\n",
    "\n",
    "    # Load trained model (from MNIST_CNN_Training step).\n",
    "    state = torch.load(model_path, map_location=device)\n",
    "    qmodel.load_state_dict(state, strict=True)\n",
    "    qmodel.eval()\n",
    "    return tensor_image, device, qmodel, num_bits, ifmap, idim, fc2_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 24576 MAC operations\n",
      "236544.0 bytes of data moved\n",
      "0 digital operations completed\n",
      "Completed 212992 MAC operations\n",
      "1624064.0 bytes of data moved\n",
      "12 digital operations completed\n",
      "Number written is 2\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "capture_input()\n",
    "tensor_image, device, model, num_bits, ifmap, idim, fc2_nodes = setup_CNN()\n",
    "plt.imshow(tensor_image.permute(1, 2, 0))\n",
    "\n",
    "with torch.no_grad():\n",
    "        data = tensor_image.unsqueeze(0)\n",
    "        data = data.to(device)\n",
    "        batch = 1\n",
    "\n",
    "        # Custom forward pass function\n",
    "        non_linear_model = forward_custom(model, data, batch, num_bits, ifmap, ofmap, idim, knl, stride, conv2kernels,\n",
    "                                            maxpool_dim, fc1_nodes, fc2_nodes, non_ideal_file, ncol, nrow, device)\n",
    "        output = non_linear_model.forward_pass()\n",
    "        pred = output.argmax(dim=1, keepdim=True)[0][0]\n",
    "        print(f\"Number written is {pred}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
